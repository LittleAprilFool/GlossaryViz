[
{"content": ["It is time to move on to computer graphics in three dimensions, although\nit won't be until Section\u00a02 of this chapter that we really get into 3D.\nYou will find that many concepts from 2D graphics carry over to 3D, but the move into\nthe third dimension brings with it some new features that take a while to\nget used to.", "Our focus will be ", ", a graphics API\nthat was introduced in 1992 and has gone through many versions and many \nchanges since then.  OpenGL is a  low-level graphics API, similar to the 2D APIs we have\ncovered.  It is even more primitive in some ways, but of course it is\ncomplicated by the fact that it supports 3D.", "For the next two chapters, the discussion is\nlimited to OpenGL\u00a01.1.  OpenGL 1.1 is a large API, and we will \nonly cover a part of it. The goal is to introduce 3D graphics concepts, \nnot to fully cover the API.  A significant part of what we cover here\nhas been removed from the most modern versions of OpenGL.  However,\nmodern OpenGL in its pure form has a very steep initial learning curve,\nand it is really not a good starting place for someone who is encountering\n3D graphics for the first time.  Some additional support is needed\u2014if not OpenGL 1.1\nthen some similar framework.  Since OpenGL 1.1 is\nstill supported, at least by all desktop implementations of OpenGL,\nit's a reasonable place to start.", "This chapter concentrates on the geometric aspects of 3D graphics, such as defining\nand transforming objects and projecting 3D scenes into 2D images.  The images that\nwe produce will look very unrealistic.  In the next chapter, we will see how to add\nsome realism by simulating the effects of lighting and of the material properties of surfaces."], "section_id": "0", "chapter_id": "Chapter 3", "chapter_title": "OpenGL 1.1: Geometry", "section_title": "null"},
{"content": ["With this chapter, we begin our study of computer graphics by looking at the two-dimensional case.\nThings are simpler, and a lot easier to visualize, in 2D than in 3D, but most of\nthe ideas that are covered in this chapter will also be very relevant to 3D.", "The chapter begins with four sections that examine 2D graphics in a general way,\nwithout tying it to a particular programming language or graphics API.  The coding\nexamples in these sections are written in pseudocode that should make sense to\nanyone with enough programming background to be reading this book.\nIn the next three sections, we will take quick looks at 2D graphics in three\nparticular languages: Java with ", ",\nJavaScript with HTML ", " graphics, and SVG.  We will see how these\nlanguages use many of the general ideas from earlier in the chapter."], "section_id": "0", "chapter_id": "Chapter 2", "chapter_title": "Two-Dimensional Graphics", "section_title": "null"},
{"content": ["The term \"computer graphics\" refers to anything involved in the creation or\nmanipulation of images on computer, including animated images.  It is a very\nbroad field, and one in which changes and advances seem to come at a dizzying pace.\nIt can be difficult for a beginner to know where to start.  However, there is\na core of fundamental ideas that are part of the foundation of most applications\nof computer graphics.  This book attempts to cover those foundational ideas, or\nat least as many of them as will fit into a one-semester college-level course.\nWhile it is not possible to cover the entire field in a first course\u2014or even a large\npart of it\u2014this should be a good place to start.", "This short chapter provides an overview and introduction to the material\nthat will be covered in the rest of the book, without going into a lot of detail."], "section_id": "0", "chapter_id": "Chapter 1", "chapter_title": "Introduction", "section_title": "null"},
{"content": ["\n", " and ", " introduced 3D graphics \nusing ", "\u00a01.1.  Most of the ideas covered in those chapters remain relevant to modern\ncomputer graphics, but there have been many changes and improvements since the early\ndays of OpenGL.  In the remaining chapters, we will be using ", ",\na modern version of OpenGL that is used to create 3D graphics content for web pages.", "WebGL is a low level language\u2014even more so than OpenGL 1.1, since a WebGL program\nhas to handle a lot of the low-level implementation details that were handled internally\nin the original version of OpenGL.  This makes WebGL much more flexible, but more difficult\nto use.  We will soon turn to working\ndirectly with WebGL.  However, before we do that, we will look\nat a higher-level ", " for 3D web graphics that is built on top of WegGL:  \n", ".  There are several reasons for starting at this high level.\nIt will allow you to see how some of the things that you have learned are used in\na modern graphics package.  It will allow me to introduce some new features such as\nshadows and environment mapping.  It will let you work with a graphics library that\nyou might use in real web applications.\nAnd it will be a break from the low-level detail we have been dealing with, before\nwe move on to an even lower level.", "You can probably follow much of the discussion in this chapter without knowing \nJavaScript.  However, if you want to do any programming with ", " (or with \nWebGL), you need to know JavaScript.  The basics of the language are covered in\n", " in ", "."], "section_id": "0", "chapter_id": "Chapter 5", "chapter_title": "Three.js: A 3D Scene Graph API", "section_title": "null"},
{"content": ["The previous chapter covered WebGL, but only in the context of\ntwo-dimensional graphics.   As we move into 3D, we will have to\nwork with more complex ", ".\nFor that, we will rely mainly on an open-source JavaScript library for \nvector and matrix math.  We will also need to implement\n", " and ", ", which we will\ndo directly in ", ".", "We begin the chapter by duplicating most of the capabilities of ", "\u00a01.1\nthat were covered in ", " and ", ".\nBut we will soon move beyond that by adding features such as\n", ", ", ", and\n", "."], "section_id": "0", "chapter_id": "Chapter 7", "chapter_title": "3D Graphics with WebGL", "section_title": "null"},
{"content": ["One of the goals of computer graphics is physical realism, that is, making\nimages that look like they could be photographs of reality.  This is not the only \ngoal.  For example, for scientific visualization, the goal is to use computer \ngraphics to present information accurately and clearly.  Artists can use computer\ngraphics to create abstract rather than realistic art.  However, realism is a\nmajor goal of some of the most visible uses of computer graphics, such as video\ngames, movies, and advertising.", "One important aspect of physical realism is ", ":\nthe play of light and shadow, the way that light reflects from different \n", ", the\nway it can bend or be diffracted into a spectrum as it passes through translucent\nobjects.  The techniques that are used to produce the most realistic graphics\ncan take all these factors and more into account.", "However, another goal of computer graphics is ", ".  OpenGL, in particular,\nwas designed for ", ", where the time that is available\nfor rendering an image is a fraction of a second.  For an animated movie, it's OK if\nit takes hours to ", " each frame.  But a video game is expected to\nrender sixty frames every second.  Even with the incredible speed of modern computer graphics\nhardware, compromises are necessary to get that speed.  And twenty years ago, when OpenGL\nwas still new, the compromises were a lot bigger", "In this chapter, we look at light and material in OpenGL 1.1.   You will learn how to\nconfigure light sources and how to assign material properties to objects.  Material properties determine how\nthe objects interact with light.  And you will learn how to apply an image to a surface\nas a ", ".  The support for light, material, and texture in OpenGL 1.1\nis relatively crude and incomplete, by today's standards.  But the concepts that it uses\nstill serve as the foundation for modern real-time graphics and, to a significant extent,\neven for the most realistic computer graphics."], "section_id": "0", "chapter_id": "Chapter 4", "chapter_title": "OpenGL 1.1: Light and Material", "section_title": "null"},
{"content": ["In this chapter, we turn to ", ", the version of ", "\nfor the Web.  ", ", which was covered in the ", ",\nuses WebGL for 3D graphics.  Of course, it is more difficult to use WebGL directly, but doing\nso gives you full control over the graphics hardware.  And learning it will be a good introduction\nto modern graphics programming.", "There have been many versions of OpenGL.  WebGL is based on OpenGL ES 2.0, a version\ndesigned for use on embedded systems such as smart phones and tablets.  OpenGL ES 1.0\nwas very similar to OpenGL 1.1, which we studied in ", "\nand ", ".  However, the 2.0 version of OpenGL\u00a0ES introduced\nmajor changes.  It is actually a smaller, simpler ", " that puts more responsibility\non the programmer.  For example, functions for working with transformations, such\nas ", " and ", ", were eliminated from the API, making the\nprogrammer responsible for keeping track of transformations.  WebGL does not\nuse ", " to generate geometry, and it doesn't use function such\nas ", " or ", " to specify attributes of vertices.  Nevertheless,\nit will turn out that much of what you learned in previous chapters will carry over to\nWebGL.", "There are two sides to any WebGL program.  Part of the program is written in\n", ", the programming language for the web. The second part is\nwritten in ", ", a language for writing \"shader\" programs that run on the\n", ". I will try to always be clear about which language I am\ntalking about.", "For this introductory chapter about WebGL, we will stick to basic \n2D graphics.  You will learn about the structure of WebGL programs.  You will learn\nmost of the JavaScript side of the API, and you learn how to write and use simple\nshaders.  In the ", ", we will move\non to 3D graphics, and you will learn a great deal more about GLSL."], "section_id": "0", "chapter_id": "Chapter 6", "chapter_title": "Introduction to WebGL", "section_title": "null"},
{"content": ["The first seven chapters of this textbook have covered real-time computer graphics,\nthat is, graphics systems in which an image can be generated in a fraction of a second.\nThe typical case is a video game, where new frames can be rendered as many as sixty times\nper second.  Very complex and realistic-looking scenes can be rendered in real time,\nusing techniques covered in this book and the immense processing power of modern\nGPUs, plus some tricks and advanced algorithms.  \nHowever, real-time graphics still can't match the realism\nof the very highest quality computer graphics, such as what can be found in\nmovies.  In fact, the CGI (computer generated imagery) in today's movies is sometimes\nindistinguishable from reality.  Getting graphics of that quality can require\nhours of computing time to render a single frame.  (According to the Wikipedia\narticle about the 2013 animated movie ", ", some complex scenes in that\nmovie used 30 hours of computing to render each frame.)", "This chapter is a very brief look at some techniques that can be used for very\nhigh quality graphics.  The discussion will be in general terms.  I won't be giving \nsample code or detailed discussions of the mathematics behind the techniques,\nbut I hope to provide at least a basic conceptual understanding.", "The first thing that you should understand, though, is\nthat most of what you have leaned so far still applies.  Scenes are still composed\nusing geometric primitives, transformations, materials, textures, and light sources\n(although perhaps using more advanced material properties and lighting than we have\nencountered so far).  The graphic designers working on CGI for a movie can see real \ntime previews of their work that are rendered using techniques that we have covered.\nThe final scene that you see in the movie is just rendered using different,\nmuch more computation-intensive techniques."], "section_id": "0", "chapter_id": "Chapter 8", "chapter_title": "Beyond Realtime Graphics", "section_title": "null"},
{"content": ["We will be using OpenGL as the primary basis for 3D graphics programming.\nThe original version of OpenGL was released in 1992 by a company named\nSilicon Graphics, which was known for its graphics workstations\u2014powerful,\nexpensive computers designed for intensive graphical applications.  (Today,\nyou probably have more graphics computing power on your smart phone.)  OpenGL\nis supported by the graphics hardware in most modern computing devices, including\ndesktop computers, laptops, and many mobile devices.  This section will give\nyou a bit of background about the history of OpenGL and about the graphics \nhardware that supports it.", "In the first desktop computers, the contents of the screen were managed\ndirectly by the ", ".  For example, to draw a line segment on the screen, the CPU\nwould run a loop to set the color of each pixel that lies along the line.\nNeedless to say, graphics could take up a lot of the CPU's time.  And graphics\nperformance was very slow, compared to what we expect today.  So what has changed?\nComputers are much faster in general, of course, but the big change is that\nin modern computers, graphics processing is done by a specialized component\ncalled a ", ", or Graphics Processing Unit.  A GPU includes processors\nfor doing graphics computations; in fact, it can include a large number of such\nprocessors that work in parallel to greatly speed up graphical operations.  \nIt also includes its own dedicated memory for storing things like images and \nlists of coordinates.  GPU processors have very fast\naccess to data that is stored in GPU memory\u2014much faster than their access to data\nstored in the computer's main memory.", "To draw a line or perform some other graphical operation, the CPU simply has to\nsend commands, along with any necessary data, to the GPU, which is responsible\nfor actually carrying out those commands.  The CPU offloads most of the graphical\nwork to the GPU, which is optimized to carry out that work very quickly.\nThe set of commands that the GPU understands make up the ", "\nof the GPU.  OpenGL is an example of a graphics API, and most GPUs support\nOpenGL in the sense that they can understand OpenGL commands, or at least\nthat OpenGL commands can efficiently be translated into commands that the\nGPU can understand.", "OpenGL is not the only graphics API.  The best-known alternative is probably \nDirect3D, a 3D graphics API used for Microsoft Windows.  OpenGL is more widely\navailable, since it is not limited to Microsoft, but Direct3D is supported by\nmost graphics cards, and it has often introduced new features earlier than OpenGL. ", "I have said that OpenGL is an API, but in fact it is a series of APIs that have\nbeen subject to repeated extension and revision.  The current version, in early\n2015, is 4.5, and it is very different from the 1.0 version from 1992.  Furthermore,\nthere is a specialized version called OpengGL\u00a0ES for \"embedded systems\" such\nas mobile phones and tablets.  And there is also WebGL, for use in Web browsers,\nwhich is basically a port of OpenGL ES\u00a02.0.  It's useful to know something\nabout how and why OpenGL has changed.", "First of all, you should know that OpenGL was designed as a \"client/server\"\nsystem.  The server, which is responsible for controlling the computer's\ndisplay and performing graphics computations, carries out commands issued by the\nclient.  Typically, the server is a GPU, including its graphics processors and memory.\nThe server executes OpenGL commands.  The client is the CPU in the same computer, along \nwith the application program that it is running. OpenGL commands come from the\nprogram that is running on the CPU.  However,\nit is actually possible to run OpenGL programs remotely over a network.  That\nis, you can execute an application program on a remote computer (the OpenGL client), while\nthe graphics computations and display are done on the computer that you are\nactually using (the OpenGL server).", "The key idea is that the client and the server are separate components, and there\nis a communication channel between those components.  OpenGL commands and the\ndata that they need are communicated from the client (the CPU) to the server (the GPU)\nover that channel.  The capacity of the channel can be a limiting factor in graphics\nperformance.  Think of drawing an image onto the screen.  If the GPU can draw the\nimage in microseconds, but it takes milliseconds to send the data for the image\nfrom the CPU to the GPU, then the great speed of the GPU is irrelevant\u2014most of\nthe time that it takes to draw the image is communication time.", "For this reason, one of the driving factors in the evolution of OpenGL has been\nthe desire to limit the amount of communication that is needed between the CPU and\nthe GPU.  One approach is to store information in the GPU's memory.  If some data\nis going to be used several times, it can be transmitted to the GPU once and\nstored in memory there, where it will be immediately accessible to the GPU.\nAnother approach is to try to decrease the number of OpenGL commands that must\nbe transmitted to the GPU to draw a given image.", "OpenGL draws ", " such as triangles.\nSpecifying a primitive means specifying ", "\nand ", " for each of its ", ".  In the\noriginal OpenGL\u00a01.0, a separate command was used to specify the coordinates of each vertex,\nand a command was needed each time the value of an attribute changed.  To draw a single \ntriangle would require three or more commands.  Drawing a complex object made up of\nthousands of triangles would take many thousands of commands.  Even in OpenGL\u00a01.1,\nit became possible to draw such an object with a single command instead of thousands.  All the data\nfor the object would be loaded into arrays, which could then be sent in a single\nstep to the GPU.  Unfortunately, if the object was going to be drawn more than\nonce, then the data would have to be retransmitted each time the object was drawn.\nThis was fixed in OpenGL\u00a01.5 with ", ".\nA VBO is a block of memory in the GPU that can store the coordinates or attribute values for\na set of vertices.  This makes it possible to reuse the data without having to retransmit it\nfrom the CPU to the GPU every time it is used.", "Similarly, OpenGL 1.1 introduced ", "\nto make it possible to store several images on the GPU for use as ", ".\nThis means that texture images that are going to be reused several times can be loaded once\ninto the GPU, so that the GPU can easily switch between images without having to reload them.", "As new capabilities were added to OpenGL, the API grew in size.  But the growth was still\noutpaced by the invention of new, more sophisticated techniques for doing graphics.  Some\nof these new techniques were added to OpenGL, but\nthe problem is that no matter how many features you add, there will always be demands for \nnew features\u2014as well as complaints that all the new features are making things too \ncomplicated! OpenGL was a giant machine, with new pieces always being tacked onto it, \nbut still not pleasing everyone. The real solution was to make the machine ", ".\nWith OpenGL 2.0, it became possible to write programs to be executed as part of the\ngraphical computation in the GPU.  The programs are run on the GPU at GPU speed.\nA programmer who wants to use a new graphics technique can write a program to \nimplement the feature and just hand it to the GPU.  The OpenGL API doesn't have to\nbe changed.  The only thing that the API has to support is the ability to send programs\nto the GPU for execution.", "The programs are called ", " (although the term does't\nreally describe what most of them actually do).  The first shaders to be introduced were\n", " and ", ".\nWhen a ", " is drawn, some work has to be done at each vertex of the primitive,\nsuch as applying a ", " to the vertex coodinates or\nusing the ", " and global ", " environment\nto compute the color of that vertex.  A vertex shader is a program that can take over the\njob of doing such \"per-vertex\" computations.  Similarly, some work has to be done for each\npixel inside the primitive.  A fragment shader can take over the job of performing such\n\"per-pixel\" computations.  (Fragment shaders are also called pixel shaders.)", "The idea of programmable graphics hardware was very successful\u2014so successful that\nin OpenGL\u00a03.0, the usual per-vertex and per-fragment processing\nwas deprecated (meaning that its use was discouraged). \nAnd in OpenGL\u00a03.1, it was removed from\nthe OpenGL standard, although it is still present as an optional extension.  In practice,\nall the original features of OpenGL are still supported in desktop versions of OpenGL and will\nprobably continue to be available in the future.  On the embedded system side, however,\nwith OpenGL\u00a0ES\u00a02.0 and later, the use of shaders is mandatory, and a large part\nof the OpenGL\u00a01.1 API has been completely removed.\nWebGL, the version of OpenGL for use in web browsers, \nis based on OpenGL\u00a0ES\u00a02.0, and it also requires shaders to get anything at all done.\nNevertheless, we will begin our study of OpenGL with version 1.1.  Most of the concepts and\nmany of the details from that version are still relevant, and it offers an easier entry point\nfor someone new to 3D graphics programming.", "OpenGL shaders are written in ", " (OpenGL Shading Language).  Like\nOpenGL itself, GLSL has gone through several versions. We will spend some time later in the\ncourse studying GLSL\u00a0ES\u00a01.0, the version used with WebGL\u00a01.0 and\nOpenGL\u00a0ES\u00a02.0.  GLSL uses a syntax similar to the C programming language.", "As a final remark on GPU hardware, I should note that the computations that are done for\ndifferent vertices are pretty much independent, and so can potentially be done in parallel.\nThe same is true of the computations for different fragments.  In fact, GPUs can\nhave hundreds or thousands of processors that can operate in parallel.  Admittedly, the\nindividual processors are much less powerful than a CPU, but then typical per-vertex\nand per-fragment computations are not very complicated.  The large number of processors,\nand the large amount of parallelism that is possible in graphics computations, makes\nfor impressive graphics performance even on fairly inexpensive GPUs."], "section_id": "Section 1.3", "chapter_id": "Chapter 1", "chapter_title": "Introduction", "section_title": "Hardware and Software"},
{"content": ["We finish this chapter with a look at one more 2D graphics system:\n", ", or Scalable Vector Graphics.  So far, we have\nbeen considering graphics programming APIs.  SVG, on the other\nhand is a ", " rather\nthan a programming language.  Where a programming language creates\na scene by generating its contents procedurally, a scene description\nlanguage specifies a scene \"declaratively,\" by listing its content.\nSince SVG is a ", " language, the content of\nof a scene includes shapes, attributes such as color and line width,\nand geometric transforms.  Most of this should be familiar to you,\nbut it should be interesting to see it in a new context.", "SVG is an ", " language, which means it has a very strict\nand somewhat verbose syntax.  This can make it a little annoying to write,\nbut on the other hand, it makes it possible to read and understand\nSVG documents even if you are not familiar with the syntax.  It's possible\nthat SVG originally stood for \"Simple\" Vector Graphics, but it is by\nno means a simple language at this point.  I will cover only a part of it\nhere, and there are many parts of the language and many options that I will\nnot mention.  My goal is to introduce the idea of a scene description language\nand to show how such a language can use the same basic ideas that are\nused in the rest of this chapter.", "SVG can be used as a file format for storing vector graphics\nimages, in much the same way that PNG and JPEG are file formats for\nstoring pixel-based images.  That means that you can open an SVG\nfile with a web browser to view the image.  (This is true, at least,\nfor modern web browsers.)  An SVG image can be included in a web page\nby using it as the source of an ", " element.  That's how the\nSVG examples on this page are displayed.  Since SVG documents are written in plain text,\nyou can create SVG images using a regular text editor, and you can read the\nsource for an SVG image by opening it in a text editor or by viewing the\nsource of the image when it is displayed in a web browser.", "An SVG file, like any XML document, starts with some standard code that almost\nno one memorizes.  It should just be copied into a new document.  Here\nis some code that can be copied as a starting point for SVG \ndocuments of the type discussed in this section (which, remember use \nonly a subset of the full SVG specification):", "The first three lines say that this is an XML SVG document.  The rest of\nthe document is an ", " element that acts as a container for the entire\nscene description.  You'll need to know a little about XML syntax.\nFirst, an XML \"element\" in its general form looks like this:\n", "The element starts with a \"start tag,\" which begins with a \"<\" followed by an identifier\nthat is the name of the tag, and ending with a\u00a0\">\".  The start tag can include\n\"attributes,\" which have the form ", ".  The ", " is an identifier;\nthe ", " is a string.  The value must be enclosed in single or double quotation marks.\nThe element ends with an \"end tag,\" which has an element name that matches the element name\nin the start tag and has the form </", ">.  Element names and attribute names\nare case-sensitive.  Between the start and end tags\ncomes the \"content\" of the element.  The content can consist of text and nested elements.\nIf an element has no content, you can replace the \">\" at the end of the start tag with\n\"/>\", and leave out the end tag.  This is called a \"self-closing tag.\" For example,\n", "This is an actual SVG element that specifies a circle.  It's easy to forget the \"/\"\nat the end of a self-closing tag, but it has to be there to have a legal XML document.", "Looking back at the SVG document, the five lines starting with <svg are just a long\nstart tag.  You can use the tag as shown, and customize the values of the ", ",\n", ", ", ", and ", " attributes.  The next line\nis a comment; comments in XML start with \"", "\" and end with \"", "\".", "The ", " and ", " attributes of the ", " tag specify a\nnatural or preferred size for the image.  It can be forced into a different size, for\nexample if it is used in an ", " element on a web page that specifies a different\nwidth and height.  The size can be specified using units of measure such as ", " for\ninches, ", " for centimeters, and ", ", for pixels, with 90 pixels to the inch.\nIf no unit of measure is specified, pixels are used.  There cannot be any space between\nthe number and the unit of measure.", "The ", " attribute sets up the ", " that will be used for \ndrawing the image.  It is what I called the ", " in ", ".\nThe value for viewBox is a list of four numbers,\ngiving the minimum ", "value, the minimum ", ", the width, and the height\nof the view window.  The width and the height must be positive, so ", " increases from\nleft-to-right, and ", " increases from top-to-bottom.  The four numbers in the list\ncan be separated either by spaces or by commas; this is typical for lists of numbers in SVG.", "Finally, the ", " attribute tells what happens when the\n", " of the viewBox does not match the aspect ratio of the rectangle\nin which the image is displayed.  The default value, \"xMidYMid\", will extend the limts\non the viewBox either horizontally or vertically to preserve the aspect ratio, and the\nviewBox will appear in the center of the display rectangle.  If you would like your \nimage to stretch to fill the display rectangle, ignoring the aspect ratio, set the\nvalue of ", " to \"none\".  (The aspect ratio issue was\ndiscussed in ", ".)", "Let's look at a complete SVG document that draws a few simple shapes.  Here's the\ndocument.  You could probably figure out what it draws even without knowing any more\nabout SVG:", "and here's the image that is produced by this example:", "\n", "In the drawing coordinate system for this example, ", " ranges from 0 to 3, and\n", " ranges from 0 to 2.  All values used for drawing, including stroke width\nand font size, are given in terms of this coordinate system.  Remember that you can\nuse any coordinate system that you find convenient!  Note, by the way, that parts\nof the image that are not covered by the shapes that are drawn will be transparent.", "Here's another example, with a larger variety of shapes.  The source code for this\nexample has a lot of comments. It uses features that we will discuss in the remainer of\nthis section.", "\n", "You can take a look at the source code, ", ".\n(For example, open it in a text editor, or open it in a web browser and use the\nbrowser's \"view source\" command.)", "In SVG, a basic shape is specified by an element in which the tag name gives the\nshape, and attributes give the properties of the shape.  There are attributes to specify\nthe geometry, such as the endpoints of a line or the radius of a circle.\nOther attributes specify style properties, such as fill color and line width.\n(The style properties are what I call ", " elsewhere\nin this book; in this section, I am using the term \"attribute\" in its XML sense.)\nAnd there is a ", " attribute that can be used to apply a\n", " to the shape.", "For a detailed example, consider the ", " element, which specifies a rectangle.  \nThe geometry of the rectangle is given by attributes named ", ", ", ", ", "\nand ", " in the usual way.  The default value for ", " and ", " is zero;\nthat is, they are optional, and leaving them out is the same as setting their value to zero.\nThe ", " and the ", " are required attributes.  Their values must be\nnon-negative.  For example, the element", "specifies a rectangle with corner at (0,0), width 3, and height 2, while", "gives a rectangle with corner at (100,200), width 640, and height 480.  (Note, by\nthe way, that the attributes in an XML element can be given in any order.)  The ", "\nelement also has optional attributes ", " and ", " that can be used to make\n\"roundRects,\" with their corners replaced by elliptical arcs.  The values of ", "\nand ", " give the horizontal and vertical radii of the elliptical arcs.", "Style attributes can be added to say how the shape should be stroked and filled.\nThe default is to use a black fill and no stroke.  (More precisely, as we will see later,\nthe default for is for a shape to inherit the values of style attributes from its \nenvironment.  Black fill and no stroke is the initial environment.)  Here are some\ncommon style attributes:", "As an example that uses many of these options, let's make a square is rounded rather than pointed \nat the corners, with size 1, centered\nat the origin, and using a translucent red fill and a gray stroke:", "and a simple outline of a rectangle with no fill:", "The ", " attribute can be used to apply a transform or a series of\ntransforms to a shape.  As an example, we can make a rectangle tilted 30 degrees from\nthe horizontal:", "The value \"rotate(30)\" represents a rotation of 30 degrees (not radians!) about the \norigin, (0,0). The positive direction of rotation, as usual, rotates the positive x-axis in the\ndirection of the positive y-axis.  You can specify a different center of rotation by\nadding arguments to ", ".  For example, to rotate the same rectangle about its\ncenter", "Translation and scaling work as you probably expect, with transform values of\nthe form \"translate(", ")\" and \"scale(", ")\".  There are also\n", " transforms, but they go by the\nnames ", " and ", ", and the argument is a skew angle rather\nthan a shear amount.  For example, the transform \"skewX(45)\" tilts the y-axis\nby 45 degrees and is equivalent to an x-shear with shear factor\u00a01.\n(The function that tilts the y-axis is called ", " because it modifies,\nor skews, the x-coordinates of points while leaving their y-coordinates unchanged.)\nFor example, we can use ", " to tilt a rectangle and make it into a\nparallelogram:", "I used an angle of -30 degrees to make the rectangle tilt to the right\nin the usual pixel coordinate system.", "The value of the ", " attribute can be a list of transforms,\nseparated by spaces or commas.  The transforms are applied to the object, as\nusual, in the opposite of the order in which they are listed. So,", "would first skew the rectangle into a parallelogram, then rotate the parallelogram\nby 45 degrees about the origin, then translate it by 50 units in the y-direction.", "In addition to rectangles, SVG has lines, circles, ellipses, and text as basic\nshapes.  Here are some details.  A ", " element represents a line segement and\nhas geometric attributes ", ", ", ", ", ", and ", " to specify the \ncoordinates of the endpoints of the line segment.  These four attributes have\nzero as default value, which makes it easier to specify horizontal and vertical lines.\nFor example,", "Without the ", " attribute, you wouldn't see the line, since the default\nvalue for ", " is \"none\".", "For a ", " element, the geometric attributes are ", ", ", ", and ", "\ngiving the coordinates of the center of the circle and the radius.  The center coordinates\nhave default values equal to zero.  For an ", " element, the attributes are\n", ", ", ", ", ", and ", ", where ", " and ", " give\nthe radii of the ellipse in the x- and y-directions.", "A ", " element is a little different.  It has attributes ", " and ", ",\nwith default values zero, to specify the location of the basepoint of the text.  However,\nthe text itself is given as the content of the element rather than as an attribute.  That is,\nthe element is divided into a start tag and an end tag, and the text that will appear in\nthe drawing comes between the start and end tags.  For example,", "The usual stroke and fill attributes apply to text, but text has additional style\nattributes.  The ", " attribute specifies the font itself.  Its value\ncan be one of the generic font names \"serif\", \"sans-serif\", \"monospace\", or the name of\na specific font that is available on the system.  The ", " can be a number\ngiving the (approximate) height of the characters in the coordinate system.  (Font size\nis subject to coordinate and modeling transforms like any other length.)  You can get\nbold and italic text by setting ", " equal to \"bold\" and\n", " equal to \"italic\".  Here is an example that uses all of these options,\nand applies some additional styles and a transform for good measure:", "SVG has some nice features for making more complex shapes.  The ", " element\nmakes it easy to create a polygon from a list of coordinate pairs.  For example,", "creates a five-sided polygon with vertices at (0,0), (100,0), (100,75), (50,100), and\n(0,75).  Every pair of numbers in the ", " attribute specifies a vertex.  The numbers\ncan be separated by either spaces or commas.  I've used a mixture of spaces and commas here to\nmake it clear how the numbers pair up.   Of course, you can add the usual style attributes\nfor stroke and fill to the polygon element.  A ", " is similar to a ", ",\nexcept that it leaves out the last line from the final vertex back to the starting vertex.\nThe difference only shows up when a polyline is stroked; a polyline is filled as if the\nmissing side were added.", "The ", " element is much more interesting. In fact, all of the other basic shapes,\nexcept text, could be made using path elements.  A path can consist of line segments,\n", ", and elliptical arcs (although I won't\ndiscuss elliptical arcs here).  The syntax for\nspecifying a path is very succinct, and it has some features that we have not seen before.\nA path element has an attribute named ", " that contains the data for the path.  The\ndata consists of one or more commands, where each command consists of a single letter followed\nby any data necessary for the command.  The moveTo, lineTo, cubic Bezier, and quadratic\nBezier commands that you are already familiar with are coded by the letters M, L, C, and Q.\nThe command for closing a path segment is Z, and it requires no data.\nFor example the path data \"M\u00a010\u00a020\u00a0L\u00a0100\u00a0200\" would draw a line segment\nfrom the point (10,20) to the point (100,200).  You can combine several connected line segments\ninto one L command.  For example, the ", " example given above could be created\nusing the ", " element", "The Z at the end of the data closes the path by adding the final side to the polygon.\n(Note that, as usual, you can use either commas or spaces in the data.)", "The C command takes six numbers as data, to specify the two control points and the final\nendpoint of the cubic Bezier curve segment.  You can also give a multiple of six values to get\na connected sequence of curve segements.  Similarly, the Q command uses four data values to\nspecify the control point and final endpoint of the quadratic Bezier curve segment.\nThe large, curvy, yellow shape shown in the picture earlier in this section was created\nas a path with two line segments and two Bezier curve segments:", "SVG paths add flexibility by defining \"relative\" versions of the path commands,\nwhere the data for the command is given relative to the current position.\nA relative move command, for example, instead of telling ", " to move,\ntells ", " to move from the current position.  The names of the \nrelative versions of the path commands are lower case letters instead of upper case.\n\"M\u00a010,20\" means to move to the point with coordinates (10,20), while\n\"m\u00a010,20\" means to move 10 units horizontally and 20 units vertically\nfrom the current position.  Similarly, if the current position is (", "), then\nthe command \"l\u00a03,5\", where the first character is a lower case L, draws a line from (", ") to\n(", "+3,", ").", "SVG would not be a very interesting language if it could only work with\nindividual simple shapes.  For complex scenes, we want to be able to do\n", ", where objects can be constructed from\nsub-objects, and a transform can be applied to an entire complex object.\nWe need a way to group objects so that they can be treated as a unit.\nFor that, SVG has the ", " element.  The content of a ", "\nelement is a list of shape elements, which can be simple shapes or\nnested ", " elements.", "You can add style and ", " attributes to a ", " element.\nThe main point of grouping is that a group can be treated as a single\nobject.  A ", " attribute in a ", " will transform the\nentire group as a whole.  A style attribute, such as ", " or\n", ", on a ", " element will set a default value \nfor the group, replacing the current default.  Here is an example:", "The nested shapes use fill=\"none\" stroke=\"black\" stroke-width=\"2\" for the\ndefault values of the attributes.  The default can be overridden by specifying\na different value for the element, as is done for the stroke-width of the\n", " element in this example.  Setting transform=\"scale(1,\u22121)\"\nfor the group flips the entire image vertically.  I do this only because\nI am more comfortable working in a coordinate system in which y increases\nfrom bottom-to-top rather than top-to-bottom.  Here is the simple line\ndrawing of a face that is produced by this group:", "\n", "Now, suppose that we want to include multiple copies of an object in\na scene.  It shouldn't be necessary to repeat the code for drawing the object.\nIt would be nice to have something like reusable subroutines.  In fact,\nSVG has something very similar: You can define reusable objects inside a\n", " element.  An object that is defined inside ", " is\nnot added to the scene, but copies of the object can be added to the scene\nwith a single command.  For this to work, the object must have an ", " attribute\nto identify it.  For example, we could define an object that looks like a plus sign:", "A ", " element can then be used to add a copy of the plus sign\nobject to the scene.  The syntax is", "The value of the ", " attribute must be the ", " of the object,\nwith a \"#\" character added at the beginning. (Don't forget the\u00a0#.  If you leave it out,\nthe ", " element will simply be ignored.)  You can add a ", " attribute\nto the ", " element to apply a transformation to the copy of the object.  You can also apply\nstyle attributes, which will be used as default values for the attributes in the copy.  For\nexample, we can draw several plus signs with different transforms and stroke widths:", "Note that we can't change the color of the plus sign, since it already specifies\nits own stroke color.", "An object that has been defined in the ", " section can also be used\nas a sub-object in other object definitions.  This makes it possible to create\na hierarchy with multiple levels.  Here is an example from ", "\nthat defines a \"wheel\" object, then uses two copies of the wheel as sub-objects in a \n\"cart\" object:", "The SVG file goes on to add one copy of the wheel and four copies of the\ncart to the image.  The four carts have different colors and transforms.\nHere is the image:", "\n", "SVG has a number of advanced features that I won't discuss here, but I do want to\nmention one: ", ".  It is possible to animate almost any property\nof an SVG object, including geometry, style, and transforms.  The syntax for animation\nis itself fairly complex, and I will only do a few examples.  But I will tell you enough\nto produce a fairly complex hierarchical animation like the \"cart-and-windmills\"\nexample that was discussed and used as a demo in ", ".\nAn SVG version of that animation can be found in ", ".\nHere is what it looks like, although some web browsers might show it as a static\nimage instead of an animation:", "\n", "Many attributes of a shape element can be animated by adding an ", "\nelement to the content of the shape element.   Here is an example that makes a rectangle\nmove across the image from left to right:", "Note that the ", " is nested inside the ", ".\nThe ", " attribute tells which attribute of the ", "\nis being animated, in this case,\u00a0", ".  The ", " and ", " attributes\nsay that ", " will take on values from 0 to 430.  The ", " attribute is the\n\"duration\", that is, how long the animation lasts; the value \"7s\" means \"7 seconds.\"\nThe attribute ", "=\"indefinite\" means that after the animation completes,\nit will start over, and it will repeat indefinitely, that is, as long as the image is\ndisplayed.  If the ", " attribute is omitted, then after the animation\nruns once, the rectangle will jump back to its original position and remain there.\nIf ", " is replaced by ", "=\"freeze\", then after the animation runs,\nthe rectangle will br frozen in its final position, instead of jumping back to the starting\nposition.  The animation begins when the image first loads.  If you want the animation to\nstart at a later time, you can add a ", " attribute whose value gives the time\nwhen the animation should start, as a number of seconds after the image loads.", "What if we want the rectangle to move back and forth between its initial and final\nposition?  For that, we need something called ", ",\nwhich is an important idea in its own right.  The ", " and ", " attributes\nallow you to specify values only for the beginning and end of the animation.  In a keyframe\nanimation, values are specified at additional times in the middle of the animation.\nFor a keyframe animation in SVG, the ", " and ", " attributes are replaced\nby ", " and ", ".  Here is our moving rectangle example,\nmodified to use keyframes:", "The ", " attribute is a list of numbers, separated by semicolons.\nThe numbers are in the range 0 to 1, and should be in increasing order.  The first number\nshould be 0 and the last number should be 1.  A number specifies a time during the animation,\nas a fraction of the complete animation.  For example, 0.5 is a point half-way through the\nanimation, and 0.75 is three-quarters of the way.  The ", " attribute is a list\nof values, with one value for each key time.  In this case, the value for ", " is\n0 at the start of the animation, 430 half-way through the animation, and 0 again at the\nend of the animation.  Between the key times, the value for ", " is obtained by interpolating\nbetween the values specified for the key times.  The result in this case is that the rectangle\nmoves from left to right during the first half of the animation and then back from right to\nleft in the second half.", "Transforms can also be animated, but you need to use the ", "\ntag instead of ", ", and you need to add a ", " attribute to specify\nwhich transform you are animating, such as \"rotate\" or \"translate\".  Here, for example,\nis a transform animation applied to a group:", "The animation shows a growing \"tree\" made from a green triangle and a brown rectangle.\nIn the animation, the transform goes from ", "(0,0) to ", "(0.4,0.7).\nThe animation starts 3 seconds after the image loads and lasts 15 seconds.  At the end\nof the animation, the tree freezes at its final scale.  The ", " attribute\non the ", " element specifies the scale that is in effect until the animation\nstarts.  (A scale factor of 0 collapses the object to size zero, so that it is invisible.)\nYou can find this example, along with a moving rectangle and a keyframe animation, in \nthe sample file ", ". Here is the\nanimation itself.  To see the growing trees, you might have to reload this page or view\nthe image in a separate window:", "\n", "You can create animated objects in the ", " section of an SVG file,\nand you can apply animation to ", " elements.  This makes it possible\nto create hierarchical animations.  Here is a simple example:", "\n", "The example shows a rotating hexagon with a rotating square at each vertex of the\nhexagon.  The hexagon is constructed from six copies of one object, with a different rotation\napplied to each copy.  (A copy of the basic object is shown in the image to the right of the\nhexagon.)  The square is defined as an animated object with its own rotation.  It is used\nas a sub-object in the hexagon.  The rotation that is applied to the hexagon applies to the\nsquare, on top of its own built-in rotation.  That's what makes this an example of\nhierarchical animation.", "If you look back at the ", " \nexample now, you can probably see how to do the animation.  Don't forget to check out the source code,\nwhich is surprisingly short!"], "section_id": "Section 2.7", "chapter_id": "Chapter 2", "chapter_title": "Two-Dimensional Graphics", "section_title": "SVG: A Scene Description Language"},
{"content": ["OpenGL is an ", " for graphics only, with no support for things like\nwindows or events.  OpenGL depends on external mechanisms to\ncreate the drawing surfaces on which it will draw.  Windowing APIs\nthat support OpenGL often do so as one library among many others that\nare used to produce a complete application.  We will look at\ntwo cross-platform APIs that make it possible to use OpenGL\nin applications, one for C/C++ and one for Java.", "For simple applications written in C or C++, one possible\nwindowing API is ", " (OpenGL Utility Toolkit).  GLUT is a very small\nAPI.  It is used to create windows that serve as\nsimple frames for OpenGL drawing surfaces.  It has support for\nhandling mouse and keyboard events, and it can do basic animation.\nIt does not support controls such as buttons or input fields,\nbut it does allow for a menu that pops up in response to\na mouse action.  You can find information about the GLUT API at", "\n", "\n", "If possible, you should use FreeGLUT, which is compatible with GLUT but has\na few extensions and a fully open source license.   See", "\n", "\n", "\n", " (Java OpenGL) is a collection of classes that make it\npossible to use OpenGL in Java applications.  JOGL is integrated\ninto Swing and AWT, the standard Java graphical user interface APIs.\nWith JOGL, you can create Java GUI components on which\nyou can draw using OpenGL.  These OpenGL components can be\nused in any Java application, in much the same way that you\nwould use a ", "\nor ", " as a drawing surface.\nLike many things Java, JOGL is immensely complicated.  We will use it\nonly in fairly simple applications.\nJOGL is not a standard part of Java.  It's home web site is", "\n", "\n", "This section contains information to get you started using GLUT and JOGL, assuming\nthat you already know how the basics of programming with C and Java.  It also briefly\ndiscusses ", ", a JavaScript library that I have written to simulate the subset\nof OpenGL 1.1 that is used in this book.", "To work with GLUT, you will need\na C compiler and copies of the OpenGL and GLUT (or FreeGLUT)\ndevelopment libraries.  I can't tell you exactly that means on\nyour own computer.  On my computer, which runs Linux Mint, for example,\nthe free C compiler gcc is already available.  To do OpenGL\ndevelopment, I installed several packages, including\n", " and ", ".\n(Mesa is a Linux implementation of OpenGL.)  If ", " contains\na complete C program that uses GLUT, I can compile it using a command such as\n", "The \"-o glutprog\" tells the compiler to use \"glutprog\" as the\nname of its output file, which can then be run as a normal executable file;\nwithout this option, the executable file would be named \"a.out\".\nThe \"-lglut\" and \"-lGL\" options tell the compiler to link the program with the GLUT and OpenGL libraries.\n(The character after the \"-\" is a lower case \"L\".)\nWithout these options, the compiler won't recognize any GLUT or OpenGL functions.  If the program\nalso uses the ", " library, compiling it would require the option \"-lGLU, and if it uses\nthe math library, it would need the option \"-lm\".  If a program requires additional .c files,\nthey should be included as well.  For example, the sample program\n", " depends on ", ", and it\ncan be compiled with the Linux gcc compiler using the command:", "The sample program ", " can be used as a starting\npoint for writing programs that use GLUT.  While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "The GLUT library makes it easy to write basic OpenGL applications in\u00a0C.  GLUT\nuses event-handling functions.  You write functions to handle events that occur\nwhen the display needs to be redrawn or when the user clicks the mouse or presses a key\non the keyboard.", "To use GLUT, you need to include the header file ", " (or ", ")\nat the start of any source code file that uses it, along with the general OpenGL header file,\n", ".  The header files should be installed in a standard location, in a folder named ", ".\nSo, the program usually begins with", "On my computer, saying ", " actually includes the subset\nof FreeGLUT that corresponds to GLUT.  To get access to all of FreeGLUT, I would\nsubstitute ", ".  Depending on the features that it uses,\na program might need other header files, such as ", " \nand ", ".", "The program's ", "() function must contain some code to initialize GLUT, to\ncreate and open a window, and to set up event handling by registering the functions that\nshould be called in response to various events.  After this setup, it must\ncall a function that runs the GLUT event-handling loop.  That function\nwaits for events and processes them by calling the functions that have been registered\nto handle them.  The event loop runs until the program ends, which happens when\nthe user closes the window or when the program calls the standard ", "() function.", "To set up the event-handling functions,\nGLUT uses the fact that in C, it is possible to pass a function name as a parameter\nto another function.  For example, if ", "() is the function that \nshould be called to draw the content of the window, then the\nprogram would use the command", "to install this function as an event handler for\ndisplay events. A display event occurs when the contents of the window need to be redrawn, including\nwhen the window is first opened.\nNote that ", " must have been previously defined, as a function with no parameters:", "Keep in mind that it's not the name of this function that makes it an OpenGL display\nfunction.  It has to be set as the display function by calling ", "(", ").\nAll of the GLUT event-handling functions work in a similar way (except many of them do need\nto have parameters).", "There are a lot of possible event-handling functions, and I will only cover some of\nthem here.  Let's jump right in and look at a possible ", "() routine for a GLUT\nprogram that uses most of the common event handlers:", "The first five lines do some necessary initialization, the next seven lines install event\nhandlers, and the call to ", "() runs the GLUT event loop.  I will discuss all of\nthe functions that are used here.  The first GLUT function call must be ", ",\nwith the parameters as shown.  (Note that ", " and ", "\nrepresent command-line arguments for the program.  Passing them to ", " allows\nit to process certain command-line arguments that are recognized by GLUT.  I won't discuss those\narguments here.)  The functions ", " and ", "\ndo the obvious things; size is given in pixels, and \nwindow position is given in terms of pixel coordinates on the computer\nscreen, with (0,0) at the upper left corner of the screen.  The function ", "\ncreates the window, but note that nothing can happen in that window until ", "\nis called.  Often, an additional, user-defined function is called in ", "() to do\nwhatever initialization of global variables and OpenGL state is required by the program.\nOpenGL initialization can be done after calling ", " and before\ncalling ", ".  Turning to the other functions used in ", "(),", "\n", " \u2014 Must be called to\ndefine some characteristics of the OpenGL drawing context.  The parameter specifies\nfeatures that you would like the OpenGL context to have.  The features are represented by\nconstants that are OR'ed together in the parameter.  ", " says that a depth buffer\nshould be created; without it, the depth test won't work.  If you are doing 2D graphics, you\nwouldn't include this option.  ", " asks for ", ", \nwhich means that drawing is actually done off-screen, and the\noff-screen copy has to copied to the screen to be seen.  The copying is done by\n", ", which ", " be called at the end of the display function.\n(You can use ", " instead of ", " to get ", "; \nin that case, you have to call ", "() at the end of the display function instead\nof ", "().  However, all of the examples in this book use ", ".)", "\n", " \u2014 The display function\nshould contain OpenGL drawing code that can completely redraw the scene.  This is\nsimilar to ", "() in Java.\nThe display function can have any name, but it must be declared as a void\nfunction with no parameters: ", "().", "\n", " \u2014 The reshape function\nis called when the user changes the size of the window.  Its parameters tell the\nnew width and height of the drawing area:", "For example, you might use this method to set up the projection transform, if the\nprojection depends only on the window size.  A reshape function is not required, but\nif one is provided, it should always set the\nOpenGL ", ", which is the part of the window that\nis used for drawing.  Do this by calling", "The viewport is set automatically if no reshape function is specified.", "\n", " \u2014 The keyboard function is\ncalled when the user types a character such as 'b' or 'A' or a space.  It is not called\nfor special keys such as arrow keys that do not produce characters when pressed.\nThe keyboard function has a parameter of type ", " which\nrepresents the character that was typed.  It also has two ", " parameters \nthat give the location of the mouse when the key was pressed, in pixel coordinates\nwith (0,0) at the upper left corner of the display area.  So, the definition of\nthe key function must have the form:", "Whenever you make any changes to the program's data that require the display to be redrawn,\nyou should call ", "().  This is similar to calling ", "() in\nJava.  It is better to call ", "()\nthan to call the display function directly.  (I also note that it's possible to\ncall OpenGL drawing commands directly in the event-handling functions, but it probably only makes\nsense if you are using single buffering; if you do this, call ", "()\nto make sure that the drawing appears on the screen.)", "\n", " \u2014 The \"special\"\nfunction is called when the user presses certain special keys, such as an arrow\nkey or the Home key.  The parameters are an integer code for the key that was pressed, plus the\nmouse position when the key was pressed:", "GLUT has constants to represent the possible key codes, including\n", ", ", ", ", ", and ", "\nfor the arrow keys and ", " for the Home key. For example,\nyou can check whether the user pressed the left arrow key by testing\n", "\u00a0", ".", "\n", " \u2014 The mouse function is\ncalled both when the user presses and when the user releases a button on the mouse, with a parameter to tell\nwhich of these occurred.  The function will generally look like this:", "The first parameter tells which mouse button was pressed or released; its\nvalue is the constant ", " for the left, ", " for the \nmiddle, and ", " for the right mouse button.  The other\ntwo parameters tell the position of the mouse.  The mouse position\nis given in pixel coordinates with (0,0) in the top left corner of the display area and\nwith y increasing from top to bottom.", "\n", " \u2014 The motion function\nis called when the user moves the mouse while dragging, that is, while a mouse button\nis pressed.  After the user presses the mouse in the OpenGL window, this function will\ncontinue to be called even if the mouse moves outside the window, and the mouse\nrelease event will also be sent to the same window.  The function has two parameters\nto specify the new mouse position:", "\n", " \u2014 The idle function is called by the\nGLUT event loop whenever there are no events waiting to be processed.  The\nidle function has no parameters.  It is called as often as possible, not at\nperiodic intervals.  GLUT also has a timer function, which schedules some function to be\ncalled once, after a specified delay.  To set a timer, call", "and define ", " as", "The parameter to ", " when it is called will be the same integer that was passed as\nthe third parameter to ", ".  If you want to use ", "\nfor animation, then ", " should end with another call to ", ".", "A GLUT window does not have a menu bar, but it is possible to add a hidden popup menu to the window.\nThe menu will appear in response to a mouse click on the display.  You can set whether it\nis triggered by the left, middle, or right mouse button.", "A menu is created using the function ", ",\nwhere the parameter is the name of a function that will be called when the user\nselects a command from the menu.  The function must be defined with a parameter of\ntype ", " that identifies the command that was selected:", "Once the menu has been created, commands are added to the menu by calling the function\n", "(", ").  The first parameter is the string that\nwill appear in the menu.  The second is an ", " that identifies the\ncommand; it is the integer that will be passed to the menu-handling function when\nthe user selects the command from the menu.", "Finally, the function ", "(", ") attaches the menu to the\nwindow.  The parameter specifies which mouse button will trigger the menu.  Possible\nvalues are ", ", ", ", and ", ".\nAs far as I can tell, if a mouse click is used to trigger the popup menu, than the same\nmouse click will ", " also produce a call to the mouse-handler function.", "Note that a call to ", " doesn't mention the menu, and a\ncall to ", " doesn't mention either the menu or the window.\nWhen you call ", ", the menu that is created becomes the \"current\nmenu\" in the GLUT state.  When ", " is called, it adds a command\nto the current menu.  When ", " is called, it attaches the current\nmenu to the current window, which was set by a call to ", ".\nAll this is consistent with the OpenGL \"state machine\" philosophy, where functions\nact by modifying the current state.", "As an example, suppose that we want to let the user set the background color for\nthe display.  We need a function to carry out commands that we will add to the menu.  For example,\nwe might define", "We might have another function to create the menu.  This function would be called\nin ", "(), after calling ", ":", "It's possible to have submenus in a menu.  I won't discuss the procedure here, but you can look\nat the sample program ", " for an example of using submenus.", "In addition to window and event handling, GLUT includes some functions for drawing basic 3D shapes\nsuch as spheres, cones, and ", ".  \nIt has two functions for each shape, a \"solid\" version that draws\nthe shape as a solid object, and a ", " version that draws \nsomething that looks like it's made of wire mesh.  (The wireframe is produced by drawing \njust the outlines of the polygons that make up the object.)  For example, the function", "draws a solid sphere with the given radius, centered at the origin.  Remember that this is\njust an approximation of a sphere, made up of polygons.  For the approximation, the sphere is divided by\nlines of longitude, like the slices of an orange, and by lines of latitude, like a stack of disks.\nThe parameters ", " and ", " tell how many subdivisions to use.  Typical values\nare 32 and 16, but the number that you need to get a good approximation for a sphere depends on the\nsize of the sphere on the screen.  The function ", " has the same parameters but\ndraws only the lines of latitude and longitude.  Functions for a cone, a cylinder, \nand a ", " (doughnut) are similar:", "For a torus, the ", " is the size of the doughnut hole.  The function", "draws a cube of a specified size.\nThere are functions for the other regular polyhedra that have no parameters and draw the \nobject at some fixed size:  ", "(), ", "(),\n", "(), and ", "().\nThere is also ", "(", ") that draws a famous object that is often used as an\nexample.  Here's what the teapot looks like:", "\n", "Wireframe versions of all of the shapes are also available.  For example,\n", "(", ") draws a wireframe teapot.  Note that \nGLUT shapes come with ", " that\nare required for lighting calculations.  However, except for the teapot, they do\nnot come with ", ", which are required for applying\ntextures to objects. ", "GLUT also includes some limited support for drawing text in an OpenGL drawing\ncontext.  I won't discuss that possibility here.  You can check the API\ndocumentation if you are interested, and you can find an example in the\nsample program ", ".", "JOGL is a framework for using OpenGL in Java programs.  It is a large and complex API that\nsupports all versions of OpenGL, but it is fairly easy to use for basic applications.\nIn my examples and discussion, I will be using JOGL\u00a02.3, the latest version\nas of March, 2015.  Note that version 2.3 is not fully compatible with earlier versions.\n(Hopefully, later versions will remain compatible with 2.3.)", "The sample program ", " can be used as a starting\npoint for writing OpenGL programs using JOGL. While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "To use JOGL, you will need two .jar files containing the Java classes for JOGL:\n", " and ", ".  In addition, you will\nneed two native library files.  A native library is\na collection of routines that can be called from Java but are not written in Java.  Routines\nin a native library will work on only kind of computer; you need a different native library\nfor each type of computer on which your program is to be used.  The native libraries for\nJOGL are stored in additional .jar files, which are available in several versions for\ndifferent computers.  For example, for 64-bit Linux, you need\n", " and ", ".\nFor 32-bit Linux, the files are\n", " and ", ".\nIt is unfortunate that there are different versions for 64 and 32 bit operating systems, since\nmany people don't know which they are using.  However, if you are in doubt, you can get\nboth; JOGL will figure out which of the two to use.\nFor Mac\u00a0OS, you need\n", " and ", ".\nFor 64-bit Windows, the files are\n", " and ", ".", "You can get the jar files from the JOGL web site, ", ".\nI extracted them from the very large (54 megabyte) archive file", "\n\n", "\n\n", "I have also made the necessary files available on my own web site, at", "\n\n", "\n\n", "JOGL is open-source, and the files are freely redistributable, according to their\n", ".", "To do JOGL development, you should create a directory somewhere on your computer to hold the jar\nfiles.  Place the two JOGL jar files in that directory, along with the two native library jar files\nfor your platform.  (Having extra native library jar files doesn't hurt, as long as you have\nthe ones that you need.)", "It is possible to do JOGL development on the command line.  You have to tell the\n", " command where to find the two JOGL jar files. You do that in the\nclasspath (\"-cp\") option to the ", " command.  For example, if you are working\nin Linux or MacOS, and if the jar\nfiles happen to be in the same directory where you are working, you might say:", "It's similar for Windows, except that the classpath uses a \";\" instead of a \":\" to\nseparate the items in the list:", "There is an essential period at the end of the classpath, which makes it possible for Java to\nfind .java files in the current directory.\nIf the jar files are not in the current directory,\nyou can use full path names or relative path names to the files.  For example,", "Running a program with the ", " command is exactly similar. For example:", "Note that you don't have to explicitly reference the native library jar files.\nThey just have to be in the same directory with the JOGL jar files.", "I do most of my Java development using the Eclipse IDE (", ").\nTo do development with JOGL in Eclipse, you will have to configure Eclipse\nwith information about the jar files.  To do that, start up Eclipse.  You want to\ncreate a \"User Library\" to contain the jar files:\nOpen the Eclipse Preferences window, and select \"Java\" / \"Build\u00a0Path\" / \"User\u00a0Libraries\"\non the left.  Click the \"New\" button on the right.  Enter \"JOGL\" (or any name you like) as the\nname of the user library.  Make sure that the new user library is selected in the\nlist of libraries, then click the \"Add External Jars\" button.  In the file selection box,\nnavigate to the directory that contains the JOGL jar files, and select the two jar files that\nare needed for JOGL, ", " and ", ".  \n(Again, you do not need to add the native libraries; they just need to be in the same directory\nas the JOGL jar files.)  Click \"Open.\"  The selected\njars will be added to the user library. (You could also add them one at a time, if you don't\nknow how to select multiple files.)  It should\nlook something like this:", "\n", "Click \"OK.\"  The user library has been created. You will only have to do this\nonce, and then you can use it in all of your JOGL projects.", "Now, to use OpenGL in a project, create a new Java project as usual in Eclipse.\nRight-click the project in the Project Explorer view, and select \"Build\u00a0Path\" /\n\"Configure\u00a0Build\u00a0Path\" from the menu.  You will see the project Properties\ndialog, with \"Build Path\" selected on the left.  (You can also access this through the\n\"Properties\" command in the \"Project\" menu.)  Select \"Libraries\" at the top of the\nwindow, and then click the \"Add\u00a0Library\" button.  In the popup window, select \"User\u00a0Library\"\nand click \"Next.\"  In the next window, select your JOGL User Library and click \"Finish.\"\nFinally, click \"OK\" in the main Properties window.  Your project should now be set up\nto do JOGL development.  You should see the JOGL User Library listed as part of the\nproject in the Project Explorer.  Any time you want to start a new JOGL project, you can go through\nthe same setup to add the JOGL User Library to the build path in the project.", "With all that setup out of the way, it's time to talk about actually\nwriting OpenGL programs with Java.  With JOGL,\nwe don't have to talk about mouse and keyboard handling or animation, since that can be done\nin the same way as in any Java program.  You will only need to know about a few classes from\nthe JOGL API.", "First, you need a GUI component on which you can draw using OpenGL.  For that, you\ncan use ", ", which is a subclass of ", ".\n(", " is for use in programs based on the Swing API; an alternative\nis ", ", which is a subclass of the older AWT class\n", ".)  The class is defined in the package ", ".\nAll of the other classes that we will need for basic OpenGL programming \nare in the package ", ".", "JOGL uses Java's event framework to manage OpenGL drawing contexts, and it defines a\ncustom event listener interface, ", ", to manage\nOpenGL events.  To draw on a ", " with OpenGL, you need to\ncreate an object that implements the ", " interface, and\nregister that listener with your ", ".  The ", "\ninterface defines the following methods:\n", "The ", " parameter in these methods tells which OpenGL drawing surface\nis involved.  It will be a reference to the ", ".\n(", "  is an interface that is implemented by\n", " and other OpenGL drawing surfaces.)\nThe ", "() method is a place to do OpenGL initialization.  (According to the\ndocumentation, it can actually be called several times, if the OpenGL context\nneeds to be recreated for some reason. So ", "() should not be used to\ndo initialization that shouldn't be done more than once.) The \n", "() method will be called to give you a chance to\ndo any cleanup before the OpenGL drawing context is destroyed.\nThe ", "() method is called when the window first opens and\nwhenever the size of the ", " changes.\nOpenGL's ", "() function is called automatically before ", "()\nis called, so you won't need to do it yourself.  Usually, you won't need to write\nany code in ", "() or ", "(), but they have to be there to\nsatisfy the definition of the ", " interface.", "The ", "() method is where the actual drawing is done and where you\nwill do most of your work.  It should ordinarily clear the drawing area and completely redraw the scene.\nTake a minute to study an outline for a minimal JOGL program.  It creates a\n", " which also serves as the\n", ":", "At this point, the only other thing you need to know is how to use OpenGL\nfunctions in the program.  In JOGL, the OpenGL\u00a01.1 functions are collected into\nan object of type ", ".  (There are different classes\nfor different versions of OpenGL;  ", " contains\nOpenGL\u00a01.1 functionality, along with later versions that are compatible with 1.1.)\nAn object of type ", " is an OpenGL graphics context,\nin the same way that an object of type ", "\nis a graphics context for ordinary Java 2D drawing.  The statement\n", "in the above program obtains the drawing context for\nthe ", ", that is, for the\n", " in that program.  The name of the\nvariable could, of course, be anything, but ", " or ", " is conventional.", "For the most part, using OpenGL functions in JOGL is the same as in C,\nexcept that the functions are now methods in the object ", ".  For example,\na call to ", "(", ") becomes", "The redundant \"gl.gl\" is a little annoying, but you get used to it.  OpenGL constants\nsuch as ", " are static members of ", ", so that, for\nexample, ", " becomes ", " in JOGL.\nParameter lists for OpenGL functions\nare the same as in the C API in most cases.  One exception is for functions such as ", "()\nthat take an array/pointer parameter in C.  In JOGL, the parameter becomes an ordinary\nJava array, and an extra integer parameter is added to give the position of the data in\nthe array.  Here, for example, is how one might draw a triangle in JOGL, with all the\nvertex coordinates in one array:", "The biggest change in the JOGL API is the use of ", "\ninstead of arrays in functions such as ", ".  This is discussed\nin ", ".  We will see in ", " that texture images also get special\ntreatment in JOGL.", "The JOGL API includes a class named ", " that makes GLUT's\nshape-drawing functions available in Java.  (Since you don't need GLUT's window or event functions\nin Java, only the shape functions are included.)  Class ", "\nis defined in the package ", ".\nTo draw shapes using this class, you need\nto create an object of type GLUT.  It's only necessary to make one of these for use in a program:", "The methods in this object include all the shape-drawing functions from the GLUT C API,\nwith the same names and parameters.  For example:", "(I don't know why these are instance methods in an object rather than\nstatic methods in a class; logically, there is no need for the object.)", "The GLU library is available through the class ", ",\n and it works similarly to GLUT.   That is, you have to create an object of type\n ", ", and the GLU functions will be available as methods\n in that object.  We have encountered GLU only for the functions ", "\n and ", ", which are discussed in ", ".\n For example,", "The JavaScript library ", " was written to accompany and support this textbook.\nIt implements the subset of OpenGL 1.1 that is discussed in ", " and\n", ", except for display lists (", ").\nIt is used in the demos that appear in\nthose chapters.  Many of the sample programs that are discussed in those chapters are available\nin JavaScript versions that use glsim.js.", "If you would like to experiment with OpenGL 1.1, \nbut don't want to go through the trouble of setting up a C or Java environment that supports \nOpenGL programming, you can consider writing your programs as web pages using glsim.js.\nNote that glsim is meant for experimentation and practice only, not for serious applications.", "The OpenGL API that is implemented by glsim.js is essentially the same as the C API, although \nsome of the details of semantics are different.  Of course the techniques for creating a\ndrawing surface and an OpenGL drawing context are specific to JavaScript and differ from\nthose used in GLUT or JOGL.", "To use glsim.js, you need to create an ", " document with a <canvas> element\nto serve as the drawing surface.  The HTML file has to import the script; if glsim.js is in the\nsame directory as the HTML file, you can do that with", "To create the OpenGL drawing context, use the JavaScript command", "where ", " is either a string giving the ", " of the <canvas> element or\nis the JavaScript ", " object corresponding to the <canvas> element. Once you\nhave created the drawing context in this way, any OpenGL commands that you give will apply to\nthe canvas.  To run the program, you just need to open the HTML document in a web browser that\nsupports ", ".", "The easiest way to get started programming is to modify a program that already exists.\nThe sample program ", ", from ", "\nis a very minimal example of using glsim.js.\nThe sample web page ", " can be used as a starting\npoint for writing longer programs that use glsim.js.  It provides a framework for doing OpenGL drawing,\nwith support for animation and mouse and keyboard events.  The code contains comments that tell \nyou how to use it.  Some documentation for the glsim.js library can be found in\n", "."], "section_id": "Section 3.6", "chapter_id": "Chapter 3", "chapter_title": "OpenGL 1.1: Geometry", "section_title": "Using GLUT and JOGL"},
{"content": ["We will finish this chapter with a look at several additional features\nof ", ".  In the process, you will learn about some new aspects\nof 3D graphics.", "We start with a simple example: ", ".\nAnaglyph refers to 3D images that are meant to be viewed through red/cyan (or red/green\nor red/blue) glasses.  The image contains two copies of the scene, one as viewed from the\nleft eye, drawn using only red, and one as viewed from the right eye, drawn using only\ngreen and blue.  When the image is viewed through red/cyan glasses, the left eye\nsees only the left-eye view of the scene and the right eye sees only the right-eye\nview of the scene.  The result is what looks like a real 3D scene, with depth perception.\nThe result isn't perfect, because screen colors and the color filters in the glasses\naren't perfect.  You will see some \"ghosts,\" where part of the left-eye image gets through\nto the right eye or vice versa.  Anaglyph stereo works best for monochrome images\nthat contain only shades of gray.  Color images are more problematic.  There are \nseveral ways of separating the colors into left and right-eye images, none of them \nperfect. The one used in ", " works pretty well except for pure shades of red,\nwhich will only be visible to one eye.", "Here is an example, from the sample program ", ",\nwhich is identical to ", ", except for the\nuse of anaglyph rendering.  For a proper 3D view, you need to look at this image\nthrough red/cyan glasses:", "\n", "In ", ", anaglyph stereo is implemented by the class\n", ".  This class is not defined\nin the main ", " JavaScript file; it's defined in a separate file named\n", ", \nwhich can be found in the ", " download in the folder\n", ".  To use it, you need to include that file in\na ", " element in your HTML file.", "The class is very easy to use.  The constructor for an object of type\n", " takes an ordinary\n", " as a parameter. The size of the\ndrawing area is also specified in the constructor (or call the object's \n", " method later to specify the size). For example,:", "Once you have an ", ",\nyou can use it in place of the ", "\nto render the image:", "That's all there is to it!  The scene will be rendered in anaglyph stereo.", "Most real programs require some kind of user interaction.  For a web application,\nof course, the program can get user input using HTML widgets such as buttons and text input boxes.\nBut direct mouse interaction with a 3D world is more natural in many programs.", "The most basic example is using the mouse to rotate the scene.  In ", ",\nrotation can be implemented using the class ", " or the\nclass ", ".  The main difference is that with ", ",\nthe rotation is constrained so that the positive ", "-axis is always the up direction\nin the view.  ", ", on the other hand, allows completely free\nrotation.  These classes are used in my ", " examples and demos that\nimplement mouse rotation.", "The two control classes are not part of the main ", " JavaScript file.\nTo use one of them, you need the JavaScript file ", "\nor ", ", which can be\nfound in the folder ", " in the ", " download.", "The two classes are used in a similar way.  I will discuss ", ".\nIn my examples, I create a camera and move it away from the origin. I usually add a light object\nto the camera object, so that the light will move along with the camera, providing some\nillumination to anything that is visible to the camera.  The ", "\nobject is used to rotate the camera around the scene.  The constructor for the\ncontrol object has two parameters, the camera and the canvas on which the scene is rendered.\nHere is typical setup:", "The constructor installs listeners on the ", " so that the controls can respond\nto mouse events.  It is also necessary to call", "just before each rendering of the scene.  This method adjusts the camera rotation.\nThe controls are designed to be used in a scene that is constantly being animated,\nso that the render function is being called regularly.  No change will be visible until\nthe render function is called.  (In my programs that don't use animation, I add an extra\nlistener to make sure that ", "() and ", "() are called whenever\nthe mouse is dragged.  See, for example, the source code for \n", " to see how it's done.)", "The controls can also do \"panning\" (dragging the scene in the plane of the screen)\nwith the right mouse button and \"zooming\" (scaling the scene) with the middle mouse button\nor scroll wheel.  I generally turn off those features by setting", "A much more interesting form of mouse interaction is to let the user select objects\nin the scene by clicking on them.  The problem is to determine which object the\nuser is clicking.  The general procedure is something like this:  Follow a ray from\nthe camera through the point on the screen where the user clicked and find the first\nobject in the scene that is intersected by that ray.  That's the object that is visible\nat the point where the user clicked.  Unfortunately, the procedure involves a lot\nof calculations.  Fortunately, ", " has a class that can do the work for\nyou:  ", ".", "A ", " can be used to find intersections of a ray with\nobjects in a scene.  (A ray is just half of a line, stretching from some given starting\npoint in a given direction towards infinity.)  You can make one raycaster object to use\nthroughout your program:", "To tell it which ray to use, you can call", "where both of the parameters are of type ", ".\nTheir values are in terms of ", ", the same coordinate\nsystem that you use for the scene as a whole.  The ", " must\nbe a ", ", with length equal to one.\nFor example, suppose that you want to fire a laser gun....  The \n", " is the location of the gun, and the ", "\nis the direction that the gun is pointing.  Configure the raycaster \nwith those parameters, and you can use it to find out what object is struck\nby the laser beam.", "Alternatively, and more conveniently\nfor processing user input, you can express the ray in terms of the camera and\na point on the screen:", "The ", " are given as a ", " expressed\nin ", ".  This means the horizontal coordinate ranges from\n\u22121 on the left edge of the viewport to 1 on the right, and the vertical coordinate ranges from\n\u22121 at the bottom to 1 on the top.  (Clip coordinates are called \"normalized device\ncoordinates\" in ", ".)  So, we need to convert from pixel coordinates on a\ncanvas to clip coordinates.  Here's one way to do it, given a mouse event, ", ":", "Once you have told the raycaster which ray to use, it is ready to find\nintersections of that ray with objects in the scene.  This can be\ndone with the function", "The first parameter is an array of ", ".\nThe raycaster will search for intersections of its current ray with objects in the array.\nIf the second parameter is ", ", it will also search descendants of those\nobjects in the ", "; it it is ", " or is omitted, then only the\nobjects in the array will be searched.  For example, to search for intersections\nwith all objects in the scene, use", "The return value from ", " is an array of JavaScript objects.\nEach item in the array represents an intersection of the ray with an ", ".\nThe function finds all such intersections, not just the first.  If no intersection is\nfound, the array is empty.  The array is sorted by increasing distance from the starting \npoint of the ray.  If you just want the first intersection, use the first element of the array.", "An element in the array is an object whose properties contain information about the\nintersection.  Suppose that ", " is one of the array elements.  Then the most useful\nproperties are:  ", ", which is the ", " that \nwas intersected by the ray; and ", ", which  is the point of intersection, \ngiven as a ", " in world coordinates.  That information is\nenough to implement some interesting user interaction.", "The following demo\nuses some basic mouse interaction to let the user edit a scene.  The scene shows\na number of tapered yellow cylinders standing on a green base.  The user can\ndrag the cylinders, add and delete cylinders, and rotate the scene.  A set of \nradio buttons lets the user select which action should be performed by the mouse.", "\n", "\n", "Let's look at how the actions are implemented.   The only objects are the base and the\ncylinders.  In the program, the base is referred to as ", ", and all the objects\nare children of an ", " named ", ".  (I use the ", "\nobject to make it easy to rotate the set of all visible objects without moving the camera or lights.)\nFor all drag, add, and delete actions, I look for intersections of these objects with\na ray that extends from the camera through the mouse position:", "If ", " is zero, there are no intersections, and there is nothing\nto do.  Otherwise, I look at ", "[0], which represents an intersection with\nthe object that is visible at the mouse position.  So, ", "[0]", "\nis the object that the user clicked, and ", "[0]", " is the\npoint of intersection.", "The Delete action is the simplest to implement:  When the user clicks a cylinder, the cylinder should be \nremoved from the scene.  If the first intersection is with the ", ", then \nnothing is deleted.  Otherwise, the clicked object was a cylinder and should be deleted:", "For an Add action, we should add a cylinder only if the user clicked the ground. In that case,\nthe point of intersection tells where the cylinder should be added.  An interesting issue\nhere is that we get the point of intersection in world coordinates, but in order to\nadd the cylinder as a child of ", ", I need to know the point of intersection\nin the local coordinate system for ", ".  The two coordinate systems will be different\nif the world has been rotated.  Fortunately, every ", "\nhas a method ", "(", ") that can be used to transform a ", ",\n", ", from world coordinates to local coordinates for that object.  This method does not\nreturn a value; it modifies the coordinates of the vector\u00a0", ".  (There is also a\n", " method.)  So, the Add action can be implemented like this:", "For a Drag action, we can determine which cylinder was clicked using the same test as for delete.\nHowever, the problem of moving the cylinder as the user drags the mouse raises a new issue:\nhow do we know where to put the cylinder?  We somehow have to transform a new mouse\nposition into a new position for the cylinder.   For that, we can use the raycaster\nagain.  My first thought was to use a ray from the camera through the new mouse position,\nto find the intersection of that ray with the ground, and then to move the cylinder to that \npoint of intersection.  Unfortunately, this puts the ", " of the cylinder at the\nmouse position, and it made the cylinder jump to the wrong position as soon as I started moving\nthe mouse.  I realized that I didn't want to track the intersection with the ground; I needed to\ntrack the intersection with a plane that lies at the same height as the original point of intersection.\nTo implement this, I add an invisible plane at that height just during dragging,\nand I use intersections with that plane instead of intersections with the ground.\n(You can have invisible objects in ", "\u2014just set the ", " \nproperty of the material to ", ".)", "One thing that has been missing in our 3D images is shadows.  Even if you didn't\nnotice the lack consciously, it made many of the images look wrong.\nShadows can add a nice touch of realism to a scene, but OpenGL, including WebGL, \ncannot generate shadows automatically.  There are ways to compute shadows that \ncan be implemented in OpenGL, but they are tricky to use and they are not completely\nrealistic physically.  One method, which is called ", ",\nis implemented in ", ".  Shadow mapping in ", " is certainly not trivial to use,\nbut it is easier than trying to do the same thing from scratch.", "Here is\na demo that\nshows a ", " scene that uses shadow mapping.  The lights that\ncast the shadows can be animated, so you can watch the shadows change\nas the lights move.", "\n", "\n", "The basic idea of shadow mapping is fairly straightforward: To tell what parts\nof a scene are in shadow, you have to look at the scene from the point of view of\nthe light source.  Things that are visible from the point of view of the light\nare illuminated by that light.  Things that are not visible from the light are\nin shadow.  (This is ignoring the possibility of transparency and indirect, reflected\nlight, which cannot be handled by shadow mapping.)  To implement this idea, place a camera at the light source and take a picture.\nIn fact, you don't need the picture.  What you need is the ", ".  After the\npicture has been rendered, the value stored in the depth buffer for a given pixel\ncontains, essentially, the distance from the light to the object that is visible from the\npoint of view of the light at that pixel.  That object is illuminated by the light. If an object\nis at greater depth than the value stored in the depth buffer, then that object is\nin shadow.  The depth buffer is the shadow map.  Now, go back to the point of view of the camera,\nand consider a point on some object as it is rendered from the camera's point of view.  Is that\npoint in shadow or not?  You just have to transform that point from the camera's viewing coordinates\nto the light's viewing\ncoordinates and check the depth of the transformed point.  If that depth is greater than\nthe corresponding value in the shadow map, then the point is in shadow.  Note that if there\nare several lights, each light casts its own shadows, and you need a shadow map for each light.", "It is computationally expensive to compute shadow maps and to apply them, and\nshadows are disabled by default in ", ".  To get shadows, you need to do\nseveral things.  You need to enable shadow computations in the WebGL renderer by\nsaying", "Only ", " and\n", " can cast shadows.  To get\nshadows from a light, even after enabling shadows in the renderer, you have to set\nthe light's ", " property to ", ":", "Furthermore, shadows have to be enabled for each object that\nwill cast or receive shadows.  \"Receiving\" a shadow means that shadows will\nbe visible on that object.  Casting and receiving are enabled separately for\nan object.", "Even this might not make any shadows show up, and if they do they might\nlook pretty bad.  The problem is that you usually have to configure the\ncameras that are used to make the shadow maps.", "Each ", " or\n", " has its own\nshadow camera, which is used to create the shadow\nmap from the point of view of that light.  The shadow camera for a directional\nlight uses an ", ".  An orthographic projection is \nconfigured by view volume limits\n", ", ", ", ", ", ", ", ", ", and\n", " (see ", ").   \nFor a directional light, ", ", these limits correspond to\nthe properties ", ", ", ", ", ",\n", ", ", ",  and ", ".\nThese values are relative to ", ".  It is important to make sure that\nall the objects in your scene, or at least those that cast shadows, are within the view volume of\nthe shadow camera.  Furthermore,\nyou don't want the limits to be too big: If the scene occupies only a small part\nof the camera's view volume, then only a small part of the shadow map will contain\nuseful information\u2014and since there is so little information about shadows, your\nshadows won't be very accurate.  The default values are set for a very large scene.\nFor a relatively small scene, you might set:", "The shadow camera for a spotlight uses a ", ".  (The use of\na camera with a limited view is why you can have shadows from spotlights but not from\npoint lights.)  For a spotlight ", ", the shadow camera is configured by the\nproperties ", ", ", ",\nand  ", " (where \"fov\" is the vertical\nfield of view angle, given in degrees rather than radians).  The default value\nfor fov is probably OK, except that if you change the spotlight's cutoff angle,\nyou will want to change the fov to match.  But you should be sure to set appropriate values\nfor near and far, to include all of your scene and as little extra as is practical.\nAgain, near and far are distances from ", ".", "Both types of light have a ", " property, with a value between 0 and\n1, that determines how dark the shadows are.  The default value, 0.5, gives fairly light\nshadows, and you might want to increase it.  Finally, you might want to increase\nthe size of the shadow map.  The shadow map is a kind of texture image which by default is 512 by 512\npixels.  You can increase the accuracy of the shadows by using a larger shadow map.\nTo do that for a light, ", ", set the values of the properties\n", " and ", ".  For example,", "I'm not sure whether power-of-two values are absolutely required here, but\nthey are commonly used for textures.", "We have created and viewed simple scenes, shown on a solid-colored background.\nIt would be nice to put our scenes in an \"environment\" such as the interior of a building,\na nature scene, or a public square.  It's not practical to build representations of\nsuch complex environments out of geometric primitives, but we can get a reasonably\ngood effect using textures.  The technique that is used is called a\n", ".  A skybox is a large cube where a\ndifferent texture is applied to each face of the cube.  The textures are images\nof some environment.  For a viewer inside the cube, the six texture images on the cube fit together to provide\na complete view of the environment in every direction.  The six texture images\ntogether make up what is called a ", ".\nThe images must match up along the edges of the cube to form a seamless view of\nthe environment.", "A cube map of an actual physical environment can be made by taking six pictures\nof the environment in six directions: left, right, up, down, forward, and back.  (More realistically,\nit is made by taking enough photographs to cover all directions, with overlaps, and then using software to \"stitch\"\nthe images together into a complete cube map.)  The six\ndirections are referred to by their relation to the coordinate axes as:\npositive\u00a0x, negative\u00a0x, positive\u00a0y, negative\u00a0y, positive\u00a0z, and negative\u00a0z,\nand the images must be listed in that order when you specify the cube map.\nHere is an example.  The first picture shows the six images of a cube map laid\nout next to each other.  The positive y image is at the top, the negative y image is at the\nbottom.  In between are the negative x, positive z, positive x, and negative z images\nlaid out in a row.  The second picture shows the images used to texture a cube, viewed here from \nthe outside.  You\ncan see how the images match up along the edges of the cube:", "\n", "(This cube map, and others used in this section, are by Emil Persson, who has made a \nlarge number of cube maps available for download\nat ", "\nunder a creative commons license.)", "For a skybox, a very large cube is used.  The camera, lights, and any objects that are to be part\nof the scene are inside the cube.  The skybox cube itself should not be lit; any lighting\nof the environment is already part of the cube map texture images.", "One way to make a skybox is to load the cube map as a set of six separate texture images,\nand then use ", " to apply one texture image to\neach face of the cube.  The material for each face should be a ", ",\nwhich does not use lighting.  The six images can be loaded using ", " and\ncan be applied to a face using the ", " property of the material,\nas discussed in ", "  For example:", "(This code does not include a way to get the scene redrawn after\nthe textures have finished loading.  That's fine if an animation is running.\nIf not, you have to use a callback function in the texture loader to do the\nredrawing.)", "However, WebGL has built-in support for using cubemap textures directly.  It's possible\nto load the six images of a cube map into a single object representing the\ncubemap texture as a whole.  To use such a cubemap texture, ", "\nrequires a new kind of material called ", ".\nA shader material uses custom vertex and fragment shaders to render the image.\n(", " and ", "\nare programs that run on the ", ".  The are required for rendering images \nin WebGL, and we will start working with them directly in the ", ".) \nThe shaders that are required in this case are defined in a ", " shader library.\nHere's an example of making a skybox using a cubemap texture.  This code is copied\nfrom examples in the ", " download.  It uses the same ", " array\nas the previous example:", "The sample program ", "\nshows two WebGL scenes that use cube maps.  In the first, the texture is applied using\na ", ", and the cube is viewed from the\noutside.   In the second, the texture is a applied to a skybox cube as a cubemap texture, \nand the cube is viewed from the inside.", "A reflective surface shouldn't just reflect light\u2014it should reflect its\nenvironment.  ", " can use ", "\nto simulate reflection.  (Environment mapping is also called \"reflection mapping\").\nEnvironment mapping uses a cube map texture.  Given a\npoint on a surface, a ray is cast from the camera position to that point,\nand then the ray is reflected off the surface.  The point where that reflected\nray hits the cube determines which point from the texture maps to the point on the\nsurface.  For a simulation of perfect, mirror-like reflection, the surface\npoint is simply painted with the color from the texture.\nNote that the surface does not literally reflect other objects in the scene.\nIt reflects the contents of the cube map texture.  However,\nif the same cube map texture is used on a skybox, and if the skybox is\nthe only other object in the scene,  then it will look like\nthe surface is a mirror that perfectly reflects its environment.", "This type of reflection is very easy to do in ", ".  You only need to\nmake a ", " and set its\n", " property equal to the cubemap texture object.  For example,\nif ", " is the texture object obtained using\n", "(), as in the skybox examples above, we can make a sphere that\nperfectly reflects the texture by saying:", "For the effect to look good, you would want to use the\nsame texture on a skybox.  Note that no lighting would be necessary in the scene, since\nboth the skybox and the sphere use a ", ".\nThe colors seen on the sphere come entirely from the environment map and\nthe basic color of the sphere material.  The environment map color is multiplied\nby the basic color.  In this example, the basic ", " of the\nmaterial is white, and the sphere color is exactly equal to the color\nfrom the texture.  With a different base color, the environment map texture would be \"tinted\"\nwith that color.  You could even apply a regular texture map to the sphere, to be used in place of\nthe color, so that the reflection of the skybox would be combined with the texture.", "The sample program ", " demonstrates\nenvironment mapping.  It can show a variety of environment-mapped objects, with\na variety of skymap textures, and it has several options for the base color of the\nobject.  Here are two images from that demo.  The one on the left shows a reflective\narrowhead shape with a white base color.  On the right, the object is a model\nof a horse (taken from the ", " download) whose base color is pink:", "\n", "\nHere is a demo that is very similar to the sample program.  In the demo, you can choose\nto view just the skybox or just the reflective object, and you can see that the object only\nseems to be reflecting its actual environment.  Use your mouse to rotate the scene to see\nhow the reflection changes as the orientation of the object changes.", "\n", "\n", "\n", " can also do ", ".  Refraction\noccurs when light passes through a transparent or translucent object.  A ray of light will be\nbent as it passes between the inside of the object and the outside.  The amount\nof bending depends on the so-called \"indices of refraction\" of the material outside\nand the material inside the object.  More exactly, it depends on the ratio between\nthe two indices.  Even a perfectly transparent object will be visible because of\nthe distortion induced by this bending.", "In ", ", refraction is implemented using environment maps.  As with reflection,\na refracting object does not show its actual environment; it refracts the cubemap texture\nthat is used as the environment map.  For refraction, a special\n\"mapping\" must be used for the environment map texture.  The mapping is the second\nparameter to ", "().  Here is an example of\nloading a cubemap texture for use in refraction mapping", "In addition to this, the ", " property of the material\nthat is applied to the refracting object should be set.  The value is a number\nbetween 0 and\u00a01; the closer to 1, the less bending of light.  The default value\nis so close to 1 that the object will be almost invisible.  My example, below, uses\na value of 0.6:", "This gives a strong refractive effect.  If you set the material color to\nsomething other than white, you will get something that looks like tinted glass.\nAnther property that you might set is the ", ".  For a refractive\nobject, this value tells how much light is transmitted through the object rather than reflected\nfrom its surface.\nThe default value, 1, gives 100% transmission of light; smaller values make objects\nlook like they are made out of \"cloudy\" glass that blocks some of the light.", "The sample program ", "\nis a copy of ", " that has been modified to\ndo refraction instead of reflection.  The objects look like they are made of glass instead\nof mirrors.  An option has been added to make the glass look cloudy.  The following images are\nfrom that program.  A perfectly transmissive arrowhead is shown in the first image, and a cloudy\nsphere in the second.  Notice how the sphere shows an inverted image of the objects\nbehind it:", "\n", "In my reflection and refraction examples, the environment is a skybox, and there is a single\nobject that reflects or refracts that environment.  But what if a scene includes more than one\nobject?  The objects won't be in the cubemap texture.  If you use the cubemap texture on\nthe objects, they won't reflect or refract ", ".  There is no complete solution\nto this problem in WebGL.  However, you can make an object reflect or refract other objects\nby making an environment map that includes those objects.  If the objects are moving, this means\nthat you have to make a new environment map for every frame.  Recall that an environment map\ncan be made by taking six pictures of the environment.  ", " has a kind of camera that\ncan do just that, ", ".  I won't go into the\nfull details, but a CubeCamera can take a\nsix-fold picture of a scene from a given point of view and make a cubemap texture from\nthose images.  To use the camera, you have to place it at the location of an object\u2014and make the\nobject invisible so it doesn't show up in the pictures.  Snap the picture, and apply it\nas an environment map on the object.  For animated scenes, you have to do this in every frame,\nand you need to do it for every reflective/refractive object in the scene.  Obviously, this\ncan get very computationally expensive!  And the result still isn't perfect.  For one thing,\nyou won't see multiple reflections, where objects reflect back and forth on each other several\ntimes. For that, you need a different kind of rendering from the one used by OpenGL."], "section_id": "Section 5.3", "chapter_id": "Chapter 5", "chapter_title": "Three.js: A 3D Scene Graph API", "section_title": "Other Features"},
{"content": ["We have seen how ray tracing can be extended to approximate a variety of\neffects that are not handled by the basic algorithm.  We look next at an\nalgorithm that accounts for all those effects and more in a fairly\nstraightforward and unified way: ", ".   Like ray tracing,\npath tracing computes colors for points in an image by tracing the\npaths of light rays backwards from the viewer through points on the\nimage and into the scene.  But in path tracing, the idea is to \naccount for ", " possible paths that the light could have\nfollowed.  Of course, that is not literally possible, but following a\nlarge number of paths can give a good approximation\u2014one that\ngets better as the number of paths is increased.", "In order to model a wide variety of physical phenomena, path tracing\nuses a generalization of the idea of ", " property.\nIn OpenGL, a material is a combination of ", ", \n", ", ", ", \nand ", " colors, plus ", ".\nThese properties, except for emission color, model how the surface interacts with\nlight.  Material properties can vary from point to point on a surface; that's an\nexample of a ", ".", "OpenGL material is only a rough approximation of reality.  In path tracing, a more general\nnotion is used that is capable of more accurately representing the properties of\nalmost any real physical surface or volume.  The replacement for materials\nis call a ", ", or Bidirectional Scattering Distribution\nFunction.", "Think about how light that arrives at some point can be affected by the\nphysical properties of whatever substance exists at that point.  Some of the\nlight might be absorbed.  Some might pass through the point without\nbeing affected at all.  And some might be \"scattered,\" that is,\nsent off in another direction.  In fact, we consider passing through the point\nas a special case of scattering.  A BSDF describes how light is scattered\nfrom each point on a surface or in a volume.", "Think of a single ray, or photon, of light that arrives at some point.  What happens to it can\ndepend on the direction from which it arrives.  In general, assuming that it is not absorbed, the light is more \nlikely to be scattered in some directions than in others.  (As in specular reflection,\nfor example.)  The BSDF at the \npoint gives the probability that the ray will leave the point heading in a \ngiven direction.  It is a \"bidirectional\" function because the answer is\na function of two directions, the direction from which the light arrives and\nthe outgoing direction that you are asking about.  (It is a \"distribution function\" in\nthe sense of the mathematical theory of continuous probability distributions,\nbut you don't need to understand that to get the general idea.  For us,\nit's enough to understand that the function says how light coming in from a\ngiven direction is distributed among possible outgoing directions.)\nNote that a BSDF is also a function of the point that you are talking about,\nand it can be a function of the wavelength of the light as well.", "Any point in space can be assigned a BSDF.  For empty space, the BSDF is trivial:\nIt simply says that light arriving at a point has a 100% probability of continuing\nin the same direction.  But light passing through fog or dusty air or dirty water has some\nprobability of being absorbed and some probability of being scattered to a random\ndirection.  Similar remarks apply to light passing through the interior of a\ntranslucent solid object.", "Traditionally, though, computer graphics has been mostly concerned with what\nhappens to light at the surface of an object.  Light can be absorbed or reflected or,\nif the object is translucent, transmitted through the surface.\nThe function that describes the reflection of light from a surface is sometimes\ncalled a BRDF (Bidirectional Reflectance Distribution Function), and the formula\nfor transmission of light is a BTDF (Bidirectional Transmission Distribution function).\nThe BSDF for a surface is a combination of the two.", "Let's consider OpenGL materials in terms of BSDFs.  In basic OpenGL, light can only\nbe reflected or absorbed.  For diffuse reflection, light has an\nequal probability of being reflected in every direction that makes an angle of less than\n90 degrees with the normal vector to the surface, and there is no dependence on\nthe direction from which the light arrives.  For specular reflection, the incoming\nlight direction matters.  In OpenGL, the possible outgoing directions for specularly reflected light form a cone,\nwhere the angle between the axis of the cone and the normal vector is equal to the angle between\nthe normal vector and the incoming light direction.  The axis of the cone is the most \nlikely direction for outgoing light, and the probability falls off as the angle between the outgoing\ndirection and\nthe direction of the axis increases.  The rate of falloff is specified by the shininess property of the material.\nThe BRFD for the surface combines the diffuse and specular reflection.  (The ambient material\nproperty doesn't fit well into the BSDF framework, since physically there is no such\nthing as an \"ambient light\" that is somehow different from regular light.)", "Ray tracing adds two new possibilities to the interaction of light with a surface:\nperfect, mirror-like reflection, where the outgoing light makes exactly the same angle\nwith the normal vector as the incoming light, and transmission of light into a translucent\nobject, where the outgoing angle is determined by the indices of refraction outside and\ninside the object.", "But BSDFs can provide even more realistic models of the interaction of light with\nsurfaces.  For example, the distinction between mirror-like reflection of an object \nand specular reflection of a light source is artificial.  A perfect mirror\nshould reflect both light sources and objects in a mirror-like way.  For a shiny but\nrough surface, all specular reflection would send the light in a cone of directions,\ngiving fuzzy images of objects and lights alike. A BSFD should handle both cases, and\nit shouldn't distinguish between light from light sources and light reflected off\nother objects.", "BSDFs can also correctly handle a phenomenon called ", ", which\ncan be an important visual effect for materials that are just a bit translucent, such as\nmilk, jade, and skin.  In sub-surface scattering, light that hits a surface can be \ntransmitted into the object, be scattered a few times internally inside the object, and\nthen emerge from the surface at another point.  How the light behaves inside the object is determined\nby the BSDF of the material in the interior of the object.  The BSDF in this case would\nbe similar to the one for fog, except that the probability of scattering would be larger.", "The point is that just about any physically realistic material can be modeled\nby a correctly chosen BSDF.", "Path tracing is based on a formula known as the \"rendering equation.\"  The formula\nsays that the amount of light energy leaving a given point in a given direction\nis is equal to the amount of light energy emitted by the point in that direction\nplus the amount of light energy arriving at the point from other sources that\nis then scattered in that direction.", "Here, emitted light means light that is created, as by a light source.  In the rendering\nequation, any object can be an emitter of light.  In OpenGL terms, it's as if an object\nwith an emission color actually emits light that can illuminate other objects.  An\narea light is just an extended object that emits light from every point, and it is common\nto illuminate scenes with large light-emitting objects.  (In fact, in\na typical path tracing setup, point lights and directional lights have to be assigned\nsome area to make them work correctly in the algorithm.)", "As for scattered light, the BSDF at a point determines how light arriving at a point is scattered.\nLight can, in general, arrive from any direction\nand can originate from any other point in the scene.  The rendering equation holds\nat ", " point.  It relates the light arriving at and departing from each point to\nthe light arriving at and departing from every other point.  It describes, in other words,\nan immensely complicated system, one for which you are unlikely to be able to find an\nexact solution.  A rendering algorithm can be thought of as an attempt to find\na good approximate solution to the rendering equation.", "Path tracing is a probabilistic rendering algorithm.  It looks at possible paths\nthat might have been followed by light arriving at the position of the viewer.\nEach possible path has a certain probability. Path tracing generates a random\nsample of possible paths, choosing paths in the sample according to their probabilities.\nIt uses those paths to create an image that approximates a \nsolution to the rendering equation.  It can be shown that as the size of the random\nsample increases, the image that is generated will approach the true solution.\nTo get a good quality image, the algorithm will have to trace thousands of paths\nfor each pixel in the image, but the result can be an almost shocking level of\nrealism.", "Let's think about how it should work.  First, consider the case where light\nis only emitted and reflected by surfaces.  As with ray tracing, we start at\nthe position of the viewer and cast a ray in the direction of a point on the\nimage, into the scene.  (See ", ".) \nWe find the first intersection of that ray with an object in the scene. \nOur goal to trace one possible path that the ray could have followed from \nits point of origin until it arrives at the viewer, and we want the\nprobability that we select a given path to be the probability that the\nlight actually followed that path.  This means that each time the light is\nscattered from a surface, we should choose the direction of the next segment\nof the path based on the BSDF for the surface.  That is, the direction is\nchosen at random, using the probability distribution that is encoded in the BSDF.\nWe construct the next segment of the path by casting a ray in the selected\ndirection.", "We continue to trace the path, backwards in time, possibly through multiple reflections, until it\nencounters an object that emits light.  That object serves as the original\nsource of the light.  The color that the path contributes to the image is\ndetermined by the color and intensity of the emitter, by the colors of \nsurfaces that the light hits along the way,\nand by the angles at which the light hits each surface.  If the path escapes\nfrom the scene before it hits a light emitting object, then it does not\ncontribute any color to the image. (It might be desirable to have a light-emitting\nbackground, like a sky, that emits light over a large area.)  Note that it\nis possible for an object to be both an emitter and a reflector of light.\nIn that case, a path can continue even after it gets to a light source.", "Of course, we have to trace many such paths.  The color for a pixel in\nthe image is computed as an average of the colors obtained for all the paths\nthat pass through that pixel.", "The algorithm can be extended to handle the case where light can be\nscattered at arbitrary points in space, and not just at surfaces.\nFor light traveling in a medium in 3D space, the question is, how far will the light travel before it is\nscattered?  The BSDF for the medium will determine a probability distribution on possible travel distances\nbetween scatterings.  When light enters a medium, that probability distribution is used to select a\nrandom distance that the light will travel before it is scattered (unless it hits a surface or enters a\nnew medium before it has traveled that distance).  When it scatters from a point in the medium, a\nnew direction and length are chosen at random for the next segment of the path, according to the BSDF\nof the medium.  For a light fog, the average distance between scatterings would be quite large;\nfor a dense medium like milk, it would be quite short.", "A great deal of computation is required to trace enough light paths to get\na high-quality image.  Although path tracing was invented in the 1980s, it\nis only recently that it has become practical for general use, and it can still\ntake many hours to get acceptable quality.  In fact, you can do path tracing\non your desktop computer using the 3D modeling program Blender, which is discussed\nin ", ".  Blender has an alternative rendering engine,\ncalled the Cycles renderer, that uses path tracing.  Cycles is not discussed in the\nappendix, but you can look up some tutorials on it, if you are interested in\nseeing what path tracing can do."], "section_id": "Section 8.2", "chapter_id": "Chapter 8", "chapter_title": "Beyond Realtime Graphics", "section_title": "Path Tracing"},
{"content": ["This chapter uses WebGL for 2D drawing.  Of course, the real motivation\nfor using WebGL is to have high-performance 3D graphics on the web.\nWe will turn to that in the ", ".\nWith WebGL, implementing ", " is the\nresponsibility of the programmer, which adds a level of complexity \ncompared to OpenGL\u00a01.1.  But before we attempt to deal with that complexity\nin three dimensions, this short section shows how to implement\ntransforms and ", " in a 2D context.", "Transforms in 2D were covered in ", ".  To review: The basic\ntransforms are scaling, rotation, and translation.  A sequence of such transformations\ncan be combined into a single ", ".  A\u00a02D affine transform maps\na point (", ") to the point (", ") given by formulas of the form", "where ", ", ", ", ", ", ", ", ", ", and ", " are constants.\nAs explained in ", ", this transform can be represented as\nthe 3-by-3 matrix", "\n", "With this representation, a point (", ") becomes the three-dimensional vector\n(", "), and the transformation can be implemented as multiplication of the vector\nby the matrix.", "To apply a transformation to a primitive, each vertex of the primitive has to be\nmultiplied by the transformation matrix.  In GLSL, the natural place to do that\nis in the vertex shader.  Technically, it would be possible to do the multiplication\non the JavaScript side, but GLSL can do it more efficiently, since it can work on multiple\nvertices in parallel, and it is likely that the GPU has efficient hardware support for\nmatrix math.  (It is, by the way, a property of affine transformations that it suffices\nto apply them at the vertices of a primitive. Interpolation of the transformed vertex coordinates\nto the interior pixels of the primitive will given the correct result; that is, it gives the same\nanswer as interpolating the original vertex coordinates and then applying the transformation\nin the fragment shader.)", "In GLSL, the type ", " represents 3-by-3 matrices, and ", " represents three-dimensional\nvectors.  When applied to a ", " and a ", ", the multiplication operator ", "\ncomputes the product.  So, a transform can applied using a simple GLSL assignment statement such as", "For 2D drawing, the original coordinates are likely to come into the vertex shader\nas an ", " of type ", ".  \nWe need to make the attribute value into a \n", " by adding 1.0 as the ", "-coordinate.  The transformation matrix\nis likely to be a ", ", to allow the JavaScript side\nto specify the transformation.  This leads to the following minimal vertex shader\nfor working with 2D transforms:", "In the last line, the value for ", " must be a ", ".  For a 2D point,  the\n", "-coordinate should be 0.0, not 1.0, so we use only the ", "- and ", "-coordinates\nfrom ", ".", "On the JavaScript side, the function ", " is used to specify a\nvalue for a uniform of type ", " (", ").\nTo use it, the nine elements of the matrix must be\nstored in an array in ", ".  For loading an affine transformation matrix into\na ", ", the command would be something like this:", "To work with transforms on the JavaScript side, we need a way to represent\nthe transforms.  We also need to keep track of a\n\"current transform\" that is the product all the individual\n", " that are in effect.\nThe current transformation changes whenever a transformation such as rotation or\ntranslation is applied.  We need a way to save a copy of the current transform\nbefore drawing a complex object and to restore it after drawing.  Typically, a\nstack of transforms is used for that purpose.  You should be well familiar \nwith this pattern from both 2D and 3D graphics.  The difference here is that\nthe data structures and operations that we need are not built into the\nstandard API, so we need some extra JavaScript code to implement them.", "As an example, I have written a JavaScript class, ", ",\nto represent affine transforms in 2D.  This is a very minimal implementation.\nThe data for an object of type ", "\nconsists of the numbers ", ", ", ", ", ", ", ", ", ", and ", "\nin the transform matrix.  There are methods in the class for multiplying the transform\nby scaling, rotation, and translation transforms.  These methods modify the transform\nto which they are applied, by multiplying it on the right by the appropriate matrix.\nHere is a full description of the API, where ", " is an object of type \n", ":", "In fact, an ", " object does not represent an affine\ntransformation as a matrix.  Instead, it stores the coefficients ", ", ", ", ", ", \n", ", ", ", and ", " as properties of the object.  With this representation,\nthe ", " method in the ", " class can defined as\nfollows:", "This code multiplies the transform represented by \"this\" object by a scaling matrix,\non the right.  Other methods have similar definitions, but you don't need to understand\nthe code in order to use the API.", "Before a primitive is drawn, the current transform must sent as a ", " into the\nvertex shader, where the ", " is needed to transform \nthe vertices of the primitive.  The method ", "() returns the \ntransform as an array that can be passed to ", ", which sends\nit to the shader program.", "To implement the stack of transformations, we can use an array of objects of\ntype ", ".  In JavaScript, an array does not\nhave a fixed length, and it comes with ", "() and ", "() methods\nthat make it possible to use the array as a stack.  For convenience,\nwe can define functions ", "() and ", "()\nto manipulate the stack.  Here, the current transform is stored in a global\nvariable named ", ":", "This code is from the sample program ", ",\nwhich demonstrates using ", " and a stack of \ntransforms to implement hierarchical modeling. Here is a screenshot of one of the objects\ndrawn by that program:", "\n", "and here's the code that draws it:", "The function ", "() draws a square that has size 1 and is centered at (0,0) in its\nown object coordinate system.  The coordinates for the square have been stored in a\nbuffer, ", ", and ", " is the location of an attribute variable\nin the shader program.  The variable ", " holds the current modeling transform that\nmust be applied to the square.  It is sent to the shader program by calling", "The second function, ", "(), draws\n16 squares.  Between the squares, it modifies the modeling transform with", "The effect of these commands is cumulative, so that each square is a little smaller than \nthe previous one, and is rotated a bit more than the previous one.  The amount of rotation\ndepends on the frame number in an animation.", "The nested squares are one of three compound objects drawn by the program.  The function\ndraws the nested squares centered at (0,0).  In the main ", "() routine, I wanted to\nmove them and make them a little smaller.  So, they are drawn using the code:", "The ", "() and ", "() ensure that all of the changes\nmade to the modeling transform while drawing the squares will have no effect on \nother objects that are drawn later.  Transforms are, as always, applied to objects in the\nopposite of the order in which they appear in the code.", "I urge you to read the ", "\nand take a look at what it draws.  The essential ideas for working with transforms are\nall there.  It would be good to understand them before we move on to 3D."], "section_id": "Section 6.5", "chapter_id": "Chapter 6", "chapter_title": "Introduction to WebGL", "section_title": "Implementing 2D Transforms"},
{"content": ["A scene in computer graphics can be a complex collection of objects, each with\nits own ", ".  In ", ",\nwe saw how a ", " can be used to organize all the objects in a 2D\nscene.  ", " a scene means traversing the\nscene graph, rendering each object in the graph as it is encountered.\nFor 3D graphics, scene graphs must deal with a larger variety of objects,\nattributes, and transforms.  For example, it is often useful to consider lights\nand cameras to be objects and to be able to include them in scene graphs.  In this\nsection, we consider scene graphs in 3D, and how to treat cameras and lights\nas objects.", "When designing scene graphs, there are many options to consider.  For example,\nshould transforms be properties of object nodes, or should there be separate nodes\nto represent transforms?  The same question can be asked about attributes.\nAnother question is whether an attribute value should apply only to the node\nof which it is a property, or should it be inherited by the children of that node?", "A fundamental choice is the shape of the graph.  In general, a scene graph can\nbe a ", ", or \"dag,\" which is a tree-like structure except\nthat a node can have several parents in the graph.  The scene graphs in\n", " were dags.  This has the advantage that a\nsingle node in the graph can represent several objects in the scene, since in a\ndag, a node can be encountered several times as the graph is traversed.  On the other\nhand, representing several objects with one scene graph node can lead to a lack of flexibility,\nsince those objects will all have the same value for any property encoded in\nthe node.  So, in some applications, scene graphs are required to be trees.  \nIn a tree, each node has a unique parent, and the node will be encountered only\nonce as the tree in traversed.  The distinction between trees and dags will show\nup when we discuss camera nodes in scene graphs.", "We have seen how the functions ", " and ", " are used\nto manipulate the transform stack.  These functions are useful when traversing a\nscene graph: When a node that contains a transform is encountered during a traversal\nof the graph, ", " can be called before applying the transform.  Then, after the\nnode and its descendants have been rendered, ", " is called to restore the\nprevious modelview transformation.", "Something similar can be done for attributes such as color and material, if it is assumed \nthat an attribute value in a scene graph node should be inherited as the default value of\nthat attribute for children of the node.  OpenGL 1.1 maintains an attribute stack, which is\nmanipulated using the functions ", " and ", ".  In addition\nto object attributes like the current color, the attribute stack can store global\nattributes like the global ambient color and the enabled state of the depth test.\nSince there are so many possible attributes, ", " does not simply\nsave the value of every attribute.  Instead, it saves a subset of the\npossible attributes.  The subset that is to be saved is specified as a parameter to\nthe function.  For example, the command", "will save a copy of each of the OpenGL state variables that can be enabled or\ndisabled.  This includes the current state of ", ", ", ",\n", ", and others.  Similarly,", "saves a copy of the current color, normal vector, and texture coordinates.  And", "saves attributes relevant to lighting such as the values of material properties and light properties,\nthe global ambient color, color material settings, and the enabled state for lighting and each of\nthe individual lights.  Other constants can be used to save other sets of attributes; see the\nOpenGL documentation for details.  It is possible to OR together several constants to combine\nsets of attributes.  For example,", "will save the attributes in both the ", " set and in the\n", " set.", "Calling ", "() will restore all the values that were saved by the\ncorresponding call to ", ".  There is no need for a parameter to\n", ", since the set of attributes that are restored is determined\nby the parameter that was passed to ", ".", "It should be easy to see how ", " and ", " can be used\nwhile traversing a scene graph:  When processing a node, before changing attribute values,\ncall ", " to save a copy of the relevant set or sets of attributes.\nRender the node and its descendants. Then call ", " to restore the\nsaved values.  This limits the effect of the changes so that they apply only to the node and\nits descendants.", "There is an alternative way to save and restore values.  OpenGL has a variety of \"get\" functions\nfor reading the values of various state variables.  I will discuss just some of them here.\nFor example,", "retrieves the current color value, as set by ", ".  The ", " parameter\nshould be an array of ", ", whose length is at least four.  The RGBA color components\nof the current color will be stored in the array.  Note that, later, you can simply call\n", "(", ") to restore the color.  The same function can be used\nwith different first parameters to read the values of different floating-point state variables.\nTo find the current value of the ", ", use", "This will set ", "[0] and ", "[1] to be the ", " and ", " coordinates\nof the lower left corner of the current viewport, ", "[2] to be its width, and ", "[3]\nto be its height. To read the current values of material properties, use", "The ", " must be ", " or ", ".  The property must be\n", ", ", ", ", ", ", ", or ", ".\nThe current value of the property will be stored in ", ", which must be of length at least\nfour for the color properties, or length at least one for ", ".  There is\na similar command, ", ", for reading properties of lights.", "Finally, I will mention ", "(", "), which can be used to check the\nenabled/disabled status of state variables such as ", " and ", ".\nThe parameter should be the constant that identifies the state variable.  The function returns 0 if the state\nvariable is disabled and 1 if it is enabled.  For example, ", "(", ")\ntests whether lighting is enabled.  Suppose that a node in a scene graph has an attribute\n", " to tell whether that node (and its descendants) should be rendered with lighting\nenabled.  Then the code for rendering a node might include something like this:", "Since ", " can be used to push large\ngroups of attribute values, you might think that it would\nbe more efficient to use ", " and the ", " family of commands\nto read the values of just those state variables that you are \nplanning to modify.  However, recall that OpenGL can queue a number\nof commands into a batch to be sent to the graphics card, and those commands\ncan be executed by the ", " at the same time that your program\ncontinues to run.  A ", " command can require your \nprogram to communicate with the graphics card and wait for the response.\nThis means that any pending OpenGL commands will have to be sent to the\ngraphics card and executed before the ", " command can complete.\nThis is the kind of thing that can hurt performance.  \nIn contrast, calls to ", " and ", " can\nbe queued with other OpenGL commands and sent to the graphics\ncard in batches, where they can be executed efficiently by\nthe graphics hardware.  In fact, you should generally prefer\nusing ", "/", " instead of a\n", " command when possible.", "Let's turn to another aspect of modeling.  Suppose that we want to implement a\nviewer that can be moved around in the world like other objects.  Sometimes, such\na viewer is thought of as a moving camera.  The camera is used to take pictures of\nthe scene.  We want to be able to apply transformations\nto a camera just as we apply transformations to other objects.  The position\nand orientation of the camera determine what should be visible when the scene is \nrendered.  And the \"size\" of the camera, which can be affected by a scaling transformation,\ndetermines how large a field of view it has.   But a camera is not\njust another object.  A camera really represents the viewing transformation that\nwe want to use.  Recall that modeling and viewing transformations have opposite effects:\nMoving objects to the right with a modeling transform is equivalent to moving the\nviewer to the left with a viewing transformation.  (See ", ".)\nTo apply a modeling transformation to the camera, we\nreally want to apply a viewing transformation to the scene as a whole, and that viewing transformation\nis the ", " of the camera's modeling transformation.", "The following illustration shows a scene viewed from a moving camera.  The camera starts\nin the default viewing position, at the origin, looking in the direction of the negative ", "-axis.\nThis corresponds to using the identity as the viewing transform.  For the second image,\nthe camera has moved forward by ten units.  This would correspond to applying the modeling\ntransformation ", "(0,0,\u221210) to the camera (since it is moving in the negative\n", "-direction).  But to implement this movement as a change of view, \nwe want to apply the inverse operation as a viewing transformation.  So, the viewing\ntransform that we actually apply is ", "(0,0,10).  This can be seen, \nif you like, as a modeling transformation\nthat is applied to all the ", " objects in the scene:  Moving the camera ten units in\none direction is equivalent to moving all the other objects 10 units in the opposite direction.", "\n", "For the third image, the camera has rotated in place by 21 degrees to the right\u2014a 21-degree\nclockwise rotation about the ", "-axis\u2014", " it has been translated.  This can be \nimplemented by the transformation ", "(21,0,1,0)\u2014a 21-degree counterclockwise\nrotation about the ", "-axis\u2014applied ", " the translation. Remember that the\ninverse of a composition of transformations is the composition of their inverses, in the opposite\norder.  Mathematically, using ", " to represent the inverse of a\ntransformation ", ", we have that \n", " for\ntwo transformations ", " and ", ".", "The images in the illustration are from the following demo.  The demo lets you move around in a scene.  More accurately, of course, it \nlets you change the viewing transformation to see the scene from different viewpoints.", "\n", "\n", "When using scene graphs, it can be useful to include a camera object in the graph.  That is,\nwe want to be able to include a node in the graph that represents the camera, and we want to\nbe able to use the camera to view the scene.  It can even be useful to have several cameras\nin the scene, providing alternative points of view.  To implement this, we need to be able\nto render a scene from the point of view of a given camera.  From the previous discussion,\nwe know that in order to do that, we need to use a viewing transformation that is the\ninverse of the modeling transformation that is applied to the camera object.\nThe viewing transform must be applied before any of the objects in the scene are rendered.", "When a scene graph is traversed, a modeling transformation can be applied at any node.\nThe modeling transform that is in effect when a given node is encountered is the composition\nof all the transforms that were applied at nodes along the path that led to given node.\nHowever, if the node is a camera node, we don't want to apply that modeling transform;\nwe want to apply its inverse as a viewing transform.  To get the inverse, we can\nstart at the camera node and follow the path backwards, applying the inverse of\nthe modeling transform at each node.", "\n", "To easily implement this, we can add \"parent pointers\" to the scene graph data structure.  \nA parent pointer for a node is a link to the parent of that node in the graph. Note that this only works\nif the graph is a tree; in a tree, each node has a unique parent, but that is not true in a general\ndirected acyclic graph.  It is possible to move up the tree by following parent pointers.", "We this in mind, the algorithm for rendering the scene from the point of view of a camera\ngoes as follows: Set the modelview transform to be the identity, by calling ", "().\nStart at the camera node, and follow parent pointers until you reach the root of the tree.\nAt each node, apply the ", " of any modeling transformation in that node.\n(For example, if the modeling transform is translation by (a,b,c), call\n", "(", ").)  Upon reaching the root, the viewing\ntransform corresponding to the camera has been established.  Now, traverse the scene graph\nto render the scene as usual.  During this traversal, camera nodes should be ignored.", "Note that a camera can be attached to an object, in the sense that the camera and the object\nare both subject to the same modeling transformation and so move together as a unit.\nIn modeling terms, the camera and the object\nare sub-objects in a complex object.  For example, a camera might be attached\nto a car to show the view through the windshield of that car.  If the car moves, because its\nmodeling transformation changes, the camera will move along with it.  ", "It can also be useful to think of lights as objects, even as part of a complex object.\nSuppose that a scene includes a model\nof a lamp.  The lamp model would include some geometry to make it visible, but if it\nis going to cast light on other objects in the scene, it also has\nto include a source of light.  This means that the lamp is a complex\nobject made up of an OpenGL light source plus some geometric objects.\nAny modeling transformation that is applied to the lamp should\naffect the light source as well as the geometry.  In terms of the\nscene graph, the light is represented by a node in the graph,\nand it is affected by modeling transformations in the same\nway as other objects in the scene graph.  You can even have\nanimated lights\u2014or animated objects that include lights\nas sub-objects, such as the headlights on a car.", "Recall from ", " that a light source is subject to the\nmodelview transform that is in effect at the time the position of the\nlight source is set by ", ".  If the light is represented as a node in\na scene graph, then the modelview transform that we need is the one that\nis in effect when that node is encountered during a traversal of the scene\ngraph.  So, it seems like we should just traverse the graph and set the position\nof the light when we encounter it during the traversal.", "But there is a problem:  Before any geometry is rendered,\nall the light sources that might affect that geometry must already be\nconfigured and enabled.  In particular, the lights' positions must be set\nbefore rendering any geometry.  This means that you can't simply set the\nposition of light sources in the scene graph as you traverse the graph in the\nusual way.  If you do that, objects that are drawn before the\nlight is encountered won't be properly illuminated by the\nlight.  Similarly,\nif the light node contains values for any other properties of\nthe light, including the enabled/disabled state of the light,\nthose properties must be set before rendering any geometry.", "One solution is to do two traversals of the scene graph, the first\nto set up the lights and the second to draw the geometry.  Since\nlights are affected by the modelview transformation, you have to\nset up the modeling transform during the first traversal\nin exactly the same way that you do in the second traversal.\nWhen you encounter the lights during the first traversal,\nyou need to set the position of the light, since setting the\nposition is what triggers the application of the current modelview\ntransformation to the light.  You also need to set any other\nproperties of the light.  During the first traversal, geometric\nobjects in the scene graph are ignored.  During the second traversal, when\ngeometry is being rendered, light nodes can be ignored."], "section_id": "Section 4.4", "chapter_id": "Chapter 4", "chapter_title": "OpenGL 1.1: Light and Material", "section_title": "Lights, Camera, Action"},
{"content": ["WebGL is designed to run on a wide variety of devices, including mobile devices\nthat have relatively limited graphical capabilities.  Because of this, only a minimal\nset of features is required of all WebGL implementations.  However, WebGL has\na mechanism for activating additional, optional features.  The optional features\nare defined in ", ".\nA web page that requires a WebGL extension is not guaranteed to work in every implementation of\nWebGL.  However, in many cases, it is fairly easy to write a page that can work with or \nwithout the extension, though perhaps with some missing feature when the extension is not available.\nThere are about two dozen extensions whose definitions have been standardized.\nThese standard extensions are documented at\n", ".\n", "Standard OpenGL also has an extension mechanism.  Historically, many features from extensions in\none version of OpenGL have become required features in later versions.  The same is likely to be\ntrue for WebGL extensions.  In fact, some of the WebGL 1.0 extensions have been incorporated as\nrequired features in the upcoming WebGL\u00a02.0.", "This section covers the WebGL extension mechanism, and it discusses a few of the standard\nextensions.", "We start with a simple extension that can improve the appearance of textures in some scenes.\nThe standard filtering methods for ", " an image texture give poor results when the\ntexture is viewed at an oblique angle.  In that case, a pixel on the surface corresponds\nto a trapezoidal region in the texture, and the standard ", "\nand ", " filter rules such as ", " don't\nhandle that case very well.  (Filtering was covered in ", ".)\nA better result can be obtained, at the cost of additional computation, using something called\n", ", which samples the texture taking the trapezoidal shape\ninto account.  Many ", " can do anisotropic filtering.  It is not a required feature\nin WebGL implementations, but it is commonly available as an extension.", "The sample program ", " shows how to use anisotropic\nfiltering in WebGL.  It shows a large plane textured with a brick image that can be viewed from a sharp,\noblique angle.  If the extension is available, then the user can turn anisotropic filtering on and off.\nIf it is not available, the program will still draw the scene, but only using standard filtering.\nHere are two images from the program.  Anisotropic filtering is used in the image on the right.\nOn the left, without anisotropic filtering, the texture is blurred even at moderate distanced from\nthe viewer:", "\n", "Each WebGL extension has a name.  The function ", "(", ") is used to activate an\nextension, where ", " is a string containing the name of the extension.  The return value of the function is ", "\nif the extension is not available, and you should always check the return value before attempting to \nuse the extension.  If the return value is not null, then it is a JavaScript object.  The object might contain,\nfor example, constants that are meant to be passed to WebGL API functions in order to make use of the functionality\nof the extension.", "The name of the anisotropic filtering extension is \"EXT_texture_filter_anisotropic.\"  (Names are\nnot case-sensitive.) To test for\nthe availability of the extension and to activate it, a program can use a statement such as", "If ", " is ", ", then the extension is not available.  If it is not\nnull, then the object has a property named ", " that can be used\nas a parameter to ", " to set the level, or amount, of anisotropic filtering that\nwill be applied to the texture.  For example, after creating and binding a texture, a program might say", "The third parameter is the anisotropic filtering level.  Setting the level to 1 will turn off\nanisotropic filtering.  Higher values give better results.  There is an implementation-dependent\nmaximum level, but asking for a level greater than the maximum is not an error\u2014you will simply get the maximum\nlevel.  To find out the maximum, you can use", "It is recommended to use ", " as the minification filter and\n", " as the magnification filter when using anisotropic filtering.  A texture\nwould typically be configured using code similar to the following:", "If the extension is not available, the texture might not look as good as it might\nhave, but it will still work (and only a very observant user is likely to notice).", "As a second example, we consider a pair of extensions named \"OES_texture_float\"\nand \"WEBGL_color_buffer_float\".  The first of these makes it possible to use textures\nin which color component values are floating-point numbers, instead of eight-bit integers.\nThe second makes it possible to render to such a texture by using it as the color buffer in a\n", ".", "Why would someone want to do this?  Eight-bit integers are fine for representing colors visually,\nbut they don't have enough precision for doing accurate calculations.  \nFor applications that do significant numerical processing with color components, floating-point values are\nessential.", "As an example, consider finding the average color value of an image, which requires\nadding up the color values from a large number of pixels.  This is something that can be\nspeeded up by using the parallel processing power of a GPU.  My technique for doing so uses\ntwo framebuffers, with two textures serving as color buffers.  \nI assume that the image width and height are powers of two.  Start by drawing\nthe image to the first texture.  Think of the image as divided in half, horizontally and vertically,\ngiving four equal-sizes rectangles.  As a first step, compute a half-size image that is the \naverage of those four rectangles.  That is, the color of a pixel in the half-size image\nis the average of the colors of four pixels in the original.\nThe averaged image can be computed by drawing a half-size rectangle\nto the second framebuffer, using multiple samples from the image in the first texture.  Here is \na fragment shader that does the work:", "In this first pass, the square with vertices at (0,0) and (0.5,0.5) is rendered, and\n", " is 0.5.  The drawing is done in a coordinate system in which the square with\nvertices (0,0) and (1,1) covers the entire drawing area.  In that coordinate system, the\nsquare with vertices at (0,0) and (0.5,0.5) covers the lower left quarter of the drawing area.\nThe first sample in the fragment shader comes from that quarter of the texture image, \nand the other three samples come from corresponding points in the other three quarters of the the image.", "In a second pass, the roles of the two framebuffers are swapped, and a square with vertices\nat (0,0) and (0.25,0.25) is drawn, using the same fragment shader with ", " equal\nto 0.25.  Since the framebuffers were swapped, the second pass is sampling the half-sized image \nthat was produced in the first pass.  The result is a quarter-sized image \nthat is the average of four rectangles that cover the half-sized image\u2014and therefore of 16 rectangles\nthat cover the original image.  This can be\nrepeated, with smaller and smaller squares, until the resulting image is small enough that\nits colors can be efficiently read back into the CPU and averaged there.  The result is\na color value that is the average of all the pixels from the original image.  We expect that,\nbecause a lot of the work is done in parallel by the GPU, we can get the answer much faster using\nthis technique than if we had simply done all the computations on the CPU.", "The point here is that for an accurate result, we want the color components to be represented\nas floating point values in the GPU, not as eight-bit integers.", "I use this technique in the sample program ", ".\nIn that program, the problem is to find the average ", " in color between two\nimages.  I start by drawing the two images to two textures. I then\nrender a difference image, in which the color of a pixel is the\nabsolute value of the difference between the colors of the same pixel in the two textures.\nThis is done with another special-purpose shader program.  I then apply the above averaging\nprocess to the difference image.", "The actual point of the sample program is to try to \"evolve\" an approximation to a given image,\nusing a \"genetic algorithm.\" (It was inspired by two students from my Fall, 2015 class,\nFelix Taschbach and Pieter Schaap, who worked on a similar program for their final project, \nthough they didn't use the GPU.)  In the program, the average difference between the original\nimage and an approximation is used as a measure of how good the approximation is.\nI used a very simple grayscale image as the goal, with\napproximations made from triangles.  You don't need to know anything about the genetic algorithm, \nespecially since the program has no practical purpose.  However,\nthe source code is heavily commented if you want to try to understand it.  Here is a screenshot\nfrom one run of the program, showing the original image and the best approximation produced after running\nthe genetic algorithm for several hundred generations:", "\n", "(See also ", ", from generation number 15,000.)", "But what interests us here is how the program uses the WebGL floating-point color extensions.\nThe program attempts to activate the extensions during initialization using the following code:", "The program requires the extensions, so an exception is thrown if they can't be activated.\nThe extension objects, ", " and ", ", dont't have any properties that are needed\nin this program; however, it is still necessary to call ", " to activate\nthe extensions.  (I only test that OES_texture_float is available.  Historically, that extension included the\nfunctionality that I need from WEBGL_color_buffer_float, and a browser that has support for\nOES_texture_float can probably run the program whether or not it says that it supports the\nother extension.  Officially, new programs are supposed to activate both extensions explicitly.)", "The program creates two floating-point textures that are attached to framebuffers for use as\ncolor buffers.  (See ", ".)  Here is the code that creates one of those\ntextures:", "The parameter ", " in the last line specifies that the data type for the color components in the texture\nis ", ".  That data type would be an error if the extensions had not been activated.", "When the GPU does the averaging computation with these textures, it is doing floating-point\ncalculations.  The program computes a series of smaller and smaller averaged images, stopping\nwith a 4-by-4 pixel image.  It then reads the 16 pixel colors back from the texture using the\nfollowing code:", "The call to ", " reads the color data for the 16 pixels into the\narray, ", ".  Again, the ", " parameter specifies the data type, and that parameter\nvalue is legal in ", " only because the extensions have been activated.", "I will discuss one more WebGL extension, one that is useful for an important\nrendering technique called ", ".  I don't have a sample\nprogram for deferred rendering, and I will only discuss it in general terms.", "Deferred shading is used as\nan optimization when rendering complex scenes, and it is often used to speed up rendering\nin video games.  It is most closely associated with lighting, since it can be used to\nrender scenes with large numbers of light sources, but it can also be useful for other effects.", "Recall that the number of lights that can be represented in OpenGL or in a WebGL shader\nis limited.  But scenes with many lights can be rendered using a ", ":\nEach pass computes the contribution of one light, or a small number of lights, and the results\nof the passes are added together to give the complete scene.  The problem is that, if the rendering in each pass\nis done in the normal way, then there are a lot of things that have to be recomputed, in exactly\nthe same way, in each pass.  For example, assuming that ", " is used, that includes computing \n", " properties and a ", " vector for each pixel in the image.  Deferred shading\naims to avoid the duplicated effort.", "In deferred shading, a first pass is used to compute material properties, normal vectors, and whatever other\ndata is needed, for each pixel in the image.  All of that data is saved, to be used in additional passes that will compute\nlighting and possibly other effects.  For a given pixel, only the data for the object that is actually visible at the pixel is\nsaved, since data for hidden surfaces is not needed to render the scene.  The first pass uses the geometry and attributes\nof objects in the scene.  Everything that the later passes need to know about geometry and attributes is\nin the saved data.", "The saved data can be stored in texture objects.  (Floating point textures are ideal for this, since the\ndata will be used in further calculations.)  In this case, the values in the textures don't necessarily\nrepresent images.  For example, the RGB color components in one texture might represent the x, y, and\nz coordinates of a normal vector.  And if a depth value is needed in later passes, it might be stored in\nthe alpha color component of the same texture.  Another texture might hold a diffuse color, while\na third holds a specular color in its RGB components and a shininess value in its alpha component.\nShader programs are free to interpret data in a texture however they like.", "A WebGL shader can write data to a texture, using a framebuffer.  But standard WebGL can only write\nto one framebuffer at a time.  Now, it would be possible to use a separate pass for each texture that\nwe need to compute, but that would involve a lot of redundant calculations, which is what we are\ntrying to avoid.  What we need is a WebGL extension that makes it possible for a shader to write\nto several framebuffers simultaneously.  The extension that we need is named \"WEBGL_draw_buffers\".\nWhen that extension is activated, it becomes possible to attach several textures (or ", ")\nto a framebuffer, and it becomes possible for a shader to write data to all of the attached\nshaders simultaneously.  The extension is relatively complicated to use.  It must be activated,\nas usual, with a statement of the form", "Assuming that the extension is available, the maximum number of color buffers that can\nbe used in a shader is given by ", ", which will be at least\nfour.  With the extension in place, you can attach multiple textures as color buffers for \na framebuffer, using code of the form", "and so on, using using constants such as ", " from the\nextension object to specify the attachment points.", "Usually in a fragment shader, the color that is output to the color buffer is specified by \nassigning a value to the special variable ", ".  That changes when multiple\ncolor buffers are used.  In that case, instead of ", ", the fragment shader\nhas a special variable ", " which is an array of ", ", one for each\npossible color buffer.  Colors are output to the color buffers by assigning values to\n", "[0], ", "[1], ....  Because this is a change in the legal\nsyntax of the shader, the extension must also be activated in the fragment shader source code\nby adding the line", "to the beginning of the code.  Suppose, for example, that we want to store\na normal vector, a diffuse color, a specular color, and object coordinates in the color\nbuffers.  Let's say that these values are input to the fragment shader as varying\nvariables or uniform variables, except for the diffuse color, which is sampled from\na texture.  Then the fragment shader might take the form", "The final requirement for using the extension is to specify the correspondence between the indices that are\nused in ", " and the color buffers that have been attached to the \nframebuffer.  It seems like the correspondence should be automatic, but it's not.\nYou have to specify it using the JavaScript function, ", "\nfrom the extension object.  This function takes an array as parameter, and the values\nin the array are chosen from the constants ", ",\n", ", ....  These are the same constants that\nare used to specify the color buffer attachment points in a framebuffer.  For example,\nif for some reason you wanted a fragment shader to output to the color buffers that\nare attached at attachment points 2 and 3, you would call", "With all that setup, you are ready to do the first pass for deferred shading.\nFor the subsequent passes, you would use a different shader, with a single color\nbuffer (and with the blend function set to do additive blending).  For those passes,\nyou want to run the fragment shader once for each pixel in the image.  The fragment\nshader will use the pixel data that was saved in the first pass, together with\nother information such as light properties, to compute the output color for the pixel.\nYou can trigger a call to the fragment shader for each pixel simply by drawing a\nsingle rectangle that covers the image.", "The theory behind deferred shading is not all that complicated, but there are\na lot of details to get right in the implementation.  Deferred shading is just\none of many tricks that are used by video game programmers to improve the rendering\nspeed for their games."], "section_id": "Section 7.5", "chapter_id": "Chapter 7", "chapter_title": "3D Graphics with WebGL", "section_title": "WebGL Extensions"}
]