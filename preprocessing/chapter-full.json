[{"section_title": "null", "chapter_id": "Chapter 2", "section_id": "Section 2.0", "content": ["With this chapter, we begin our study of computer graphics by looking at the two-dimensional case.\nThings are simpler, and a lot easier to visualize, in 2D than in 3D, but most of\nthe ideas that are covered in this chapter will also be very relevant to 3D.", "The chapter begins with four sections that examine 2D graphics in a general way,\nwithout tying it to a particular programming language or graphics API.  The coding\nexamples in these sections are written in pseudocode that should make sense to\nanyone with enough programming background to be reading this book.\nIn the next three sections, we will take quick looks at 2D graphics in three\nparticular languages: Java with ", ",\nJavaScript with HTML ", " graphics, and SVG.  We will see how these\nlanguages use many of the general ideas from earlier in the chapter."], "chapter_title": "Two-Dimensional Graphics", "id": 2.0}, {"section_title": "null", "chapter_id": "Chapter 3", "section_id": "Section 3.0", "content": ["It is time to move on to computer graphics in three dimensions, although\nit won't be until Section\u00a02 of this chapter that we really get into 3D.\nYou will find that many concepts from 2D graphics carry over to 3D, but the move into\nthe third dimension brings with it some new features that take a while to\nget used to.", "Our focus will be ", ", a graphics API\nthat was introduced in 1992 and has gone through many versions and many \nchanges since then.  OpenGL is a  low-level graphics API, similar to the 2D APIs we have\ncovered.  It is even more primitive in some ways, but of course it is\ncomplicated by the fact that it supports 3D.", "For the next two chapters, the discussion is\nlimited to OpenGL\u00a01.1.  OpenGL 1.1 is a large API, and we will \nonly cover a part of it. The goal is to introduce 3D graphics concepts, \nnot to fully cover the API.  A significant part of what we cover here\nhas been removed from the most modern versions of OpenGL.  However,\nmodern OpenGL in its pure form has a very steep initial learning curve,\nand it is really not a good starting place for someone who is encountering\n3D graphics for the first time.  Some additional support is needed\u2014if not OpenGL 1.1\nthen some similar framework.  Since OpenGL 1.1 is\nstill supported, at least by all desktop implementations of OpenGL,\nit's a reasonable place to start.", "This chapter concentrates on the geometric aspects of 3D graphics, such as defining\nand transforming objects and projecting 3D scenes into 2D images.  The images that\nwe produce will look very unrealistic.  In the next chapter, we will see how to add\nsome realism by simulating the effects of lighting and of the material properties of surfaces."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.0}, {"section_title": "null", "chapter_id": "Chapter 4", "section_id": "Section 4.0", "content": ["One of the goals of computer graphics is physical realism, that is, making\nimages that look like they could be photographs of reality.  This is not the only \ngoal.  For example, for scientific visualization, the goal is to use computer \ngraphics to present information accurately and clearly.  Artists can use computer\ngraphics to create abstract rather than realistic art.  However, realism is a\nmajor goal of some of the most visible uses of computer graphics, such as video\ngames, movies, and advertising.", "One important aspect of physical realism is ", ":\nthe play of light and shadow, the way that light reflects from different \n", ", the\nway it can bend or be diffracted into a spectrum as it passes through translucent\nobjects.  The techniques that are used to produce the most realistic graphics\ncan take all these factors and more into account.", "However, another goal of computer graphics is ", ".  OpenGL, in particular,\nwas designed for ", ", where the time that is available\nfor rendering an image is a fraction of a second.  For an animated movie, it's OK if\nit takes hours to ", " each frame.  But a video game is expected to\nrender sixty frames every second.  Even with the incredible speed of modern computer graphics\nhardware, compromises are necessary to get that speed.  And twenty years ago, when OpenGL\nwas still new, the compromises were a lot bigger", "In this chapter, we look at light and material in OpenGL 1.1.   You will learn how to\nconfigure light sources and how to assign material properties to objects.  Material properties determine how\nthe objects interact with light.  And you will learn how to apply an image to a surface\nas a ", ".  The support for light, material, and texture in OpenGL 1.1\nis relatively crude and incomplete, by today's standards.  But the concepts that it uses\nstill serve as the foundation for modern real-time graphics and, to a significant extent,\neven for the most realistic computer graphics."], "chapter_title": "OpenGL 1.1: Light and Material", "id": 4.0}, {"section_title": "null", "chapter_id": "Chapter 5", "section_id": "Section 5.0", "content": ["\n", " and ", " introduced 3D graphics \nusing ", "\u00a01.1.  Most of the ideas covered in those chapters remain relevant to modern\ncomputer graphics, but there have been many changes and improvements since the early\ndays of OpenGL.  In the remaining chapters, we will be using ", ",\na modern version of OpenGL that is used to create 3D graphics content for web pages.", "WebGL is a low level language\u2014even more so than OpenGL 1.1, since a WebGL program\nhas to handle a lot of the low-level implementation details that were handled internally\nin the original version of OpenGL.  This makes WebGL much more flexible, but more difficult\nto use.  We will soon turn to working\ndirectly with WebGL.  However, before we do that, we will look\nat a higher-level ", " for 3D web graphics that is built on top of WegGL:  \n", ".  There are several reasons for starting at this high level.\nIt will allow you to see how some of the things that you have learned are used in\na modern graphics package.  It will allow me to introduce some new features such as\nshadows and environment mapping.  It will let you work with a graphics library that\nyou might use in real web applications.\nAnd it will be a break from the low-level detail we have been dealing with, before\nwe move on to an even lower level.", "You can probably follow much of the discussion in this chapter without knowing \nJavaScript.  However, if you want to do any programming with ", " (or with \nWebGL), you need to know JavaScript.  The basics of the language are covered in\n", " in ", "."], "chapter_title": "Three.js: A 3D Scene Graph API", "id": 5.0}, {"section_title": "null", "chapter_id": "Chapter 6", "section_id": "Section 6.0", "content": ["In this chapter, we turn to ", ", the version of ", "\nfor the Web.  ", ", which was covered in the ", ",\nuses WebGL for 3D graphics.  Of course, it is more difficult to use WebGL directly, but doing\nso gives you full control over the graphics hardware.  And learning it will be a good introduction\nto modern graphics programming.", "There have been many versions of OpenGL.  WebGL is based on OpenGL ES 2.0, a version\ndesigned for use on embedded systems such as smart phones and tablets.  OpenGL ES 1.0\nwas very similar to OpenGL 1.1, which we studied in ", "\nand ", ".  However, the 2.0 version of OpenGL\u00a0ES introduced\nmajor changes.  It is actually a smaller, simpler ", " that puts more responsibility\non the programmer.  For example, functions for working with transformations, such\nas ", " and ", ", were eliminated from the API, making the\nprogrammer responsible for keeping track of transformations.  WebGL does not\nuse ", " to generate geometry, and it doesn't use function such\nas ", " or ", " to specify attributes of vertices.  Nevertheless,\nit will turn out that much of what you learned in previous chapters will carry over to\nWebGL.", "There are two sides to any WebGL program.  Part of the program is written in\n", ", the programming language for the web. The second part is\nwritten in ", ", a language for writing \"shader\" programs that run on the\n", ". I will try to always be clear about which language I am\ntalking about.", "For this introductory chapter about WebGL, we will stick to basic \n2D graphics.  You will learn about the structure of WebGL programs.  You will learn\nmost of the JavaScript side of the API, and you learn how to write and use simple\nshaders.  In the ", ", we will move\non to 3D graphics, and you will learn a great deal more about GLSL."], "chapter_title": "Introduction to WebGL", "id": 6.0}, {"section_title": "null", "chapter_id": "Chapter 8", "section_id": "Section 8.0", "content": ["The first seven chapters of this textbook have covered real-time computer graphics,\nthat is, graphics systems in which an image can be generated in a fraction of a second.\nThe typical case is a video game, where new frames can be rendered as many as sixty times\nper second.  Very complex and realistic-looking scenes can be rendered in real time,\nusing techniques covered in this book and the immense processing power of modern\nGPUs, plus some tricks and advanced algorithms.  \nHowever, real-time graphics still can't match the realism\nof the very highest quality computer graphics, such as what can be found in\nmovies.  In fact, the CGI (computer generated imagery) in today's movies is sometimes\nindistinguishable from reality.  Getting graphics of that quality can require\nhours of computing time to render a single frame.  (According to the Wikipedia\narticle about the 2013 animated movie ", ", some complex scenes in that\nmovie used 30 hours of computing to render each frame.)", "This chapter is a very brief look at some techniques that can be used for very\nhigh quality graphics.  The discussion will be in general terms.  I won't be giving \nsample code or detailed discussions of the mathematics behind the techniques,\nbut I hope to provide at least a basic conceptual understanding.", "The first thing that you should understand, though, is\nthat most of what you have leaned so far still applies.  Scenes are still composed\nusing geometric primitives, transformations, materials, textures, and light sources\n(although perhaps using more advanced material properties and lighting than we have\nencountered so far).  The graphic designers working on CGI for a movie can see real \ntime previews of their work that are rendered using techniques that we have covered.\nThe final scene that you see in the movie is just rendered using different,\nmuch more computation-intensive techniques."], "chapter_title": "Beyond Realtime Graphics", "id": 8.0}, {"section_title": "null", "chapter_id": "Chapter 1", "section_id": "Section 1.0", "content": ["The term \"computer graphics\" refers to anything involved in the creation or\nmanipulation of images on computer, including animated images.  It is a very\nbroad field, and one in which changes and advances seem to come at a dizzying pace.\nIt can be difficult for a beginner to know where to start.  However, there is\na core of fundamental ideas that are part of the foundation of most applications\nof computer graphics.  This book attempts to cover those foundational ideas, or\nat least as many of them as will fit into a one-semester college-level course.\nWhile it is not possible to cover the entire field in a first course\u2014or even a large\npart of it\u2014this should be a good place to start.", "This short chapter provides an overview and introduction to the material\nthat will be covered in the rest of the book, without going into a lot of detail."], "chapter_title": "Introduction", "id": 1.0}, {"section_title": "null", "chapter_id": "Chapter 7", "section_id": "Section 7.0", "content": ["The previous chapter covered WebGL, but only in the context of\ntwo-dimensional graphics.   As we move into 3D, we will have to\nwork with more complex ", ".\nFor that, we will rely mainly on an open-source JavaScript library for \nvector and matrix math.  We will also need to implement\n", " and ", ", which we will\ndo directly in ", ".", "We begin the chapter by duplicating most of the capabilities of ", "\u00a01.1\nthat were covered in ", " and ", ".\nBut we will soon move beyond that by adding features such as\n", ", ", ", and\n", "."], "chapter_title": "3D Graphics with WebGL", "id": 7.0}, {"section_title": "Transforms", "chapter_id": "Chapter 2", "section_id": "Section 2.3", "content": ["In ", ", we discussed ", "\nand how it is possible to transform coordinates from one coordinate system to another.  In this section,\nwe'll look at that idea a little more closely, and also look at how \n", " can\nbe used to place graphics objects into a coordinate system.", "In a typical application, we have a rectangle made of pixels, with its natural pixel coordinates, \nwhere an image will be displayed.  This rectangle will be called the ", ".\nWe also have a set of geometric objects that are defined in a possibly different coordinate system,\ngenerally one that uses real-number coordinates rather than integers.  These objects make up the\n\"scene\" or \"world\" that we want to view, and the coordinates that we use to define the scene\nare called ", ".", "For 2D graphics, the world\nlies in a plane.  It's not possible to show a picture of the entire infinite plane.  We need to pick some rectangular\narea in the plane to display in the image.  Let's call that rectangular area the ", ",\nor view window.  A coordinate transform is used to map the window to the viewport.", "\n", "In this illustration, ", " represents the coordinate transformation.  ", "\u00a0is a function that\ntakes world coordinates (", ",", ") in some window and maps them to pixel coordinates ", "(", ",", ")\nin the viewport.  (I've drawn the viewport and window with different sizes to emphasize that they\nare not the same thing, even though they show the same objects, but in fact they don't even exist in\nthe same space, so it doesn't really make sense to compare their sizes.) In this example, as you\ncan check,", "Look at the rectangle with corners at (-1,2) and (3,-1) in the window. When this rectangle\nis displayed in the viewport, it is displayed as the\nrectangle with corners ", "(-1,2) and ", "(3,-1). In this example,\n", "(-1,2)\u00a0=\u00a0(300,100) and ", "(3,-1)\u00a0=\u00a0(700,400).", "We use coordinate transformations in this way because it allows us to choose a world\ncoordinate system that is natural for describing the scene that we want to display, and it\nis easier to do that than to work directly with viewport coordinates.\nAlong the same lines, suppose that we want to define some complex object, and suppose that there will be several\ncopies of that object in our scene.  Or maybe we are making an animation, and we would like the\nobject to have different positions in different frames.  We would like to choose\nsome convenient coordinate system and use it to define the object once and for all.\nThe coordinates that we use to define an object are called ", "\nfor the object.  When we want to place the object into a scene, we need to transform\nthe object coordinates that we used to define the object into the world coordinate system\nthat we are using for the scene.  The transformation that we need is called a\n", ".  This picture illustrates an object defined\nin its own object coordinate system and then mapped by three different modeling transformations\ninto the world coordinate system:", "\n", "Remember that in order to view the scene, there will be another transformation that maps the object\nfrom a view window in world coordinates into the viewport.", "Now, keep in mind\nthat the choice of a view window tells which part of the scene is shown in the image.  Moving,\nresizing, or even rotating the window will give a different view of the scene.  Suppose we make\nseveral images of the same car:", "\n", "What happened between making the top image in this illustration and making the image on the bottom left?\nIn fact, there are two possibilities:  Either the car was moved to the ", ", or the view window that\ndefines the scene was moved to the ", ".  This is important, so be sure you understand it.\n(Try it with your cell phone camera. Aim it at some objects, take a step to the left, and notice\nwhat happens to the objects in the camera's viewfinder: They move to the right  in the picture!)\nSimilarly, what happens between the top picture and the middle picture on the bottom?  Either\nthe car rotated ", ", or the window was rotated ", ".  (Again, try it with a\ncamera\u2014you might want to take two actual photos so that you can compare them.)  Finally,\nthe change from the top picture to the one on the bottom right could happen because the car got\n", " or because the window got ", ".  (On your camera, a bigger window means that\nyou are seeing a larger field of view, and you can get that by applying a zoom to the camera or by\nbacking up away from the objects that you are viewing.)", "There is an important general idea here.  When we modify the view window, we change the\ncoordinate system that is applied to the viewport.  But in fact, this is the same as leaving\nthat coordinate system in place and moving the objects in the scene instead.  \nExcept that to get the same effect\nin the final image, you  have to apply the opposite transformation to the objects (for example,\nmoving the window to \nthe ", " is equivalent to moving the objects to the ", ").  So, there is\nno essential distinction between transforming the window and transforming the object.  Mathematically,\nyou specify a ", " by giving coordinates in some natural coordinate system, \nand the computer applies a sequence of transformations to those coordinates to produce, in the end,\nthe coordinates that are used to actually draw the primitive in the image.  You will think of some of\nthose transformations as modeling transforms and some as coordinate transforms, but to the computer,\nit's all the same.", "\nHere is a live demo that can help you to understand the equivalence between modeling transformations\nand viewport transformations.  The sliders control objects applied to the objects in the picture.\nIn the lower section of the demo, you see a larger view in which the viewport for the upper\nimage is represented as a translucent black rectangle.\n\nRead the help text in the demo for more information.\n", "\n", "\n", "We will return to this idea several times later in the book, but in any case, you can see that\n", " are a central concept in computer \ngraphics.  Let's look at some basic types of transformation in more detail.  The transforms\nwe will use in 2D graphics can be written in the form", "where (", ",", ") represents the coordinates of some point before the transformation\nis applied, and (", ",", ") are the transformed coordinates.  The transform is\ndefined by the six constants ", ", ", ", ", ", ", ", ", ", and ", ".  Note\nthat this can be written as a function ", ", where", "A transformation of this form is called an ", ".  An affine\ntransform has the property that, when it is applied to two parallel lines, the transformed \nlines will also be parallel.  Also, if you follow one affine transform by another affine\ntransform, the result is again an affine transform.", "A ", " transform simply moves every point by a certain amount \nhorizontally and a certain amount vertically.  If (", ",", ") is the \noriginal point and (", ",", ") is the transformed point, then the formula for a translation is", "where ", " is the number of units by which the point is moved horizontally and ", " \nis the amount by which it is moved vertically.\n(Thus for a translation, ", " = ", " = 1, and ", " = ", " = 0 in the\ngeneral formula for an affine transform.)\nA 2D graphics system will typically have a function such as", "to apply a translate transformation.  The translation would apply to everything that is\ndrawn ", " the command is given.  That is,  for all\nsubsequent drawing operations, ", " would be added to the x-coordinate and ", " would\nbe added to the y-coordinate.  Let's look at an example.  Suppose that you draw\nan \"F\" using coordinates in which the \"F\" is centered at (0,0).  \nIf you say ", "(4,2) ", " drawing the \"F\", then every point of the \"F\" \nwill be moved horizontally by 4 units and vertically by 2 units before the coordinates are\nactually used, so that after the translation, the \"F\" will be centered at (4,2):\n", "\n", "The light gray \"F\" in this picture shows what would be drawn without the translation; the\ndark red \"F\" shows the same \"F\" drawn after applying a translation by (4,2).  The top arrow shows\nthat the upper left corner of the \"F\" has been moved over 4 units and up 2 units.  Every point\nin the \"F\" is subjected to the same displacement.  Note that in my examples, I am assuming that\nthe y-coordinate increases from bottom to top.  That is, the y-axis points up.", "Remember that when you give the command ", "(", ",", "), the translation applies\nto ", " the drawing that you do after that, not just to the next shape that you draw.\nIf you apply another transformation after the translation, the second transform will not\nreplace the translation.  It will be combined with the translation, so that subsequent drawing\nwill be affected by the combined transformation.  For example, if you combine\n", "(4,2) with ", "(-1,5), the result is the same as a single\ntranslation, ", "(3,7).  This is an important point, and there will be a lot more to say about it later.", "Also remember that you don't compute coordinate transformations yourself.  You just specify the\noriginal coordinates for the object (that is, the object coordinates), and you specify the\ntransform or transforms that are to be applied. The computer takes care of applying the\ntransformation to the coordinates.  You don't even need to know the equations that are used\nfor the transformation; you just need to understand what it does geometrically.", "A ", " transform, for our purposes here, rotates each point about the origin, (0,0).\nEvery point is rotated through the same\nangle, called the angle of rotation.  For this purpose, angles can be measured either\nin degrees or in radians.  (The 2D graphics ", " that we will look at\nlater in this chapter use radians, but OpenGL uses degrees.)\nA rotation with a positive angle rotates objects in the direction from the positive x-axis towards the positive y-axis.  \nThis is counterclockwise in a coordinate system where the y-axis points up, \nas it does in my examples here, but it is\nclockwise in the usual pixel coordinates, where the y-axis points down rather than up.\nAlthough it is not obvious, when rotation through\nan angle of ", " radians about the origin is applied to the point (", ",", "),\nthen the resulting point (", ",", ") is given by\n", "That is, in the general formula for an affine transform, ", " = ", " = 0,\n", " = ", " = cos(", "), ", " = -sin(", "), and ", " = sin(", ").\nHere is a picture that illustrates a rotation about the origin by the angle  negative 135 degrees:\n", "\n", "Again, the light gray \"F\" is the original shape, and the dark red \"F\" is the shape that\nresults if you apply the rotation.  The arrow shows how the upper left corner of the original \"F\"\nhas been moved.", "A 2D graphics API would typically have a command ", "(", ") to apply a rotation.\nThe command is used ", " drawing the objects to which the rotation applies.", "We are now in a position to see what can happen when you\ncombine two transformations.  Suppose that before drawing some object, you say\n", "Assume that angles are measured in degrees.  \nThe translation will then apply to all subsequent drawing.  But, because of the rotation command,\nthe things that you draw after the translation are ", " objects.  That is, the translation\napplies to objects that have ", " been rotated.  \nAn example is shown on the left in the illustration below, where the light gray \"F\" is the original shape, and\nred \"F\" shows the result of applying the two transforms to the original.  The original \"F\"\nwas first rotated through a 90 degree angle, and then moved 4 units to the right.", "\n", "Note that transforms are\napplied to objects in the reverse of the order in which they are given in the code (because the\nfirst transform in the code is applied to an object that has already been affected by the second\ntransform).  And note that the order in which the transforms are applied is important.  If we reverse\nthe order in which the two transforms are applied in this example, by saying", "then the result is as shown on the right in the above illustration. In that picture,\nthe original \"F\" is first moved 4 units to the right and the resulting shape\nis then rotated through an angle of 90 degrees about the origin to give the shape that actually appears\non the screen.", "For another example of applying several transformations, suppose that we want to rotate a shape through\nan angle ", " about a point (", ",", ") instead of about the point (0,0).  We can do this by\nfirst moving the point (", ",", ") to the origin, using ", "(", ",", ").\nThen we can do a standard rotation about the origin by calling ", "(", "). Finally,\nwe can move the origin back to the point (", ",", ") by applying ", "(", ",", ").\nKeeping in mind that we have to write the code for the transformations in the reverse order, we need to say\n", "before drawing the shape.  (In fact, some graphics APIs let us accomplish this transform with a\nsingle command such as ", "(", ",", ",", ").  This would apply a rotation\nthrough the angle ", " about the point (", ",", ").)", "A ", " transform can be used to make objects bigger or smaller. Mathematically,\na scaling transform simply multiplies each x-coordinate by a given amount and each y-coordinate by\na given amount. That is, if a point (", ",", ") is scaled by a factor of ", " in the\nx direction and by a factor of ", " in the y direction, then the resulting point (", ",", ")\nis given by", "If you apply this transform to a shape that is centered at the origin, it will stretch the shape\nby a factor of ", " horizontally and ", " vertically.  Here is an example, in which the\noriginal light gray \"F\" is scaled by a factor of 3 horizontally and 2 vertically to give the\nfinal dark red \"F\":", "\n", "The common case where the horizontal and vertical scaling factors are the same is\ncalled ", ".  Uniform scaling stretches or shrinks a shape without\ndistorting it.", "When scaling is applied to a shape that is not centered at (0,0), then in addition to being\nstretched or shrunk, the shape will be moved away from 0 or towards 0.  In fact, the true description\nof a scaling operation is that it pushes every point away from (0,0) or pulls every point towards (0,0).\nIf you want to scale about a point other than (0,0), you can use a sequence of three transforms,\nsimilar to what was done in the case of rotation.", "A 2D graphics API can provide a function ", "(", ",", ") for\napplying scaling transformations.  As usual, the transform applies to all ", " and ", "\ncoordinates in subsequent drawing operations. Note that negative scaling factors are allowed and will result in reflecting the\nshape as well as possibly stretching or shrinking it.  For example, ", "(1,-1) will\nreflect objects vertically, through the ", "-axis.", "It is a fact that every affine transform can be created by combining translations, rotations\nabout the origin, and scalings about the origin.  I won't try to prove that, but \nhere is an\ninteractive demo that will let you experiment with translations, rotations, and scalings, and with the\ntransformations that can be made by combining them.", "\n", "\n", "I also note that a transform that is made from translations and rotations, with no scaling, will preserve\nlength and angles in the objects to which it is applied.  It will also preserve\n", " of rectangles.  Transforms with this property\nare called \"", ".\"   If you also allow ", " \nscaling, the resulting transformation will preserve angles and aspect ratio, but not lengths.", "We will look at one more type of basic transform, a ", ".\nAlthough shears can in fact be built up out of rotations and scalings if necessary, it is not\nreally obvious how to do so.  A shear will \"tilt\" objects.  A horizontal shear will tilt things \ntowards the left (for negative shear) or right (for positive shear).  A vertical shear tilts them\nup or down.  Here is an example of horizontal shear: ", "\n", "A horizontal shear does not move the x-axis.  Every other horizontal line is moved to the left\nor to the right by an amount that is proportional to the y-value along that line.  When a horizontal\nshear is applied to a point (", ",", "), the resulting point (", ",", ") is\ngiven by", "for some constant shearing factor ", ".  Similarly, a vertical shear with shearing factor ", "\nis given by the equations", "Shear is occasionally called \"skew.\"", "The last transformation that is applied to an object before it is displayed in an\nimage is the window-to-viewport transformation, which maps the rectangular ", "\nin the xy-plane that contains the scene to the rectangular grid of pixels where the \nimage will be displayed.  I'll assume here that the view window is not rotated; that it, its\nsides are parallel to the x- and y-axes.  In that case, the window-to-viewport transformation\ncan be expressed in terms of translation and scaling transforms.  Let's look at the\ntypical case where the viewport has pixel coordinates ranging from 0 on the left to \n", " on the right and from 0 at the top to ", " at the bottom.\nAnd assume that the limits on the view window are ", ", ", ",\n", ", and ", ".  In that case, the window-to-viewport transformation\ncan be programmed as:", "These should be the last transforms that are applied to a point.  Since transforms\nare applied to points in the reverse of the order in which they are specified in the\nprogram, they should be the first transforms that are specified in the program. To see how this works,\nconsider a point (", ",", ") in the view window.  (This point comes from some object in\nthe scene.  Several modeling transforms might have already been applied to the\nobject to produce the point (", ",", "), and that point is now ready for its final transformation\ninto viewport coordinates.)  The coordinates (", ",", ") are first translated by (", ",", ")\nto give (", ",", ").  These coordinates are then multiplied by the\nscaling factors shown above, giving the final coordinates", "Note that the point (", ",", ") is mapped to (0,0), while the\npoint (", ",", ") is mapped to (", ",", "),\nwhich is just what we want.", "There is still the question of ", ".  As noted in\n", ", if we want to force the aspect ratio of the\nwindow to match the aspect ratio of the viewport, it might be necessary\nto adjust the limits on the window.   Here is pseudocode for a subroutine\nthat will do that, again assuming that the top-left corner of the viewport\nhas pixel coordinates (0,0):", "The transforms that are used in computer graphics can be represented as\nmatrices, and the points on which they operate are represented as\nvectors.  Recall that a ", ", from the point of view of a\ncomputer scientist, is a two-dimensional array of numbers, while a\n", " is one-dimensional.  Matrices and vectors are\nstudied in the field of mathematics called ", ".\nLinear algebra is fundamental to computer graphics.  In fact,\nmatrix and vector math is built into ", ".\nYou won't need to know a great deal about linear algebra for this textbook,\nbut a few basic ideas are essential.", "The vectors that we need are lists of two, three, or four numbers.  They\nare often written as (", ",", "), (", ",", ",", "), and (", ",", ",", ",", ").  A matrix with N rows\nand M columns is called an \"N-by-M matrix.\"  For the most part,\nthe matrices that we need are N-by-N matrices, where N is 2, 3, or 4.\nThat is, they have 2, 3, or 4 rows and columns, and the number of \nrows is equal to the number of columns.", "If ", " and ", " are two N-by-N matrices, then they can be multiplied to\ngive a product matrix ", "\u00a0=\u00a0", ".  If ", "\u00a0is an N-by-N\nmatrix, and ", " is a vector of length N, then ", " can be multiplied\nby ", " to give another vector ", "\u00a0=\u00a0", ".  The function\nthat takes ", " to ", " is a transformation; it transforms any given\nvector of length N into another vector of length\u00a0N.  A\u00a0transformation of this\nform is called a ", ".", "Now, suppose that ", " and ", " are N-by-N matrices and ", " is \na vector of length N.  Then, we can form two different products:\n", "(", ") and (", ".  It is a central fact that these\ntwo operations have the same effect.  That is, we can multiply ", " by ", "\nand then multiply the result by ", ", or we can multiply the matrices\n", " and ", " to get the matrix product ", " and then multiply\n", " by ", ".  The result is the same.", "Rotation and scaling, as it turns out, are linear transformations.  That\nis, the operation of rotating (", ",", ") through an angle\u00a0", "\nabout the origin can be done by multiplying (", ",", ") by a 2-by-2 matrix.\nLet's call that matrix ", ".  Similarly, scaling by a factor\n", " in the horizontal direction and ", " in the vertical direction\ncan be given as a matrix ", ".  If we want to apply\na scaling followed by a rotation to the point ", " = (", ",", "), we can\ncompute ", "  ", "(", ") ", "\n(", ")", ".", "So what?  Well, suppose that we want to apply the same two operations, scale then rotate, \nto thousands of points,  as we typically do when transforming objects for computer graphics.  The point \nis that we could compute the product matrix ", " once \nand for all, and then apply the combined transform to each point with a single multiplication.\nThis means that if a program says", "the computer doens't have to keep track of two separate operations.  It combines the\noperations into a single matrix and just keeps track of that. Even if you apply, say, 50 \ntransformations to the object, the computer can just combine them all into one matrix.\nBy using matrix algebra, multiple transformations can be handled as efficiently as a\nsingle transformation!", "This is really nice, but there is a gaping problem: ", "\nTo bring translation into this framework, we do something that looks a little strange at first:\nInstead of representing a point in 2D as a pair of numbers (", ",", "), we represent\nit as the triple of numbers (", ",", ",1).  That is, we add a one as the third coordinate.\nIt then turns out that we can then represent rotation, scaling, and translation\u2014and hence\nany affine transformation\u2014on 2D space as multiplication by a 3-by-3 matrix.  The matrices \nthat we need have a bottom row containing (0,0,1).   Multiplying (", ",", ",1)\nby such a matrix gives a new vector (", ",", ",1).  We ignore the extra coordinate and consider this\nto be a transformation of (", ",", ") into (", ",", "). For the record, the 3-by-3\nmatrices for translation (", "), scaling (", "),\nand rotation (", ") in 2D are", "\n", "You can compare multiplication by these matrices to the formulas given above for translation, scaling,\nand rotation.  However, you won't need to do the multiplication yourself.  For now,\nthe important idea that you should take away from this discussion is that a sequence of transformations\ncan be combined into a single transformation.  The computer only needs to keep track of a single matrix, which we\ncan call the \"current matrix\" or \"current transformation.\"  To implement transform commands such as ", "(a,b)\nor ", "(d), the computer simply multiplies the current matrix by the matrix that represents the\ntransform."], "chapter_title": "Two-Dimensional Graphics", "id": 2.3}, {"section_title": "Shapes", "chapter_id": "Chapter 2", "section_id": "Section 2.2", "content": ["We have been talking about low-level graphics concepts like ", " \nand ", ", but\nfortunately we don't usually have to work on the lowest levels.  Most graphics systems let\nyou work with higher-level shapes, such as triangles and circles, rather than individual\npixels.  And a lot of the hard work with coordinates is done using \n", " rather than by working with coordinates\ndirectly.  In this section and the next, we will look at some of the higher-level capabilities\nthat are typically provided by 2D graphics APIs.", "In a graphics ", ", there will be certain basic shapes that can be drawn with one command, whereas\nmore complex shapes will require multiple commands.  Exactly what qualifies as a basic shape varies\nfrom one API to another.  In the JavaScript API for drawing on an ", ", for example,\nthe only basic shapes are lines and rectangles.  In this subsection, I consider lines, rectangles, and ovals\nto be basic.", "By \"line,\" I really mean line segment, that is a straight line segment connecting two given\npoints in the plane.  A simple one-pixel-wide line segment, without ", ", is \nthe most basic shape.  It can be drawn by coloring pixels that lie along the infinitely thin\ngeometric line segment.  An algorithm for drawing the line has to decide exactly which pixels\nto color.  One of the first computer graphics algorithms, \n", " for line drawing, implements\na very efficient procedure for doing so.  I won't discuss such low-level details here, but it's\nworth looking them up if you want to start learning about what graphics hardware actually has to do.\nIn any case, lines are typically more complicated. Antialiasing is one complication.  Line width is\nanother.  A wide line might actually be drawn as a rectangle.", "Lines can have other ", ", or properties, that affect\ntheir appearance. One question is, what should happen at the end of a wide line?  Appearance might\nbe improved by adding a rounded \"cap\" on the ends of the line.  A square cap\u2014that is, extending the\nline by half of the line width\u2014might also make sense.  Another question is, when two lines meet\nas part of a larger shape, how should the lines be joined?  And many graphics systems support lines \nthat are patterns of dashes and dots.  This illustration shows some of the possibilities:", "\n", "On the left are three wide lines with no cap, a round cap, and a square cap.  The geometric line\nsegment is shown as a dotted line.  (The no-cap style is called \"butt.\")  To the right are four lines\nwith different patters of dots and dashes.  In the middle are three different styles of line joins:\nmitered, rounded, and beveled.", "The basic rectangular shape has sides that are vertical and horizontal.  (A tilted rectangle generally\nhas to be made by applying a ", ".) Such a rectangle can be specified \nwith two points, (x1,y1) and (x2,y2), that give the endpoints of one of the diagonals of the rectangle.\nAlternatively, the width and the height can be given, along with a single base point, (x,y).  In that\ncase, the width and height have to be positive, or the rectangle is empty.  The base point (x,y) will\nbe the upper left corner of the rectangle if y increases from top to bottom, and it will be the\nlower left corner of the rectangle if y increases from bottom to top.", "\n", "Suppose that you are given points (x1,y1) and (x2,y2), and that you want to draw the rectangle\nthat they determine.  And suppose that the only rectangle-drawing command that you have available\nis one that requires a point (x,y), a width, and a height. For that command, x must be the\nsmaller of x1 and x2, and the width can be computed as the absolute value of x1 minus x2. And\nsimilarly for y and the height.  In pseudocode,\n", "A common variation on rectangles is to allow rounded corners.  For a \"round rect,\" the corners\nare replaced by elliptical arcs.  The degree of rounding can be specified by giving the horizontal radius\nand vertical radius of the ellipse.  Here are some examples of round rects.  For the shape at the\nright, the two radii of the ellipse are shown:", "\n", "My final basic shape is the oval.  (An oval is also called an ellipse.)  An oval is a closed\ncurve that has two radii.  For a basic oval, we assume that the radii are vertical and horizontal.\nAn oval with this property can be specified by giving the rectangle that just contains it.\nOr it can be specified by giving its center point and the lengths of its vertical radius and\nits horizontal radius.  In this illustration, the oval on the left is shown with its\ncontaining rectangle and with its center point and radii:", "\n", "The oval on the right is a circle.  A circle is just an oval in which the two radii have\nthe same length.  ", "If ovals are not available as basic shapes, they can be approximated by drawing a large\nnumber of line segments.  The number of lines that is needed for a good approximation depends on\nthe size of the oval.  It's useful to know how to do this. Suppose that an oval has center point (x,y), \nhorizontal radius r1, and vertical radius r2.  Mathematically, the points on the oval are given by", "where ", " takes on values from 0 to 360 if angles are measured in degrees or\nfrom 0 to 2\u03c0 if they are measured in radians.  Here ", " and ", " are the\nstandard sine and cosine functions.  To get an approximation for an oval, we can use this\nformula to generate some number of points and then connect those points with line segments.\nIn pseudocode, assuming that angles are measured in radians and that ", " represents\nthe mathematical constant\u00a0\u03c0,", "For a circle, of course, you would just have r1 = r2.  This is the first time\nwe have used the sine and cosine functions, but it won't be the last.  These\nfunctions play an important role in computer graphics because of their\nassociation with circles, circular motion, and rotation.  We will meet them\nagain when we talk about transforms in the ", ".", "Here's a little demo\nthat you can use to experiment with using line segements to approximate ovals:", "\n", "\n", "There are two ways to make a shape visible in a drawing.  You can ", " it.\nOr, if it is a closed shape such as a rectangle or an oval, you can ", " it.\nStroking a line is like dragging a pen along the line.  Stroking a rectangle or oval is like\ndragging a pen along its boundary.  Filling a shape means coloring all the points that are contained\ninside that shape.  It's possible to both stroke and fill the same shape; in that case, the\ninterior of the shape and the outline of the shape can have a different appearance.", "When a shape intersects itself, like the two shapes in the illustration below, it's not\nentirely clear what should count as the interior of the shape.  In fact, there are at least\ntwo different rules for filling such a shape.  Both are based on something called the\n", ".  The winding number of a shape about a point is, roughly,\nhow many times the shape winds around the point in the positive direction, which I take here\nto be counterclockwise.\nWinding number can be negative when the winding is in the opposite direction.  \nIn the illustration, the shapes\non the left are traced in the direction shown, and the winding number about each region is \nshown as a number inside the region.", "\n", "The shapes are also shown filled using the two fill rules.  For the shapes in the center,\nthe fill rule is to color any region that has a non-zero winding number.  For the shapes\nshown on the right, the rule is to color any region whose winding number is odd; regions with\neven winding number are not filled.", "There is still the question of what a shape should be filled ", ".  Of course, it\ncan be filled with a color, but other types of fill are possible, including \n", " and ", ". \nA pattern is an image, usually a small image.  When used to fill a shape, a pattern can be\nrepeated horizontally and vertically as necessary to cover the entire shape.\nA gradient is similar in that it is a way for color to vary from point to point, but \ninstead of taking the colors from an image, they are computed.  There are a lot of variations\nto the basic idea, but there is always a line segment along which the color varies.\nThe color is specified at the endpoints of the line segment, and possibly at additional\npoints; between those points, the color is ", ".\nFor other points on the line that contains the line segment, the pattern on the line segment\ncan be repeated, or the color of the endpoint can simply be extended.  For a\n", ", the color is constant along lines perpendicular to the basic\nline segment, so you get lines of solid color going in that direction.\nIn a ", ", the color is constant along circles centered at one\nof the endpoints of the line segment.  And that doesn't exhaust the possibilities.\nTo give you an idea what patterns and gradients can look like,\nhere is a shape, filled with two gradients and two patterns:\n", "\n", "The first shape is filled with a simple linear gradient defined by just two colors,\nwhile the second shape uses a radial gradient.", "Patterns and gradients are not necessarily restricted to filling shapes.  Stroking a shape is,\nafter all, the same as filling a band of pixels along the boundary of the shape,\nand that can be done with a gradient or a pattern, instead of with  a solid color.", "Finally, I will mention that a string of text can be considered to be a shape for the purpose of\ndrawing it.  The boundary of the shape is the outline of the characters.\nThe text is drawn by filling that shape.  In some graphics systems, it is also possible to\nstroke the outline of the shape that defines the text.  \nIn the following illustration, the string \"Graphics\" is shown, on top, filled with a pattern and,\nbelow that, filled with a gradient and stroked with solid black:", "\n", "It is impossible for a graphics API to include every possible shape as a basic shape, but there\nis usually some way to create more complex shapes.   For example, consider\n", ".  A polygon is a closed shape consisting of a\nsequence of line segments.  Each line segment is joined to the next at its endpoint, and the\nlast line segment connects back to the first.  The endpoints are called the vertices of the\npolygon, and a polygon can be defined by listing its vertices.", "In a ", ", all the sides are the same length and all the\nangles between sides are equal.  Squares and equilateral triangles are examples of regular\npolygons.  A ", " has the property that whenever two points are inside\nor on the polygon, then the entire line segment between those points is also inside or on the polygon.\nIntuitively, a convex polygon has no \"indentations\" along its boundary.  (Concavity can be a property\nof any shape, not just of polygons.)", "\n", "Sometimes, polygons are required to be \"simple,\" meaning that the polygon has no self-intersections.\nThat is, all the vertices are different, and a side can only intersect another side at its\nendpoints. And polygons are usually required to be \"planar,\" meaning that all the\nvertices lie in the same plane.  (Of course, in 2D graphics,\n", " lies in the same plane, so this is not an issue.  However, it does become\nan issue in 3D.)", "How then should we draw polygons?  That is, what capabilities would we like to have in a \ngraphics API for drawing them.  One possibility is to have commands for stroking and for\nfilling polygons, where the vertices of the polygon are given as an array of points or as an array\nof x-coordinates plus an array of y-coordinates.  In fact, that is sometimes done; for example,\nthe Java graphics API includes such commands.  Another, more flexible, approach is to introduce\nthe idea of a \"path.\"  Java, SVG, and the HTML canvas API all\nsupport this idea.  A path is a general shape that can include both line\nsegments and curved segments.  Segments can, but don't have to be, connected to other segments\nat their endpoints.  A path is created by giving a series of commands that tell, essentially,\nhow a pen would be moved to draw the path.  While a path is being created, there is a point\nthat represents the pen's current location.  There will be a command for moving the pen without\ndrawing, and commands for drawing various kinds of segments.  For drawing polygons, we\nneed commands such as", "(For ", ", I need to define \"starting point.\"  A path can be made up\nof \"subpaths\"  A subpath consists of a series of connected segments.  A ", "\nalways starts a new subpath.  A ", " ends the current segment and implicitly\nstarts a new one.  So \"starting point\" means the position of the pen after the most recent\n", " or ", ".)", "Suppose that we want a path that represents the triangle with vertices at (100,100), (300,100),\nand (200, 200).  We can do that with the commands", "The ", " command at the end could be replaced by ", ",\nto move the pen back to the first vertex.", "A path represents an abstract geometric object.  Creating\none does not make it visible on the screen.  Once we have a path, to make it visible we need additional\ncommands for stroking and filling the path.", "Earlier in this section, we saw how to approximate an oval by drawing, in effect, a regular\npolygon with a large number of sides.  In that example, I drew each side as a separate line segment,\nso we really had a bunch of separate lines rather than a polygon.  There is no way to fill such\na thing.  It would be better to approximate the oval with a polygonal path.  For an oval with\ncenter (x,y) and radii r1 and r2:", "Using this path, we could draw a filled oval as well as stroke it.  \nEven if we just want to draw the outline of a polygon,\nit's still better to create the polygon as a path rather than to draw the line segments as\nseparate sides.  With a path, the computer knows that the sides are part of single shape.\nThis makes it possible to control the appearance of the \"join\" between consecutive sides, as noted\nearlier in this section.", "I noted above that a path can contain other kinds of segments besides lines.  For example,\nit might be possible to include an arc of a circle as a segment.  Another type of curve\nis a ", ".  Bezier curves can be used to create very general \ncurved shapes.  They are fairly intuitive, so that they are often used in programs that\nallow users to design curves interactively.  Mathematically, Bezier curves are defined\nby parametric polynomial equations, but you don't need to understand what that means to\nuse them.  There are two kinds of Bezier curve in common use, cubic Bezier curves and\nquadratic Bezier curves; they are defined by cubic and quadratic polynomials respectively.\nWhen the general term \"Bezier curve\" is used, it usually refers to cubic Bezier curves.", "A cubic Bezier curve segment is defined by the two endpoints of the segment together\nwith two ", ".  To understand how it works,\nit's best to think about how a pen would draw the curve segment.  The pen starts at the\nfirst endpoint, headed in the direction of the first control point.  The distance of the\ncontrol point from the endpoint controls the speed of the pen as it starts drawing the\ncurve.  The second control point controls the direction and speed of the pen as it gets\nto the second endpoint of the curve.  There is a unique cubic curve that satisfies\nthese conditions.", "\n", "The illustration above shows three cubic Bezier\ncurve segments.  The two curve segments on the right are connected at an endpoint to form a longer\ncurve.  The curves are drawn as thick black lines.  The endpoints are shown as black dots\nand the control points as blue squares, with a thin red line connecting each control point\nto the corresponding endpoint. (Ordinarily, only the curve would be drawn, except in an\ninterface that lets the user edit the curve by hand.)  Note that at an endpoint, the\ncurve segment is tangent to the line that connects the endpoint to the control point.\nNote also that there can be a sharp point or corner where two curve segments meet.  However,\none segment will merge smoothly into the next if control points are properly chosen.\n", "This will all be easier to understand\nwith some hands-on experience. \nThis interative demo lets you edit cubic Bezier curve segments by dragging their endpoints \nand control points:\n", "\n", "\n", "When a cubic Bezier curve segment is added to a path, the path's current pen location acts\nas the first endpoint of the segment.  The command for adding the segment to the path must specify\nthe two control points and the second endpoint.  A typical command might look like", "This would add a curve from the current location to point (x,y), using (cx1,cy1) and (cx2,cy2) as the\ncontrol points.  That is, the pen leaves the current location heading towards (cx1,cy1), and it \nends at the point (x,y), arriving there from the direction of (cx2,cy2).  ", "Quadratic Bezier curve segments are similar to the cubic version, but in the quadratic\ncase, there is only one control point for the segment.  The curve leaves the first endpoint\nheading in the direction of the control point, and it arrives at the second endpoint coming\nfrom the direction of the control point.  The curve in this case will be an arc of a\nparabola.", "Again, this is easier to understand this with some hands-on experience.  Try this interactive demo:", "\n", "\n"], "chapter_title": "Two-Dimensional Graphics", "id": 2.2}, {"section_title": "SVG: A Scene Description Language", "chapter_id": "Chapter 2", "section_id": "Section 2.7", "content": ["We finish this chapter with a look at one more 2D graphics system:\n", ", or Scalable Vector Graphics.  So far, we have\nbeen considering graphics programming APIs.  SVG, on the other\nhand is a ", " rather\nthan a programming language.  Where a programming language creates\na scene by generating its contents procedurally, a scene description\nlanguage specifies a scene \"declaratively,\" by listing its content.\nSince SVG is a ", " language, the content of\nof a scene includes shapes, attributes such as color and line width,\nand geometric transforms.  Most of this should be familiar to you,\nbut it should be interesting to see it in a new context.", "SVG is an ", " language, which means it has a very strict\nand somewhat verbose syntax.  This can make it a little annoying to write,\nbut on the other hand, it makes it possible to read and understand\nSVG documents even if you are not familiar with the syntax.  It's possible\nthat SVG originally stood for \"Simple\" Vector Graphics, but it is by\nno means a simple language at this point.  I will cover only a part of it\nhere, and there are many parts of the language and many options that I will\nnot mention.  My goal is to introduce the idea of a scene description language\nand to show how such a language can use the same basic ideas that are\nused in the rest of this chapter.", "SVG can be used as a file format for storing vector graphics\nimages, in much the same way that PNG and JPEG are file formats for\nstoring pixel-based images.  That means that you can open an SVG\nfile with a web browser to view the image.  (This is true, at least,\nfor modern web browsers.)  An SVG image can be included in a web page\nby using it as the source of an ", " element.  That's how the\nSVG examples on this page are displayed.  Since SVG documents are written in plain text,\nyou can create SVG images using a regular text editor, and you can read the\nsource for an SVG image by opening it in a text editor or by viewing the\nsource of the image when it is displayed in a web browser.", "An SVG file, like any XML document, starts with some standard code that almost\nno one memorizes.  It should just be copied into a new document.  Here\nis some code that can be copied as a starting point for SVG \ndocuments of the type discussed in this section (which, remember use \nonly a subset of the full SVG specification):", "The first three lines say that this is an XML SVG document.  The rest of\nthe document is an ", " element that acts as a container for the entire\nscene description.  You'll need to know a little about XML syntax.\nFirst, an XML \"element\" in its general form looks like this:\n", "The element starts with a \"start tag,\" which begins with a \"<\" followed by an identifier\nthat is the name of the tag, and ending with a\u00a0\">\".  The start tag can include\n\"attributes,\" which have the form ", ".  The ", " is an identifier;\nthe ", " is a string.  The value must be enclosed in single or double quotation marks.\nThe element ends with an \"end tag,\" which has an element name that matches the element name\nin the start tag and has the form </", ">.  Element names and attribute names\nare case-sensitive.  Between the start and end tags\ncomes the \"content\" of the element.  The content can consist of text and nested elements.\nIf an element has no content, you can replace the \">\" at the end of the start tag with\n\"/>\", and leave out the end tag.  This is called a \"self-closing tag.\" For example,\n", "This is an actual SVG element that specifies a circle.  It's easy to forget the \"/\"\nat the end of a self-closing tag, but it has to be there to have a legal XML document.", "Looking back at the SVG document, the five lines starting with <svg are just a long\nstart tag.  You can use the tag as shown, and customize the values of the ", ",\n", ", ", ", and ", " attributes.  The next line\nis a comment; comments in XML start with \"", "\" and end with \"", "\".", "The ", " and ", " attributes of the ", " tag specify a\nnatural or preferred size for the image.  It can be forced into a different size, for\nexample if it is used in an ", " element on a web page that specifies a different\nwidth and height.  The size can be specified using units of measure such as ", " for\ninches, ", " for centimeters, and ", ", for pixels, with 90 pixels to the inch.\nIf no unit of measure is specified, pixels are used.  There cannot be any space between\nthe number and the unit of measure.", "The ", " attribute sets up the ", " that will be used for \ndrawing the image.  It is what I called the ", " in ", ".\nThe value for viewBox is a list of four numbers,\ngiving the minimum ", "value, the minimum ", ", the width, and the height\nof the view window.  The width and the height must be positive, so ", " increases from\nleft-to-right, and ", " increases from top-to-bottom.  The four numbers in the list\ncan be separated either by spaces or by commas; this is typical for lists of numbers in SVG.", "Finally, the ", " attribute tells what happens when the\n", " of the viewBox does not match the aspect ratio of the rectangle\nin which the image is displayed.  The default value, \"xMidYMid\", will extend the limts\non the viewBox either horizontally or vertically to preserve the aspect ratio, and the\nviewBox will appear in the center of the display rectangle.  If you would like your \nimage to stretch to fill the display rectangle, ignoring the aspect ratio, set the\nvalue of ", " to \"none\".  (The aspect ratio issue was\ndiscussed in ", ".)", "Let's look at a complete SVG document that draws a few simple shapes.  Here's the\ndocument.  You could probably figure out what it draws even without knowing any more\nabout SVG:", "and here's the image that is produced by this example:", "\n", "In the drawing coordinate system for this example, ", " ranges from 0 to 3, and\n", " ranges from 0 to 2.  All values used for drawing, including stroke width\nand font size, are given in terms of this coordinate system.  Remember that you can\nuse any coordinate system that you find convenient!  Note, by the way, that parts\nof the image that are not covered by the shapes that are drawn will be transparent.", "Here's another example, with a larger variety of shapes.  The source code for this\nexample has a lot of comments. It uses features that we will discuss in the remainer of\nthis section.", "\n", "You can take a look at the source code, ", ".\n(For example, open it in a text editor, or open it in a web browser and use the\nbrowser's \"view source\" command.)", "In SVG, a basic shape is specified by an element in which the tag name gives the\nshape, and attributes give the properties of the shape.  There are attributes to specify\nthe geometry, such as the endpoints of a line or the radius of a circle.\nOther attributes specify style properties, such as fill color and line width.\n(The style properties are what I call ", " elsewhere\nin this book; in this section, I am using the term \"attribute\" in its XML sense.)\nAnd there is a ", " attribute that can be used to apply a\n", " to the shape.", "For a detailed example, consider the ", " element, which specifies a rectangle.  \nThe geometry of the rectangle is given by attributes named ", ", ", ", ", "\nand ", " in the usual way.  The default value for ", " and ", " is zero;\nthat is, they are optional, and leaving them out is the same as setting their value to zero.\nThe ", " and the ", " are required attributes.  Their values must be\nnon-negative.  For example, the element", "specifies a rectangle with corner at (0,0), width 3, and height 2, while", "gives a rectangle with corner at (100,200), width 640, and height 480.  (Note, by\nthe way, that the attributes in an XML element can be given in any order.)  The ", "\nelement also has optional attributes ", " and ", " that can be used to make\n\"roundRects,\" with their corners replaced by elliptical arcs.  The values of ", "\nand ", " give the horizontal and vertical radii of the elliptical arcs.", "Style attributes can be added to say how the shape should be stroked and filled.\nThe default is to use a black fill and no stroke.  (More precisely, as we will see later,\nthe default for is for a shape to inherit the values of style attributes from its \nenvironment.  Black fill and no stroke is the initial environment.)  Here are some\ncommon style attributes:", "As an example that uses many of these options, let's make a square is rounded rather than pointed \nat the corners, with size 1, centered\nat the origin, and using a translucent red fill and a gray stroke:", "and a simple outline of a rectangle with no fill:", "The ", " attribute can be used to apply a transform or a series of\ntransforms to a shape.  As an example, we can make a rectangle tilted 30 degrees from\nthe horizontal:", "The value \"rotate(30)\" represents a rotation of 30 degrees (not radians!) about the \norigin, (0,0). The positive direction of rotation, as usual, rotates the positive x-axis in the\ndirection of the positive y-axis.  You can specify a different center of rotation by\nadding arguments to ", ".  For example, to rotate the same rectangle about its\ncenter", "Translation and scaling work as you probably expect, with transform values of\nthe form \"translate(", ")\" and \"scale(", ")\".  There are also\n", " transforms, but they go by the\nnames ", " and ", ", and the argument is a skew angle rather\nthan a shear amount.  For example, the transform \"skewX(45)\" tilts the y-axis\nby 45 degrees and is equivalent to an x-shear with shear factor\u00a01.\n(The function that tilts the y-axis is called ", " because it modifies,\nor skews, the x-coordinates of points while leaving their y-coordinates unchanged.)\nFor example, we can use ", " to tilt a rectangle and make it into a\nparallelogram:", "I used an angle of -30 degrees to make the rectangle tilt to the right\nin the usual pixel coordinate system.", "The value of the ", " attribute can be a list of transforms,\nseparated by spaces or commas.  The transforms are applied to the object, as\nusual, in the opposite of the order in which they are listed. So,", "would first skew the rectangle into a parallelogram, then rotate the parallelogram\nby 45 degrees about the origin, then translate it by 50 units in the y-direction.", "In addition to rectangles, SVG has lines, circles, ellipses, and text as basic\nshapes.  Here are some details.  A ", " element represents a line segement and\nhas geometric attributes ", ", ", ", ", ", and ", " to specify the \ncoordinates of the endpoints of the line segment.  These four attributes have\nzero as default value, which makes it easier to specify horizontal and vertical lines.\nFor example,", "Without the ", " attribute, you wouldn't see the line, since the default\nvalue for ", " is \"none\".", "For a ", " element, the geometric attributes are ", ", ", ", and ", "\ngiving the coordinates of the center of the circle and the radius.  The center coordinates\nhave default values equal to zero.  For an ", " element, the attributes are\n", ", ", ", ", ", and ", ", where ", " and ", " give\nthe radii of the ellipse in the x- and y-directions.", "A ", " element is a little different.  It has attributes ", " and ", ",\nwith default values zero, to specify the location of the basepoint of the text.  However,\nthe text itself is given as the content of the element rather than as an attribute.  That is,\nthe element is divided into a start tag and an end tag, and the text that will appear in\nthe drawing comes between the start and end tags.  For example,", "The usual stroke and fill attributes apply to text, but text has additional style\nattributes.  The ", " attribute specifies the font itself.  Its value\ncan be one of the generic font names \"serif\", \"sans-serif\", \"monospace\", or the name of\na specific font that is available on the system.  The ", " can be a number\ngiving the (approximate) height of the characters in the coordinate system.  (Font size\nis subject to coordinate and modeling transforms like any other length.)  You can get\nbold and italic text by setting ", " equal to \"bold\" and\n", " equal to \"italic\".  Here is an example that uses all of these options,\nand applies some additional styles and a transform for good measure:", "SVG has some nice features for making more complex shapes.  The ", " element\nmakes it easy to create a polygon from a list of coordinate pairs.  For example,", "creates a five-sided polygon with vertices at (0,0), (100,0), (100,75), (50,100), and\n(0,75).  Every pair of numbers in the ", " attribute specifies a vertex.  The numbers\ncan be separated by either spaces or commas.  I've used a mixture of spaces and commas here to\nmake it clear how the numbers pair up.   Of course, you can add the usual style attributes\nfor stroke and fill to the polygon element.  A ", " is similar to a ", ",\nexcept that it leaves out the last line from the final vertex back to the starting vertex.\nThe difference only shows up when a polyline is stroked; a polyline is filled as if the\nmissing side were added.", "The ", " element is much more interesting. In fact, all of the other basic shapes,\nexcept text, could be made using path elements.  A path can consist of line segments,\n", ", and elliptical arcs (although I won't\ndiscuss elliptical arcs here).  The syntax for\nspecifying a path is very succinct, and it has some features that we have not seen before.\nA path element has an attribute named ", " that contains the data for the path.  The\ndata consists of one or more commands, where each command consists of a single letter followed\nby any data necessary for the command.  The moveTo, lineTo, cubic Bezier, and quadratic\nBezier commands that you are already familiar with are coded by the letters M, L, C, and Q.\nThe command for closing a path segment is Z, and it requires no data.\nFor example the path data \"M\u00a010\u00a020\u00a0L\u00a0100\u00a0200\" would draw a line segment\nfrom the point (10,20) to the point (100,200).  You can combine several connected line segments\ninto one L command.  For example, the ", " example given above could be created\nusing the ", " element", "The Z at the end of the data closes the path by adding the final side to the polygon.\n(Note that, as usual, you can use either commas or spaces in the data.)", "The C command takes six numbers as data, to specify the two control points and the final\nendpoint of the cubic Bezier curve segment.  You can also give a multiple of six values to get\na connected sequence of curve segements.  Similarly, the Q command uses four data values to\nspecify the control point and final endpoint of the quadratic Bezier curve segment.\nThe large, curvy, yellow shape shown in the picture earlier in this section was created\nas a path with two line segments and two Bezier curve segments:", "SVG paths add flexibility by defining \"relative\" versions of the path commands,\nwhere the data for the command is given relative to the current position.\nA relative move command, for example, instead of telling ", " to move,\ntells ", " to move from the current position.  The names of the \nrelative versions of the path commands are lower case letters instead of upper case.\n\"M\u00a010,20\" means to move to the point with coordinates (10,20), while\n\"m\u00a010,20\" means to move 10 units horizontally and 20 units vertically\nfrom the current position.  Similarly, if the current position is (", "), then\nthe command \"l\u00a03,5\", where the first character is a lower case L, draws a line from (", ") to\n(", "+3,", ").", "SVG would not be a very interesting language if it could only work with\nindividual simple shapes.  For complex scenes, we want to be able to do\n", ", where objects can be constructed from\nsub-objects, and a transform can be applied to an entire complex object.\nWe need a way to group objects so that they can be treated as a unit.\nFor that, SVG has the ", " element.  The content of a ", "\nelement is a list of shape elements, which can be simple shapes or\nnested ", " elements.", "You can add style and ", " attributes to a ", " element.\nThe main point of grouping is that a group can be treated as a single\nobject.  A ", " attribute in a ", " will transform the\nentire group as a whole.  A style attribute, such as ", " or\n", ", on a ", " element will set a default value \nfor the group, replacing the current default.  Here is an example:", "The nested shapes use fill=\"none\" stroke=\"black\" stroke-width=\"2\" for the\ndefault values of the attributes.  The default can be overridden by specifying\na different value for the element, as is done for the stroke-width of the\n", " element in this example.  Setting transform=\"scale(1,\u22121)\"\nfor the group flips the entire image vertically.  I do this only because\nI am more comfortable working in a coordinate system in which y increases\nfrom bottom-to-top rather than top-to-bottom.  Here is the simple line\ndrawing of a face that is produced by this group:", "\n", "Now, suppose that we want to include multiple copies of an object in\na scene.  It shouldn't be necessary to repeat the code for drawing the object.\nIt would be nice to have something like reusable subroutines.  In fact,\nSVG has something very similar: You can define reusable objects inside a\n", " element.  An object that is defined inside ", " is\nnot added to the scene, but copies of the object can be added to the scene\nwith a single command.  For this to work, the object must have an ", " attribute\nto identify it.  For example, we could define an object that looks like a plus sign:", "A ", " element can then be used to add a copy of the plus sign\nobject to the scene.  The syntax is", "The value of the ", " attribute must be the ", " of the object,\nwith a \"#\" character added at the beginning. (Don't forget the\u00a0#.  If you leave it out,\nthe ", " element will simply be ignored.)  You can add a ", " attribute\nto the ", " element to apply a transformation to the copy of the object.  You can also apply\nstyle attributes, which will be used as default values for the attributes in the copy.  For\nexample, we can draw several plus signs with different transforms and stroke widths:", "Note that we can't change the color of the plus sign, since it already specifies\nits own stroke color.", "An object that has been defined in the ", " section can also be used\nas a sub-object in other object definitions.  This makes it possible to create\na hierarchy with multiple levels.  Here is an example from ", "\nthat defines a \"wheel\" object, then uses two copies of the wheel as sub-objects in a \n\"cart\" object:", "The SVG file goes on to add one copy of the wheel and four copies of the\ncart to the image.  The four carts have different colors and transforms.\nHere is the image:", "\n", "SVG has a number of advanced features that I won't discuss here, but I do want to\nmention one: ", ".  It is possible to animate almost any property\nof an SVG object, including geometry, style, and transforms.  The syntax for animation\nis itself fairly complex, and I will only do a few examples.  But I will tell you enough\nto produce a fairly complex hierarchical animation like the \"cart-and-windmills\"\nexample that was discussed and used as a demo in ", ".\nAn SVG version of that animation can be found in ", ".\nHere is what it looks like, although some web browsers might show it as a static\nimage instead of an animation:", "\n", "Many attributes of a shape element can be animated by adding an ", "\nelement to the content of the shape element.   Here is an example that makes a rectangle\nmove across the image from left to right:", "Note that the ", " is nested inside the ", ".\nThe ", " attribute tells which attribute of the ", "\nis being animated, in this case,\u00a0", ".  The ", " and ", " attributes\nsay that ", " will take on values from 0 to 430.  The ", " attribute is the\n\"duration\", that is, how long the animation lasts; the value \"7s\" means \"7 seconds.\"\nThe attribute ", "=\"indefinite\" means that after the animation completes,\nit will start over, and it will repeat indefinitely, that is, as long as the image is\ndisplayed.  If the ", " attribute is omitted, then after the animation\nruns once, the rectangle will jump back to its original position and remain there.\nIf ", " is replaced by ", "=\"freeze\", then after the animation runs,\nthe rectangle will br frozen in its final position, instead of jumping back to the starting\nposition.  The animation begins when the image first loads.  If you want the animation to\nstart at a later time, you can add a ", " attribute whose value gives the time\nwhen the animation should start, as a number of seconds after the image loads.", "What if we want the rectangle to move back and forth between its initial and final\nposition?  For that, we need something called ", ",\nwhich is an important idea in its own right.  The ", " and ", " attributes\nallow you to specify values only for the beginning and end of the animation.  In a keyframe\nanimation, values are specified at additional times in the middle of the animation.\nFor a keyframe animation in SVG, the ", " and ", " attributes are replaced\nby ", " and ", ".  Here is our moving rectangle example,\nmodified to use keyframes:", "The ", " attribute is a list of numbers, separated by semicolons.\nThe numbers are in the range 0 to 1, and should be in increasing order.  The first number\nshould be 0 and the last number should be 1.  A number specifies a time during the animation,\nas a fraction of the complete animation.  For example, 0.5 is a point half-way through the\nanimation, and 0.75 is three-quarters of the way.  The ", " attribute is a list\nof values, with one value for each key time.  In this case, the value for ", " is\n0 at the start of the animation, 430 half-way through the animation, and 0 again at the\nend of the animation.  Between the key times, the value for ", " is obtained by interpolating\nbetween the values specified for the key times.  The result in this case is that the rectangle\nmoves from left to right during the first half of the animation and then back from right to\nleft in the second half.", "Transforms can also be animated, but you need to use the ", "\ntag instead of ", ", and you need to add a ", " attribute to specify\nwhich transform you are animating, such as \"rotate\" or \"translate\".  Here, for example,\nis a transform animation applied to a group:", "The animation shows a growing \"tree\" made from a green triangle and a brown rectangle.\nIn the animation, the transform goes from ", "(0,0) to ", "(0.4,0.7).\nThe animation starts 3 seconds after the image loads and lasts 15 seconds.  At the end\nof the animation, the tree freezes at its final scale.  The ", " attribute\non the ", " element specifies the scale that is in effect until the animation\nstarts.  (A scale factor of 0 collapses the object to size zero, so that it is invisible.)\nYou can find this example, along with a moving rectangle and a keyframe animation, in \nthe sample file ", ". Here is the\nanimation itself.  To see the growing trees, you might have to reload this page or view\nthe image in a separate window:", "\n", "You can create animated objects in the ", " section of an SVG file,\nand you can apply animation to ", " elements.  This makes it possible\nto create hierarchical animations.  Here is a simple example:", "\n", "The example shows a rotating hexagon with a rotating square at each vertex of the\nhexagon.  The hexagon is constructed from six copies of one object, with a different rotation\napplied to each copy.  (A copy of the basic object is shown in the image to the right of the\nhexagon.)  The square is defined as an animated object with its own rotation.  It is used\nas a sub-object in the hexagon.  The rotation that is applied to the hexagon applies to the\nsquare, on top of its own built-in rotation.  That's what makes this an example of\nhierarchical animation.", "If you look back at the ", " \nexample now, you can probably see how to do the animation.  Don't forget to check out the source code,\nwhich is surprisingly short!"], "chapter_title": "Two-Dimensional Graphics", "id": 2.7}, {"section_title": "Pixels, Coordinates, and Colors", "chapter_id": "Chapter 2", "section_id": "Section 2.1", "content": ["To create a two-dimensional image, each point in the image is\nassigned a color.  A point in 2D can be identified by a pair of numerical\ncoordinates.  Colors can also\nbe specified numerically.  However, the assignment of numbers to points\nor colors is somewhat arbitrary.  So we need to spend some time\nstudying ", ", which associate\nnumbers to points, and ", ", which\nassociate numbers to colors.", "A digital image is made up of rows and columns of ", ".\nA pixel in such an image can be specified by saying which column and which row contains\nit.  In terms of coordinates, a pixel can be identified by a pair of integers giving\nthe column number and the row number.  For example, the pixel with coordinates (3,5)\nwould lie in column number 3 and row number 5.  Conventionally, columns are numbered from left\nto right, starting with zero.  Most graphics systems, including the ones we will study\nin this chapter, number rows from top to bottom, starting from zero.  Some, including\nOpenGL, number the rows from bottom to top instead.\n", "\n", "Note in particular that the pixel that is identified by a pair of\ncoordinates (", ",", ") depends on the choice of coordinate system.\nYou always need to know what coordinate system is in use before you know what\npoint you are talking about.", "Row and column numbers identify a pixel, not a point.  A pixel contains many points;\nmathematically, it contains an infinite number of points.  The goal of computer graphics is not\nreally to color pixels\u2014it is to create and manipulate images.  In some ideal\nsense, an image should be defined by specifying a color for each point, not just for\neach pixel.  Pixels are an approximation.  If we imagine that there is a true, ideal\nimage that we want to display, then any image that we display by coloring pixels is\nan approximation.  This has many implications.", "Suppose, for example, that we want to draw a line segment.  A mathematical line\nhas no thickness and would be invisible.  So we really want to draw a thick line\nsegment, with some specified width.  Let's say that the line should be \none pixel wide.  The problem is that, unless the line is horizontal or vertical,\nwe can't actually draw the line by coloring pixels.  A diagonal geometric line will cover some\npixels only partially. It is not possible to make part of a pixel black and part of it white.\nWhen you try to draw a line with black and white pixels only, the result is a jagged\nstaircase effect.  This effect is an example of something called \"aliasing.\"  Aliasing can also be seen\nin the outlines of characters drawn on the screen and in diagonal or curved boundaries between\nany two regions of different color.  (The term aliasing likely comes from the fact that\nideal images are naturally described in real-number coordinates.  When you try to represent\nthe image using pixels, many real-number coordinates will map to the same integer\npixel coordinates; they can all be considered as different names or \"aliases\" for the\nsame pixel.)", "\n", "  is a term for techniques that are designed to\nmitigate the effects of aliasing.  The idea is that when a pixel is only partially\ncovered by a shape, the color of the pixel should be a mixture of the color of the\nshape and the color of the background.  When drawing a black line on a white background,\nthe color of a partially covered pixel would be gray, with the shade of gray depending\non the fraction of the pixel that is covered by the line.  (In practice, calculating this\narea exactly for each pixel would be too difficult, so some approximate method is used.)\nHere, for example, is a geometric line, shown on the left, along with two approximations\nof that line made by coloring pixels.  The lines are greately magnified so that you can see the\nindividual pixels.  The line on the right is drawn using antialiasing, while the one in the \nmiddle is not:", "\n", "Note that antialiasing does not give a perfect image, but it can reduce the \"jaggies\" that \nare caused by aliasing (at least when it is viewed on a normal scale).", "There are other issues involved in mapping real-number coordinates to pixels.\nFor example, which point in a pixel should correspond to integer-valued coordinates\nsuch as (3,5)?  The center of the pixel?  One of the corners of the pixel?\nIn general, we think of the numbers as referring to the top-left corner of the pixel.\nAnother way of thinking about this is to say that integer coordinates refer to the\nlines between pixels, rather than to the pixels themselves.  But that still\ndoesn't determine exactly which pixels are affected when a geometric shape is drawn.\nFor example, here are two lines drawn using HTML canvas graphics,\nshown greatly magnified.  The lines were specified to be colored black with a\none-pixel line width:", "\n", "The top line was drawn from the point (100,100) to the point (120,100).  In\ncanvas graphics, integer coordinates corresponding to the lines between pixels, \nbut when a one-pixel line is drawn, it\nextends one-half pixel on either side of the infinitely thin geometric line.  So for the top line,\nthe line as it is drawn lies half\nin one row of pixels and half in another row.  The graphics system, which uses\nantialiasing, ", " the line by coloring both\nrows of pixels gray.  The bottom line was drawn from the point (100.5,100.5) to\n(120.5,120.5).  In this case, the line lies exactly along one line of pixels,\nwhich gets colored black.  The gray pixels at the ends of the bottom line have to do with\nthe fact that the line only extends halfway into the pixels at its endpoints.\nOther graphics systems might render the same lines differently.", "The following interactive demo lets you experiment with\npixels and antialiasing.\n\n(Note that in any of the interactive demos that accompany this book, you can click\nthe question mark icon in the upper left for more information about how to use it.)", "\n", "\n", "All this is complicated further by the fact that pixels aren't what they used to\nbe.   Pixels today are smaller!  The resolution of a display device can be measured\nin terms of the number of pixels per inch on the display, a quantity referred to\nas PPI (pixels per inch) or sometimes DPI (dots per inch).  Early screens tended to have\nresolutions of somewhere close to 72 PPI.  At that resolution, and at a typical viewing\ndistance, individual pixels are clearly visible.  For a while, it seemed like most\ndisplays had about 100 pixels per inch, but high resolution displays today can have\n200, 300 or even 400 pixels per inch.  At the highest resolutions, individual\npixels can no longer be distinguished.", "The fact that pixels come in such a range of sizes is a problem if we use\ncoordinate systems based on pixels.  An image created assuming that there are\n100 pixels per inch will look tiny on a 400 PPI display.  A one-pixel-wide line\nlooks good at 100 PPI, but at 400 PPI, a one-pixel-wide line is probably\ntoo thin.", "In fact, in many graphics systems, \"pixel\" doesn't really refer to the \nsize of a physical pixel.  Instead, it is just another unit of measure, which is\nset by the system to be something appropriate.  (On a desktop system, a pixel\nis usually about one one-hundredth of an inch.  On a smart phone, which is\nusually viewed from a closer distance, the value might be closer to 1/160 inch.\nFurthermore, the meaning of a pixel as a unit of measure can change when,\nfor example, the user applies a magnification to a web page.)", "Pixels cause problems that have not been completely solved.  Fortunately, they\nare less of a problem for ", ", which is mostly what we\nwill use in this book.  For vector graphics, pixels only become an issue during\n", ", the step in which a vector image is converted\ninto pixels for display.  The vector image itself can be created using any\nconvenient coordinate system.  It represents an idealized, resolution-independent\nimage.  A rasterized image is an approximation of that ideal image, but how to\ndo the approximation can be left to the display hardware.", "When doing 2D graphics, you are given a rectangle in which you want to\ndraw some ", ".\nPrimitives are specified using some coordinate system on the rectangle.\nIt should be possible to select a coordinate system that is appropriate\nfor the application.  For example, if the rectangle represents a floor\nplan for a 15 foot by 12 foot room, then you might want to use a\ncoordinate system in which the unit of measure is one foot and the\ncoordinates range from 0 to 15 in the horizontal direction and 0 to\n12 in the vertical direction.  The unit of measure in this case is feet\nrather than pixels, and one foot can correspond to many pixels in the\nimage.  The coordinates for a pixel will, in general, be real numbers\nrather than integers.  In fact, it's better to forget about pixels\nand just think about points in the image.  A point will have a pair\nof coordinates given by real numbers.", "To specify the coordinate system on a rectangle, you just have\nto specify the horizontal coordinates for the left and right\nedges of the rectangle and the vertical coordinates for the\ntop and bottom.  Let's call these values ", ",\n", ", ", ", and ", ".  Often, they are\nthought of as ", ", ", ", ", ", and ", ",\nbut there is no reason to assume that, for example, ", "\nis less than ", ".  We might want a coordinate system in\nwhich the vertical coordinate increases from bottom to top instead\nof from top to bottom.  In that case, ", " will correspond to\nthe maximum ", "-value instead of the minimum value.", "To allow programmers to specify the coordinates system that\nthey would like to use, it would be good to have a subroutine such as", "The graphics system would then be responsible for automatically\n ", " the  \ncoordinates from the specfiied coordinate system into pixel coordinates.\nSuch a subroutine might not be available, so it's useful to see how the transformation\nis done by hand.  Let's consider the general case.  Given coordinates for a point in \none coordinate system, we want to find the coordinates for the same point in a second \ncoordinate system.  (Remember that a coordinate system is just a way of assigning numbers\nto points.  It's the points that are real!)  Suppose that the horizontal and vertical\nlimits are ", ", ", ", ", ", and ", " for\nthe first coordinate system, and are ", ", ", ", ", ", \nand ", " for the second.  Suppose that a point has coordinates (", ")\nin the first coordinate system.  We want to find the coordinates (", ")\nof the point in the second coordinate system", "\n", "Formulas for ", " and ", " are then given by", "The logic here is that ", " is located at a certain fraction of the distance from ", " to\n", ".  That fraction is given by", "The formula for ", " just says that ", " should lie at the same fraction of the distance\nfrom ", " to ", ".  You can also check the formulas by testing that\nthey work when ", " is equal to ", " or to ", ", and when\n", " is equal to ", " or to ", ".", "As an example, suppose that we want to transform some real-number coordinate system\nwith limits ", ", ", ", ", ", and ", " into pixel\ncoordinates that range from 0 at left to 800 at the right and from 0 at the top\n600 at the bottom.  In that case, ", " and ", " are zero, and \nthe formulas become simply\n", "Of course, this gives ", " and ", " as real numbers, and they will have\nto be rounded or truncated to integer values if we need integer coordinates for pixels.\nThe reverse transformation\u2014going from pixel coordinates to real number coordinates\u2014is\nalso useful.  For example, if the image is displayed on a computer screen, and you want to\nreact to mouse clicks on the image, you will probably get the mouse coordinates in terms\nof integer pixel coordinates, but you will want to transform those pixel coordinates into \nyour own chosen coordinate system.", "In practice, though, you won't usually have to do the transformations yourself, since most\ngraphics APIs provide some higher level way to specify transforms.  We will talk more about\nthis in ", ".", "The ", " of a rectangle is the ratio of its width to its height.\nFor example an aspect ratio of 2:1 means that a rectangle is twice as wide as it is tall,\nand an aspect ratio of 4:3 means that the width is 4/3 times the height.  Although aspect ratios\nare often written in the form ", ":", ", I will use the term to refer to the\nfraction ", ".  A square has aspect ratio equal to\u00a01.  A rectangle with aspect\nratio 5/4 and height 600 has a width equal to 600*(5/4), or 750.", "A coordinate system also has an aspect ratio.  If the horizontal and vertical limits for\nthe coordinate system are ", ", ", ", ", ", and ", ", as \nabove, then the aspect ratio is the absolute value of", "If the coordinate system is used on a rectangle with the same aspect ratio, then when viewed in\nthat rectangle, one unit in the horizontal direction will have the same apparent length as a unit in the\nvertical direction.  If the aspect ratios don't match, then there will be some distortion.\nFor example, the shape defined by the equation ", "\u00a0+", "\u00a0=\u00a09\nshould be a circle, but that will only be true if the aspect ratio of the (", ",", ")\ncoordinate system matches the aspect ratio of the drawing area.", "\n", "It is not always a bad thing to use different units of length in the vertical and horizontal \ndirections.  However, suppose that you want to use coordinates with limits ", ", ", ", \n", ", and ", ", and that you do want to preserve the aspect ratio.  In that case,\ndepending on the shape of the display rectangle, you might have to adjust the values either of\n", " and ", " or of ", " and ", " to make the aspect ratios match:\n", "\n", "We will look more deeply into geometric transforms later in the chapter, and at that time,\nwe'll see some program code for setting up coordinate systems.", "We are talking about the most basic foundations of computer graphics.  One of those is\ncoordinate systems.  The other is color.  Color is actually a surprisingly complex topic.\nWe will look at some parts of the topic that are most relevant to computer graphics\napplications.", "The colors on a computer screen are produced as combinations of red, green, and blue light.\nDifferent colors are produced by varying the intensity of each type of light.  A color can be\nspecified by three numbers giving the intensity of red, green, and blue in the color.\nIntensity can be specified as a number in the range zero, for minimum intensity, to one, for\nmaximum intensity.  This method of specifying color is called the ", ",\nwhere RGB stands for Red/Green/Blue.  For example, in the RGB color model, the number triple \n(1,\u00a00.5,\u00a00.5) represents the color obtained by setting red to full intensity, while \ngreen and blue are set to half intensity. The red, green, and blue values for a color\nare called the ", " of that color \nin the RGB color model.", "Light is made up of waves with a variety of wavelengths. \nA pure color is one for which all the light has the same wavelength,\nbut in general, a color can contain many wavelengths\u2014mathematically,\nan infinite number.  How then can we represent all colors by combining just red, green, and\nblue light?  In fact, we can't quite do that.", "You might have heard that combinations of the three basic, or \"primary,\" colors are sufficient\nto represent all colors, because the human eye has three kinds of color sensors that detect red,\ngreen, and blue light.  However, that is only an approximation.  The eye does contain three\nkinds of color sensor.  The sensors are called \"cone cells.\"\nHowever, cone cells do not respond exclusively to red, green, and blue light.  Each kind\nof cone cell responds, to a varying degree, to wavelengths of light in a wide range.  A given\nmix of wavelengths will stimulate each type of cell to a certain degree, and the intensity of\nstimulation determines the color that we see.  A different mixture of wavelengths that stimulates\neach type of cone cell to the same extent will be perceived as the same color.  So a perceived\ncolor can, in fact, be specified by three numbers giving the intensity of stimulation of\nthe three types of cone cell. However, it is not possible to produce all possible patterns of\nstimulation by combining just three basic colors, no matter how those colors are chosen.  \nThis is just a fact about the way our eyes actually work; it might have been different.\nThree basic colors can produce a reasonably large fraction of the set of perceivable colors,\nbut there are colors that you can see in the world that you will never see on your computer\nscreen.  (This whole discussion only applies to people who actually have three kinds of\ncone cell.  Color blindness, where someone is missing one or more kinds of cone cell, is\nsurprisingly common.)", "The range of colors that can be produced by a device such as a computer screen is called\nthe ", " of that device.  Different computer screens can have different\ncolor gamuts, and the same RGB values can produce somewhat different colors on different screens.\nThe color gamut of a color printer is noticeably different\u2014and probably\nsmaller\u2014than the color gamut of a screen, which explain why a printed image probably\ndoesn't look exactly the same as it did on the screen.  (Printers, by the way, make colors\ndifferently from the way a screen does it.  Whereas a screen combines light to make a color, \na printer combines inks or dyes.  Because of this difference, colors meant for printers are often\nexpressed using a different set of basic colors.  A common color model for printer colors\nis CMYK, using the colors cyan, magenta, yellow, and black.)", "In any case, the most common color model for computer graphics is RGB.  RGB colors are most\noften represented using 8 bits per color component, a total of 24 bits to represent a color.\nThis representation is sometimes called \"24-bit color.\"\nAn 8-bit number can represent 2", ", or 256, different values, which we can take to\nbe the positive integers from 0 to 255. A\u00a0color is then specified as a triple of integers\n(r,g,b) in that range.", "This representation works well because 256 shades of red, green, and\nblue are about as many as the eye can distinguish.  In applications where images are processed\nby computing with color components, it is common to use additional bits per color component,\nto avoid visual effects that might occur due to rounding errors in the computations.\nSuch applications might use a 16-bit integer or even a 32-bit floating point value for\neach color component.  On the other hand, sometimes fewer bits are used.  For example, one\ncommon color scheme uses 5 bits for the red and blue components and 6 bits for the green\ncomponent, for a total of 16 bits for a color.  (Green gets an addition bit because\nthe eye is more sensitive to green light than to red or blue.)  This \"16-bit color\" saves memory\ncompared to 24-bit color and was more common when memory was more expensive.", "There are many other color models besides RGB.  RGB is sometimes criticized as being unintuitive.\nFor example, it's not obvious to most people that yellow is made of a combination of red and green.\nThe closely related color models ", " \nand ", " describe the same set of colors as RGB, but attempt\nto do it in a more intuitive way.  (HSV is sometimes called HSB, with the \"B\"\nstanding for \"brightness.\"  HSV and HSB are exactly the same model.)", "The \"H\" in these models stands for \"hue,\" a basic spectral color.\nAs H increases, the color changes from red to yellow to green to cyan to blue to magenta, and then\nback to red.  The value of H is often taken to range from 0 to 360, since the colors can be thought\nof as arranged around a circle with red at both 0 and 360 degrees.", "The \"S\" in HSV and HSL stands for \"saturation,\"\nand is taken to range from 0 to 1.  A saturation of 0 gives a shade of gray (the shade depending on\nthe value of V or L). A saturation of 1 gives a \"pure color,\" and decreasing the saturation is\nlike adding more gray to the color.  \"V\"\u00a0stands for \"value,\" and \"L\" stands for \"lightness.\"\nThey determine how bright or dark the color is.  The main difference is that in the HSV model, the\npure spectral colors occur for V=1, while in HSL, they occur for L=0.5.", "Let's look at some colors in the HSV color model.   The illustration below shows\ncolors with a full range of H-values, for S and V equal to 1 and to 0.5.  Note that for S=V=1, you\nget bright, pure colors.  S=0.5 gives paler, less saturated colors.  V=0.5 gives darker colors.", "\n", "It's probably easier to understand color models by looking at some actual colors\nand how they are represented. Here is an interactive demo that \nlet's you do that for the RGB and HSV color models:\n", "\n", "\n", "Often, a fourth component is added to color models.  The fourth component is called\n", ", and color models that use it are\nreferred to by names such as RGBA and HSLA.  Alpha is not a color as such.  It is usually used\nto represent transparency.  A color with maximal alpha value is fully opaque; that is, it is\nnot at all transparent.  A color with alpha equal to zero is completely transparent and therefore\ninvisible.  Intermediate values give translucent, or partly transparent, colors.\nTransparency determines what happens when you draw with one color (the foreground color) \non top of another color (the background color).  If the foreground color is fully opaque, it \nsimply replaces the background color.  If the foreground color is partly transparent, then\nthen it is blended with the background color.  Assuming that the alpha component ranges from\n0\u00a0to\u00a01, the color that you get can be computed as", "This computation is done separately for the red, blue, and green color components.\nThis is called ", ".\nThe effect is like viewing the background through colored glass; the color of the glass\nadds a tint to the background color.  This type of blending is not the only possible use\nof the alpha component, but it is the most common.", "An RGBA color model with 8 bits per component uses a total of 32 bits to represent a color.\nThis is a convenient number because integer values are often represented using 32-bit values.\nA 32-bit integer value can be interpreted as a 32-bit RGBA color.  How the color components are\narranged within a 32-bit integer is somewhat arbitrary.  The most common layout is to store\nthe alpha component in the eight high-order bits, followed by red, green, and blue.  (This should\nprobably be called ARGB color.)  However, other layouts are also in use."], "chapter_title": "Two-Dimensional Graphics", "id": 2.1}, {"section_title": "HTML Canvas Graphics", "chapter_id": "Chapter 2", "section_id": "Section 2.6", "content": ["Most modern web browsers support a 2D graphics ", " that can be used\nto create images on a web page.  The API is implemented using ", ", the client-side programming \nlanguage for the web.  I won't cover the JavaScript language in this section.  To understand the \nmaterial presented here, you don't need to know much about it.  Even if you\nknow nothing about it at all, you can learn something about its 2D graphics API and\nsee how it is similar to, and how it differs from, the Java API presented in the\n", ".  (For a short review\nof JavaScript, see ", " in ", ".)", "The visible content of a web page is made up of \"elements\" such\nas headlines and paragraphs.  The content is specified using the ", " language.\nA \"canvas\" is an HTML element. It appears on the page as a blank rectangular area which can\nbe used as a drawing surface by what I am calling the \"", "\" graphics API.\nIn the source code of a web page, a canvas element is created with code of the form", "The ", " and ", " give the size of the drawing area, in pixels.  The\n", " is an identifier that can be used to refer to the canvas in JavaScript.", "To draw on a canvas, you need a graphics context.  A graphics context is an object that\ncontains functions for drawing shapes.  It also contains variables that record the current graphics \nstate, including  things like the current drawing color, transform, and font.  Here, I will \ngenerally use ", " as the name of the variable that refers to the graphics context, \nbut the variable name is, of course, up to the programmer.  This graphics context plays the same role in \nthe canvas API that a variable of type ", " plays in Java.\nA typical starting point is\n", "The first line gets a reference to the canvas element on the web page, using its ", ".\nThe second line creates the graphics context for that canvas element.  (This code will\nproduce an error in a web browser that doesn't support canvas, so you might add some error\nchecking such as putting these commands inside a ", " statement.)", "Typically, you will store the canvas graphics context in a global variable\nand use the same graphics context throughout your program.  This is in contrast\nto Java, where you typically get a new ", " context\neach time the ", "() method is called, and that new context\nis in its initial state with default color and stroke\nproperties and with no applied transform.  When a graphics context \ncontext is global, changes made to the state in one function\ncall will carry over to subsequent function calls, unless you do something to limit\ntheir effect.  This can actually lead to a fairly common type of bug: For example, \nif you apply a 30-degree rotation in a function, those rotations will ", "\neach time the function is called, unless you do something to undo the previous rotation\nbefore applying the next rotation.", "The rest of this section will be mostly concerned with describing what you can do with\na canvas graphics context.  But here, for the record, is the complete source code for\na very minimal web page that uses canvas graphics:\n", "For a more complete, though still minimal, example, look at the sample page\n", ".  (You should look at the page\nin a browser, but you should also read the source code.)  This example shows\nhow to draw some basic shapes using canvas graphics, and you can use it as\na basis for your own experimentation.  There are also three more advanced\n\"starter\" examples:  ", " adds\nsome utility functions for drawing shapes and setting up a coordinate system;\n", " adds animation and includes\na simple ", " example; and\n", " shows how to respond to keyboard\nand mouse events.", "The default coordinate system on a canvas is the usual: The unit of measure is one pixel;\n(0,0) is at the upper left corner; the ", "-coordinate increases to the right;\nand the ", "-coordinate increases downward.   The range of ", " and ", "\nvalues are given by the ", " and ", " properties of the ", "\nelement.  The term \"pixel\" here for the unit of measure is not really correct.  \nProbably, I should say something like \"one nominal pixel.\"\nThe unit of measure is one pixel at typical desktop resolution with no magnification.\nIf you apply a magnification to a browser window, the unit of measure gets stretched.\nAnd on a high-resolution screen, one unit in the default coordinate system might \ncorrespond to several actual pixels on the display device.", "The canvas API supports only a very limited set of basic shapes. In fact, the\nonly basic shapes are rectangles and text.  Other shapes must be created as paths.\nShapes can be ", " and \n", ".  That includes text: When you stroke a string of\ntext, a pen is dragged along the outlines of the characters; when you fill a string,\nthe insides of the characters are filled.  It only really makes sense to stroke text\nwhen the characters are rather large.  Here are the functions for drawing\nrectangles and text, where ", " refers to the object that represents\nthe graphics context:", "A path can be created using functions in the graphics context.  The context keeps track of\na \"current path.\"  In the current version of the API, paths are not represented by objects,\nand there is no way to work with more than one path at a time or to keep a copy of a path\nfor later reuse.  Paths can contain lines, ", ", and circular arcs.\nHere are the most common functions for working with paths:", "Creating a curve with these commands does not draw anything.  To get something visible\nto appear in the image, you must fill or stroke the path.", "The commands ", "() and ", "() are used to fill\nand to stroke the current path.  If you fill a path that has not been closed, the fill\nalgorithm acts as though a final line segment had been added to close the path.\nWhen you stroke a shape, it's the center of the virtual pen that moves along the path.\nSo, for high-precision canvas drawing, it's common to\nuse paths that pass through the centers of pixels rather than through their corners.\nFor example, to draw a line that extends from the pixel with coordinates (100,200) to\nthe pixel with coordinates (300,200), you would actually stroke the geometric line\nwith endpoints (100.5,200.5) and (100.5,300.5).  We should look at some examples.\nIt takes four steps to draw a line:", "Remember that the line remains as part of the current path until the\nnext time you call ", "().  Here's how to draw a filled,\nregular octagon centered at (200,400) and with radius 100:", "The function ", "() can be used to draw a circle, with a start \nangle of 0 and an end angle of ", ".  Here's a filled circle with radius \n100, centered at 200,300:", "To draw just the outline of the circle, use ", "()\nin place of ", "().  You can apply both operations to the same path.\nIf you look at the details of ", "(),\nyou can see how to draw a wedge of a circle:", "There is no way to draw an oval that is not a circle, except by using \ntransforms.  We will cover that later in this section.  But JavaScript has\nthe interesting property that it is possible to add new functions and \nproperties to an existing object.  The sample program\n", " shows how to\nadd functions to a graphics context for drawing lines, ovals, and\nother shapes that are not built into the API.", "Attributes such as line width that affect the visual appearance of\nstrokes and fills are stored as properties of the graphics context.\nFor example, the value of ", " is a number that\nrepresents the width that will be used for strokes.  (The width is \ngiven in pixels for the default coordinate system, but it is subject \nto transforms.)  You can change the line width by assigning a value\nto this property:", "The change affects subsequent strokes.  You can also read the current\nvalue:", "The property ", " controls the appearance of\nthe endpoints of a stroke.  It can be set to\n\"round\", \"square\", or \"butt\".  The quotation marks are part of the value.  For example,", "Similarly, ", " controls\nthe appearance of the point where one segment of a stroke joins another\nsegment; its possible values are \"round\", \"bevel\", or \"miter\".  (Line\nendpoints and joins were discussed in ", ".)", "Note that the values for ", " and ", "\nare strings.  This is a somewhat unusual aspect of the API.  Several other properties\nof the graphics context take values that are strings, including the properties that\ncontrol the colors used for drawing and the font that is used for drawing\ntext.", "Color is controlled by the values of the properties ", "\nand ", ".  The graphics context maintains separate styles\nfor filling and for stroking.  A\u00a0solid color for stroking or filling is specified\nas a string.  Valid color strings are ones that can be used in ", ", the language\nthat is used to specify colors and other style properties of elements on web pages.\nMany solid colors can be specified by their names, such as \"red\", \"black\", and\n\"beige\".  An ", " can be specified as a string of the\nform \"rgb(r,g,b)\", where the parentheses contain three numbers in the range\n0 to 255 giving the red, green, and blue components of the color.  Hexadecimal color\ncodes are also supported, in the form \"#XXYYZZ\" where XX, YY, and ZZ are two-digit\nhexadecimal numbers giving the RGB color components.  For example,", "The style can actually be more complicated\nthan a simple solid color:  ", " and \n", " are also supported.  As an \nexample, a gradient can be created with a series of steps such as", "The first line creates a linear gradient that will vary in color along\nthe line segment from the point (420,420) to the point (550,200).\nColors for the gradient are specified by the ", " function:\nthe first parameter gives the fraction of the distance from the initial\npoint to the final point where that color is applied, and the second is a string\nthat specifies the color itself.  A color stop at 0 specifies\nthe color at the initial point; a color stop at 1 specifies the color\nat the final point.  Once a gradient has been created, it can be used both\nas a fill style and as a stroke style in the graphics context.", "Finally, I note that the font that is used for drawing text is the\nvalue of the property ", ".  The value is a string\nthat could be used to specify a font in ", ".\nAs such, it can be fairly complicated, but the simplest versions\ninclude a font-size (such as ", " or ", ") and a font-family\n(such as ", ", ", ", ", ", or the name of any font\nthat is accessible to the web page).\nYou can add ", " or ", " or both to the front of the string.\nSome examples:\n", "The default is \"10px sans-serif,\" which is usually too small.  Note that text, like all drawing, is\nsubject to coordinate transforms.  Applying a scaling operation changes the\nsize of the text, and a negative scaling factor can produce mirror-image text.", "A graphics context has three basic functions for modifying the current transform\nby scaling, rotation, and translation.  There are also functions that will compose\nthe current transform with an arbitrary transform and for completely\nreplacing the current transform:", "Note that there is no ", ", but you can apply a shear as\na general transform.  For example, for a horizontal shear with shear factor\u00a00.5, use", "To implement hierarchical modeling, as discussed in ", ", \nyou need to be able to save\nthe current transformation so that you can restore it later.  Unfortunately,\nno way is provided to read the current transformation from a canvas graphics\ncontext.  However, the graphics context itself keeps a stack of transformations\nand provides methods for pushing and popping the current transformation.  In fact,\nthese methods do more than save and restore the current transformation.  They actually\nsave and restore almost the entire state of the graphics context, including properties\nsuch as current colors, line width, and font (but not the current path):", "Using these methods, the basic setup for drawing an object with a modeling\ntransform becomes:", "Note that if drawing the object includes any changes to attributes\nsuch as drawing color, those changes will be also undone by the call to ", "().\nIn hierarchical graphics, this is usually what you want, and it eliminates the\nneed to have extra statements for saving and restoring things like color.", "To draw a hierarchical model, you need to traverse a ", ", either\nprocedurally or as a data structure.  It's pretty much the same as in Java.\nIn fact, you should see that the basic concepts that you learned about\ntransformations and modeling carry over to the canvas graphics API.  Those\nconcepts apply very widely and even carry over to 3D graphics APIs, with\njust a little added complexity.  The demo program\n", " from ", "\nimplements hierarchical modeling using the 2D canvas API.", "Now that we know how to do transformations, we can see how to draw an oval\nusing the canvas API.  Suppose that we want an oval with center at (", "),\nwith horizontal radius ", " and with vertical radius ", ".\nThe idea is to draw a circle of radius 1 with center at (0,0), then transform\nit.  The circle needs to be scaled by a factor of ", " horizontally and\n", " vertically.  It should then be translated to move its center from\n(0,0) to (", ").  We can use ", "() and ", "()\nto make sure that the transformations only affect the circle.  Recalling that\nthe order of transforms in the code is the opposite of the order in which they\nare applied to objects, this becomes:", "Note that the current path is ", " affected by the\ncalls to ", "() and ", "().  So,\nin the example, the oval-shaped path is not discarded when\n", "() is called. When ", "() is\ncalled at the end, it is the oval-shaped path that is stroked.  On the\nother hand, the line width that is used for the stroke is not affected\nby the scale transform that was applied to the oval.  Note that if\nthe order of the last two commands were reversed, then the line width\nwould be subject to the scaling.", "There is an interesting point here about transforms and paths.  In the HTML canvas\nAPI, the points that are used to create a path are transformed by the\ncurrent transformation before they are saved.  That is, they are saved\nin pixel coordinates.  Later, when the path is stroked or filled, the current\ntransform has no effect on the path (although it can affect,\nfor example, the line width when the path is stroked).  In particular,\nyou can't make a path and then apply different transformations.  For example,\nyou can't make an oval-shaped path, and then use it to draw several ovals\nin different positions.  Every time you draw the oval, it will be in the\nsame place, even if different translation transforms are applied to\nthe graphics context.", "The situation is different in Java, where the coordinates that are stored\nin the path are the actual numbers that are used to specify the path,\nthat is, the ", ".  When the path is stroked or\nfilled, the transformation that is in effect at that time is applied\nto the path.  The path can be reused many times\nto draw copies with different transformations.  This comment is offered as an\nexample of how APIs that look very similar can have subtle differences.", "In ", ", we looked at the sample program\n", ", which uses a\n", " both to implement an\n", " and to allow direct manipulation of\nthe colors of individual pixels.  The same ideas can be applied\nin HTML canvas graphics, although the way it's done is a little different.\nThe sample web application ", "\ndoes pretty much the same thing as the Java program (except for the\nimage filters).", "Here\nis a live demo \nversion of the program that has the same functionality.\nYou can try it out to see how the various drawing tools work.  Don't\nforget to try the \"Smudge\" tool! (It has to be applied to shapes that\nyou have already drawn.)", "\n", "\n", "For JavaScript, a web page is represented as a data structure, defined\nby a standard called the ", ", or Document Object model.\nFor an off-screen canvas, we can use a ", " that is not part of\nthat data structure and therefore is not part of the page.\nIn JavaScript, a ", "\ncan be created with the function call ", "(\"canvas\").\nThere is a way to add this kind of dynamically created canvas to the\nDOM for the web page, but it can be used as an off-screen canvas without doing so.\nTo use it, you have to set its width and height properties, and you\nneed a graphics context for drawing on it.  Here, for example, is\nsome code that creates a 640-by-480 canvas, gets a graphics\ncontext for the canvas, and fills the whole canvas with white:", "The sample program lets the user drag the mouse on the canvas to\ndraw some shapes. The off-screen canvas holds the official copy of\nthe picture, but it is not seen by the user.  There is also an\non-screen canvas that the user sees.  The off-screen canvas is copied \nto the on-screen canvas whenever the picture is modified.  \nWhile the user is dragging the mouse to\ndraw a line, oval, or rectangle, the new shape is actually\ndrawn on-screen, over the contents of the off-screen canvas. It is\nonly added to the off-screen canvas when the user finishes the\ndrag operation. For the other tools, changes are made directly\nto the off-screen canvas, and the result is then copied to the\nscreen.  This is an exact imitation of the Java program.", "(The demo version shown above \nactually uses a somewhat different technique to accomplish the\nsame thing.  It uses two on-screen canvases, one located exactly\non top of the other.  The lower canvas holds the actual image.\nThe upper canvas is completely transparent, except when the\nuser is drawing a line, oval, or rectangle.  While the user\nis dragging the mouse to draw such a shape, the new shape\nis drawn on the upper canvas, where it hides the part of\nthe lower canvas that is beneath the shape.  When the user\nreleases the mouse, the shape is added to the lower canvas\nand the upper canvas is cleared to make it completely transparent\nagain.  Again, the other tools operate directly on the lower\ncanvas.)", "The \"Smudge\" tool in the ", " \nand demo is implemented by computing with the color component values of pixels in the image.\nThe implementation requires some way to read the colors of pixels in a canvas.  That can be \ndone with the function ", "(", "), where ", " is a 2D graphics\ncontext for the canvas.  The function reads the colors of a rectangle of pixels, where (", ") is\nthe upper left corner of the rectangle, ", " is its width, and ", " is its height.  The\nparameters are always expressed in pixel coordinates.  Consider, for example", "This returns the color data for a 20-by-10 rectangle in the upper left corner of the canvas.\nThe return value, ", ", is an object with properties ", ",\n", ", and ", ".  The ", " and ", " give the number of\nrows and columns of pixels in the returned data.  (According to the documentation, on a high-resolution \nscreen, they might not be the same as the width and height in the function call.  The data can be \nfor real, physical pixels on the display device, not the \"nominal\" pixels that are used in the pixel \ncoordinate system on the canvas. There might be several device pixels for each nominal pixel.\nI'm not sure whether this can really happen.)", "The value of ", " is an array, with four array elements for each pixel.  The four\nelements contain the red, blue, green, and alpha color components of the pixel, given as integers\nin the range 0 to 255.  For a pixel that lies outside the canvas, the four component values will\nall be zero.  The array is a value of type ", " whose elements are\n8-bit unsigned integers limited to the range 0 to 255.  This is one of JavaScript's\n", " datatypes, which can only hold values of a specific numerical type.\nAs an example, suppose that you just want to read the RGB color of one pixel, at coordinates (", ").\nYou can set", "Then the RGB color components for the pixel are R = ", "[0],\nG = ", "[1], and B = ", "[2].", "The function ", "(", ",", ",", ") is\nused to copy the colors from an image data object into a canvas, placing it into\na rectangle in the canvas with upper left corner at (", ").  The ", "\nobject can be one that was returned by a call to ", ", possibly\nwith its color data modified.  Or you can create a blank image data object by\ncalling ", "(", ") and fill it with data.", "Let's consider the \"Smudge\" tool in the sample program. When the user clicks the\nmouse with this tool, I use ", " to get the color data from a \n9-by-9 square of pixels surrounding the mouse location.  ", " is the graphics\ncontext for the canvas that contains the image.  Since I want to do real-number\narithmetic with color values, I copy the color components into another typed array,\none of type ", ", which can hold 32-bit floating point numbers.\nHere is the function that I call to do this:", "The floating point array, ", ", will be used for computing new\ncolor values for the image as the mouse moves.  The color values\nfrom this array will be copied into the image data object, ", ", \nwhich will them be used to put the color values into the image.  This is done in\nanother function, which is called for each point that is visited as the user\ndrags the Smudge tool over the canvas:", "In this function, a new color is computed for each pixel in a 9-by-9 square of\npixels around the mouse location.  The color is replaced by a weighted average\nof the current color of the pixel and the color of the corresponding pixel in the\n", ".  At the same time, the color in ", "\nis replaced by a similar weighted average.", "It would be worthwhile to try to understand this example to see how pixel-by-pixel\nprocessing of color data can be done.  See the \n", "\nof the example for more details.", "For another example of pixel manipulation, we can look at \nimage filters that modify an image by replacing the color of each pixel with\na weighted average of the color of that pixel and the 8 pixels that\nsurround it. Depending on the weighting factors that are used, the\nresult can be as simple as a slightly blurred version of the image, or\nit can something more interesting.", "Here is \nan an\ninteractive demo that lets you apply several different image filters to\na variety of images:", "\n", "\n", "The filtering operation in the demo uses the image data functions\n", ", ", ", and ", "\nthat were discussed above.  Color data from the entire image is obtained\nwith a call to ", ".  The results of the averaging \ncomputation are placed in a new image data object, and the resulting\nimage data is copied back to the image using ", ".", "The remaining question is, where do the original images come\nfrom, and how do they get onto the canvas in the first place?\nAn image on a web page is specified by an element in the web page\nsource such as", "The ", " attribute specifies the URL from which the image is\nloaded.  The optional ", " can be used to reference the image\nin JavaScript.  In the script,", "gets a reference to the object that represents the image in the\ndocument structure.  Once you have such an object, you can use it to\ndraw the image on a canvas.  If ", " is a graphics context\nfor the canvas, then", "draws the image with its upper left corner at (", ").  Both\nthe point (", ") and the image itself are transformed by any\ntransformation in effect in the graphics context.  This will draw\nthe image using its natural width and height (scaled by the transformation,\nif any).  You can also specify the width and height of the rectangle\nin which the image is drawn:", "With this version of ", ", the image is scaled to fit\nthe specified rectangle.", "Now, suppose that the image you want to draw onto the canvas is not\npart of the web page?  In that case, it is possible to load the image dynamically.\nThis is much like making an off-screen canvas, but you are making\nan \"off-screen image.\"  Use the ", " object to create an\n", " element:", "An ", " element needs a ", " attribute that\nspecifies the URL from which it is to be loaded.  For example,", "As soon as you assign a value to the ", " attribute, the browser\nstarts loading the image.  The loading is done asynchronously; that is,\nthe computer continues to execute the script without waiting for the\nload to complete.  This means that you can't simply draw the image on\nthe line after the above assignment statement:  The image is very likely\nnot done loading at that time.  You want to draw the image after it has\nfinished loading.  For that to happen, you need to assign a function to the image's ", "\nproperty before setting the ", ". That function will\nbe called when the image has been fully loaded.  Putting this together, here is a simple \nJavaScript function for loading an image from a specified URL and drawing it on a canvas\nafter it has loaded:", "A similar technique is used to load the images in the filter demo.", "There is one last mystery to clear up.  When discussing the use of an off-screen\ncanvas in the ", " example earlier in this section, I noted that\nthe contents of the off-screen canvas have to be copied to the main canvas,\nbut I didn't say how that can be done.  In fact, it is done using ", ".\nIn addition to drawing an image onto a canvas, ", " can be used to draw the contents of\none canvas into another canvas.  In the sample program, the command", "is used to draw the off-screen canvas to the main canvas.  Here, ", "\nis a graphics context for drawing on the main canvas, and ", " is the object\nthat represents the off-screen canvas."], "chapter_title": "Two-Dimensional Graphics", "id": 2.6}, {"section_title": "Java Graphics2D", "chapter_id": "Chapter 2", "section_id": "Section 2.5", "content": ["In the rest of this chapter, we look at specific implementations\nof two-dimensional graphics.  There are a few new ideas here,\nbut mostly you will see how the general concepts that we have covered are used in several\nreal graphics systems.", "In this section, our focus is on the Java programming language.\nJava remains one of the most popular programming languages.  Its\nstandard desktop version includes a sophisticated 2D graphics API,\nwhich is our topic here.  Before reading this section, you should\nalready know the basics of Java programming. But even if you don't,\nyou should be able to follow most of the discussion of the\ngraphics API itself.  (See ", " in ", " for a very basic\nintroduction to Java.)", "The original version of Java had a much smaller graphics API.  It was\ntightly focused on pixels, and it used only integer coordinates.  The API\nhad subroutines for stroking and filling a variety of basic shapes,\nincluding lines, rectangles, ovals, and polygons (although Java uses the\nterm ", " instead of ", ").  Its specification\nof the meaning of drawing operations was very precise on the pixel\nlevel.  Integer coordinates are defined to refer to the lines\nbetween pixels.  For example, a 12-by-8 pixel grid has\n", "-coordinates from 0 to 12 and ", "-coordinates from\n0 to 8, as shown below.  The lines between pixels are numbered,\nnot the pixels.", "\n", "The command ", "(3,2,5,3)\nfills the rectangle with upper left corner at (3,2), with\nwidth 5, and with height 3, as shown on the left above.  The command\n", "(3,2,5,3) conceptually drags a \"pen\" around the outline\nof this rectangle.  However, the pen is a 1-pixel square, and it is the\nupper left corner of the pen that moves along the outline.  As the pen\nmoves along the right edge of the rectangle, the pixels to the ", "\nof that edge are colored; as the pen moves along the bottom edge, the\npixels below the edge are colored.  The result is as shown on the right above.\nMy point here is not to belabor the details, but to point out that having\na precise specification of the meaning of graphical operations gives you very\nfine control over what happens on the pixel level.", "Java's original graphics did not support things like real-number coordinates, \n", ", ", ", or \n", ".  Just a few years after Java was first introduced,\na new graphics API was added that does support all of these.  It is that\nmore advanced API that we will look at here.", "Java is an object-oriented language.  Its API is defined as a large set\nof classes,   The actual drawing operations in the original graphics API\nwere mostly contained in the class named ", ".  \nIn the newer API, drawing operations are methods in a class named ", ",\nwhich is a subclass of ", ", so that all the original drawing operations\nare still available.  (A class in Java is contained in a collection of classes known as a \"package.\"\n", " and ", ", for example, are in the\npackage named ", ".  Classes that define shapes and transforms are in a package\nnamed ", ".  However, in the rest of this section, I will talk about classes\nwithout mentioning their packages.)", "A graphics system needs a place to draw.  In Java, the drawing surface is often an object\nof the class ", ", which represents a rectangular area on the\nscreen.  The ", " class has a method named ", "()\nto draw its content.  To create a drawing surface, you can create a subclass of\n", " and provide a definition for its ", "()\nmethod.  All drawing should be done inside ", "(); when it is necessary\nto change the contents of the drawing, you can call the panel's ", "() method\nto trigger a call to ", "().  The ", "() method has\na parameter of type ", ", but the parameter that is passed\nto the method is actually an object of type ", ", and\nit can be type-cast to ", " to obtain access to the\nmore advanced graphics capabilities.  So, the definition of the ", "()\nmethod usually looks something like this:", "In the rest of this section, I will assume that ", " is a variable of\ntype ", ", and I will discuss some of the things\nthat you can do with it.  As a first example, I note that ", "\nsupports ", ", but it is not turned on by default.  It\ncan be enabled in a graphics context ", " with the rather intimidating\ncommand", "For simple examples of graphics in complete Java programs,\nyou can look at the sample programs ", "\nand ", ".  They provide very minimal\nframeworks for drawing static and animated images, respectively, using ", ".\nThe program ", " is a similar framework for\nworking with mouse and key events in a graphics program.\nYou can use these programs as the basis for some experimentation if you want to explore\nJava graphics.", "Drawing with the original ", " class is done using integer coordinates, \nwith the measurement given in pixels. This works well in the standard coordinate system, but is\nnot appropriate when real-number coordinates are used, since the unit of measure in such a \ncoordinate system will not be equal to a pixel.  We need to be able to specify shapes using\nreal numbers.  The Java package ", " provides support for shapes defined using real number \ncoordinates.  For example, the class ", " in that package represents line segments\nwhose endpoints are given as pairs of real numbers.", "Now, Java has two real number types: ", " and ", ".\nThe ", " type can represent a larger range of numbers than ", ", with a greater\nnumber of significant digits, and ", " is the\nmore commonly used type.  In fact, ", " are simply easier to use in Java.\nHowever, ", "  values generally have enough accuracy\nfor graphics applications, and they have the advantage of taking up less space in memory.\nFurthermore, computer graphics hardware often uses float values internally.", "So, given\nthese considerations, the ", " package actually provides\ntwo versions of each shape, one using coordinates of type ", " and\none using coordinates of type ", ".  This is done in a rather strange way.\nTaking ", " as an example, the class ", "\nitself is an ", ".  It has two subclasses, one that represents lines using\n", " coordinates and one using ", " coordinates.\nThe strangest part is that these subclasses are defined as nested classes\ninside ", ": ", " and\n", ".  This means that you can declare a variable of\ntype ", ", but to create an object, you need to use\n", " or ", ":\n", "Note that when using constants of type ", " in Java,\nyou have to add \"F\" as a suffix to the value.  this is one reason why ", "\nare easier in Java.  For simplicity, you might want to stick to using\n", ".  However, ", " might give\nslightly better performance.", "Let's take a look at some of the other classes from ", ".\nThe abstract class ", "\u2014with its concrete subclasses \n", " and ", "\u2014represents\na point in two dimensions, specified by two real number coordinates.  A point is not a\nshape; you can't fill or stroke it.  A point can be\nconstructed from two real numbers (\"", "\").  If ", "\nis a variable of type ", ", you can use ", "() and\n", "() to retrieve its coordinates, and you can use ", "(", "),\n", "(", "), or ", "(", ",", ") to set its coordinates.\nIf ", " is a variable of type ", ", you can also refer\ndirectly to the coordinates as ", " and ", "\n(and similarly for ", ").\nOther classes in ", " offer a similar variety of ways\nto manipulate their properties, and I won't try to list them all here.\n", " The package ", "\ncontains a variety of classes that represent geometric shapes, including  ", ",\n", ", ", ",\n", ", ", ",\nand ", ".\nAll of these are abstract classes, and each of them contains a pair of subclasses such as\n", " and ", ".\nSome shapes, such as rectangles, have\ninteriors that can be filled; such shapes also have\noutlines that can be stroked.  Some shapes, such as lines, are purely one-dimensional\nand can only be stroked.", "Aside from lines, rectangles are probably the simplest shapes.  A ", "\nhas a corner point (", ",", "), a ", ", and a ", ", and\ncan be constructed from that data (\"", "\").  The corner point\n(", ",", ") specifies the minimum ", "- and ", "-values in the rectangle.\nFor the usual pixel coordinate\nsystem, (", ",", ") is the upper left corner.  However, in a coordinate system in which the\nminimum value of ", " is at the bottom, (", ",", ") would be the lower left corner.\nThe sides of the rectangle are parallel to the coordinate axes.  A variable\n", " of type ", "\nhas public instance variables ", ", ", ", ", ", and ", ".\nIf the width or the height is less than or equal to zero, nothing will be drawn when the rectangle\nis filled or stroked.   A common task is to define a rectangle from two corner points (", ",", ")\nand (", ",", ").  This can be accomplished by creating a rectangle with height and width\nequal to zero and then ", " the second point to the rectangle.\nAdding a point to a rectangle causes the rectangle to grow just enough to include that point:\n", "The classes ", ",\n", ", ", "\nand ", " create other basic shapes and work similarly\nto ", ".  You can check the Java API documentation for details.", "The ", " class is more interesting. It represents general\npaths made up of segments that can be lines and ", ".\nPaths are created using methods similar to the ", " and ", "\nsubroutines that were discussed in ", ".  To create a\npath, you start by constructing an object of type ", "\n(or ", "):", "The path ", " is empty when it is first created.\nYou construct the path by moving an imaginary \"pen\" along the path that you want to create.\nThe method ", "(", ",", ") moves the pen to the point (", ",", ") without\ndrawing anything.  It is used to specify the initial point of the path or\nthe starting point of a new piece of the path.\nThe method ", "(", ",", ") draws a line\nfrom the current pen position to (", ",", "), leaving the pen at (", ",", ").\nThe method ", "() can be used to close the path (or the current piece of the path) \nby drawing a line back to its starting point.\nFor example, the following code creates a triangle with vertices at (0,5), (2,-3), and (-4,1):\n", "You can also add Bezier curve segments to a ", ".  \nBezier curves were discussed in ", ".   You can\nadd a cubic Bezier curve to a ", " ", " with the method\n", "This adds a curve segment that starts at the current pen position and ends at\n(", ",", "), using (", ",", ") and (", ",", ") as\nthe two ", " for the curve.  The\nmethod for adding a quadratic Bezier curve segment to a path is ", ".\nIt requires only a single control point:", "When a path intersects itself, its interior is determined by looking at\nthe ", ", as discussed in ", ".\nThere are two possible rules for determining whether a point is interior:\nasking whether the winding number of the curve about that point is non-zero, \nor asking whether it is even.  You can set the winding rule used by a ", "\n", " with", "The default is ", ".", "Finally, I will note that it is possible to draw a copy of an image into a graphics context.\nThe image could be loaded from a file or created by the program.  I discuss the second possibility\nlater in this section.  An image is represented by an object of type ", ".\nIn fact, I will assume here that the object is of type ", ",\nwhich is a subclass of ", ".  If ", " is such an object, then", "will draw the image with its upper left corner at the point (", ",", ").  (The fourth\nparameter is hard to explain, but it should be specified as ", " for ", ".)  \nThis draws the image\nat its natural width and height, but a different width and height can be specified in the method:", "There is also a method for drawing a string of text.  The method specifies the\nstring and the basepoint of the string.  (The basepoint is the lower left corner of\nthe string, ignoring \"descenders\" like the tail on the letter \"g\".)  For example,", "Images and strings are subject to transforms in the same way as other shapes. Transforms\nare the only way to get rotated text and images.  As an example, here is what can happen\nwhen you apply a rotation to some text and an image:", "\n", "Once you have an object that represents a shape, you can fill the shape or stroke it.\nThe ", " class defines methods for doing this.\nThe method for stroking a shape is called ", ":", "Here, ", " is of type ", ", and\nshape can be of type ", ", ", ",\n", " or any of the other shape classes.  These are\noften used on a newly created object, when that object represents a shape that\nwill only be drawn once.  For example", "Of course, it is also possible to create shape objects and reuse them many\ntimes.", "The \"pen\" that is used for stroking a shape is usually represented by an\nobject of type ", ".  The default stroke has line\nwidth equal to\u00a01.  That's one unit in the current coordinate system, not\none pixel.  To get a line with a different width, you can install a new stroke\nwith", "The ", " in the constructor is of type ", ".  It is possible to\nadd parameters to the constructor to control the shape of a stroke at its endpoints and\nwhere two segments meet.  (See ", ".)  For example,", "It is also possible to make strokes out of dashes and dots, but I won't discuss how\nto do it here.", "Stroking or filling a shape means setting the colors of certain pixels.  In Java, the rule\nthat is used for coloring those pixels is called a \"paint.\"  Paints can be solid colors,\n", ", or ", ".\nLike most things in Java, paints are represented by objects.  If ", " is\nsuch an object, then", "will set ", " to be used in the graphics context ", " for subsequent\ndrawing operations, until the next time the paint is changed.  (There is also an\nolder method, ", "(", "), that works only for colors and is\nequivalent to calling ", "(", ").)", "Solid colors are represented by objects of type ", ".  A color\nis represented internally as an ", ". An opaque color, with maximal\nalpha component, can be created using the constructor", "where ", ", ", ", and ", " are integers in the range 0 to 255 that give\nthe red, green, and blue components of the color.  To get a translucent color, you can\nadd an alpha component, also in the range 0 to 255:", "There is also a function, ", "(", "), that creates a color\nfrom values in the HSB color model (which is another name for ", ").\nIn this case, the hue, saturation, and brightness color components must be given as values of\ntype ", ".  And there are constants to represent about a dozen common\ncolors, such as ", ", ", ", and ", ".\nFor example, here is how I might draw a square with a black outline and a light\nblue interior:", "Beyond solid colors, Java has the class ", ", to represent\nsimple ", ", and ", "\nto represent to represent ", ".  (Image patterns\nused in a similar way in 3D graphics are called ", ".)\nGradients and patterns were discussed in ", ".\nFor these paints, the color that is applied to a pixel depends on the coordinates of the\npixel.", "To create a ", ", you need a ", "\nobject to specify the image that it will use as a pattern.  You also\nhave to say how coordinates in the image will map\nto drawing coordinates in the display.  You do this by specifying a rectangle in\nthat will hold one copy of the image.  So the constructor takes the form:", "where ", " is the ", " and ", " \nis a ", ".  Outside that specified rectangle, the image is\nrepeated horizontally and vertically. The constructor for a ", " \ntakes the form", "Here, ", ", ", ", ", ", and ", " are values of type ", ";\n", " and ", " are of type ", "; and\n", " is ", ".  The gradient color will vary along the line\nsegment from the point (", ",", ") to the point (", ",", ").\nThe color is ", " at the first endpoint and is ", " at the second\nendpoint.  Color is constant along lines perpendicular to that line segment.  The\nboolean parameter ", " says whether or not the color pattern repeats.\nAs an example, here is a command that will install a ", "\ninto a graphics context:", "You should, by the way, note that the current paint is used for strokes as well\nas for fills.", "The sample Java program ", " displays a polygon\nfilled with a ", " or a ", " \nand lets you adjust their properties.  The image files ", "\nand ", " are part of that program, and they must be\nin the same location as the compiled class files that make up that program when it is run.", "Java implements ", " as\nmethods in the ", " class. For example, if ", " is a\n", ", then calling ", "(1,3) will apply\na translation by (1,3) to objects that are drawn after the method is called. The methods\nthat are available correspond to the transform functions discussed in ", ":", "A transform in Java is represented as an object of the class ", ".\nYou can create a general ", " with the contstructor\n", "The transform ", " will transform a point (", ") to the point (", ") given by", "You can apply the transform ", " to a graphics context ", " by calling\n", "(", ").", "The graphics context ", " includes the current affine transform, which is the\ncomposition of all the transforms that have been applied.  Commands such as\n", " and ", " modify the current transform.  You can\nget a copy of the current transform by calling ", "(), which returns\nan ", " object.\nYou can set the current transform using ", "(", ").\nThis replaces the current transform in ", " with the ", "\n", ".  (Note that ", "(", ") is different from ", "(", ");\nthe first command ", " the current transform in ", ", while the second\n", " the current transform by composing it with ", ".)", "The ", " and ", " methods can be used to implement\n", ".  The idea, as discussed in ", ",\nis that before drawing an object, you should save the current transform.  \nAfter drawing the object, restore the saved transform.  Any additional modeling\ntransformations that are applied while drawing the object and its sub-objects will have\nno effect outside the object.  In Java, this looks like", "For hierarchical graphics, we really need a ", " of transforms.  However, if the hierarchy is implemented\nusing subroutines, then the above code would be part of a subroutine, and the value of the local\nvariable ", " would be stored on the subroutine call stack.  Effectively, we\nwould be using the subroutine call stack to implement the stack of saved transforms.", "In addition to modeling transformations, transforms are used to set up the\n", "-to-", " transformation that\nestablishes the ", " that will be used for drawing.\nThis is usually done in Java just after the graphics context has been created,\nbefore any drawing operations.  It can be done with a Java version of the\n", " \nfunction from ", ".  See the sample program\n", " for an example.", "I will mention one more use for ", " objects:  Sometimes,\nyou do need to explicitly transform coordinates.  For example, given ", "\n(", ",", "), I might need to know where they will actually end up on the screen, in \npixel coordinates.  That is, I would like to transform (", ",", ") by the current transform\nto get the corresponding pixel coordinates.  The ", " class\nhas a method for applying the affine transform to a point.  It works with objects of type\n", ".  Here is an example:", "One way I have used this is when working with strings.  Often when displaying a string in a\ntransformed coordinate system, I want to transform the basepoint of a string, but not\nthe string itself.  That is, I want the transformation to affect the location of the string\nbut not its size or rotation.  To accomplish this, I use the above technique to obtain\nthe pixel coordinates for the transformed basepoint, and then draw the string at\nthose coordinates, using an original, untransformed graphics context.", "The reverse operation is also sometimes necessary.  That is, given pixel coordinates\n(", ",", "), find the point (", ",", ") that is transformed to (", ",", ")\nby a given affine transform.  For example, when implementing mouse interaction, you will\ngenerally know the pixel coordinates of the mouse, but you will want to find the corresponding\npoint in your own chosen coordinate system.  For that, you need an ", ".\nThe inverse of an affine transform ", " is another transform that performs the opposite transformation.\nThat is, if ", "(", ",", ") = (", ",", "), \nand if ", " is the inverse transform, then ", "(", ",", ")\n= (", ",", "). In Java, the inverse transform of an ", "\n", " can be obtained with", "(A final note: The older drawing\nmethods from ", ", such as ", ", use integer coordinates.\nIt's important to note that any shapes drawn using these older methods are subject to the same transformation\nas shapes such as ", " that are specified with real\nnumber coordinates. For example, drawing a line with ", "(1,2,5,7)\nwill have the same effect as drawing a ", " that\nhas endpoints (1.0,2.0) and (5.0,7.0).  In fact, all drawing is affected by\nthe transformation of coordinates.)", "In some graphics applications, it is useful to be able to work with images that\nare not visible on the screen.  That is, you need what I call an ", ".\nYou also need a way to quickly copy the off-screen canvas onto the screen.\nFor example, it can be useful to store a copy of the on-screen image in an off-screen canvas.\nThe canvas is the official copy of the image.  Changes to the image are made to the canvas,\nthen copied to the screen.  One reason to do this is that you can then draw extra stuff on\ntop of the screen image without changing the official copy.  For example, you might draw\na box around a selected region in the on-screen image.  You can do this without damaging the\nofficial copy in the off-screen canvas.  To remove the box from the screen, you just have\nto copy the canvas image onto the screen.", "In Java, an off-screen image can be implemented as an object of type ", ".\nA ", " represents a region in memory where you can draw, in exactly the\nsame way that you can draw to the screen.  That is, you can obtain a graphics context\n", " of type ", " that you can use for drawing on the image.\nA ", " is an ", ", and you can draw\nit onto the screen\u2014or into any other graphics context\u2014like any other ", ",\nthat is, by using the ", " method of the graphics context where you want to display the\nimage.  In a typical setup, there are variables", "The objects are created using, for example,", "The constructor for ", " specifies the\nwidth and height of the image along with its type.  The type tells what\ncolors can be represented in the image and how they are stored.  Here,\nthe type is ", ", which means the image uses\nregular ", " with 8 bits for each\ncolor component.  The three color components for a pixel are packed\ninto a single integer value.", "In a program that uses a ", " to store a copy of\nthe on-screen image, the ", " method generally has the form", "A sample program that uses this technique is ", ".\nIn that program, the user can draw lines, rectangles, and ovals by dragging the mouse.\nAs the mouse moves, the shape is drawn between the starting point of the mouse and its\ncurrent location.  As the mouse moves, parts of the existing image can be repeatedly covered \nand uncovered, without changing the existing image.  In fact, the image is in an off-screen \ncanvas, and the shape that the user is drawing is actually drawn by ", "\nover the contents of the canvas.  The shape is not drawn to the official image in the canvas\nuntil the user releases the mouse and ends the drag operation.", "But my main reason for writing the program was to illustrate pixel manipulation, that is,\ncomputing with the color components of individual pixels.  The ", "\nclass has methods for reading and setting the color of individual pixels.  An image consists of\nrows and columns of pixels.  If ", " is a ", ", then", "gets the integer that represents the color of the pixel in column number ", " and row\nnumber ", ".  Each color component is stored in an 8-bit field in the integer ", "\nvalue.  The individual color components can be extracted for processing using Java's bit\nmanipulation operators:", "Similarly, given red, green, and blue color component values in the range 0 to 255,\nwe can combine those component values into a single integer and use it to set the\ncolor of a pixel in the image:", "There are also methods for reading and setting the colors of an entire rectangular\nregion of pixels.", "Pixel operations are used to implement two features of the sample program.  First, there is a\n\"Smudge\" tool.  When the user drags with this tool, it's like smearing wet paint.  When\nthe user first clicks the mouse, the color components from a small square of pixels surrounding\nthe mouse position are copied into arrays.  As the user moves the mouse, color from the\narrays is blended with the color of the pixels near the mouse position.  Here is a\nsmall rectangle that has been \"smudged\":", "\n", "The second use of pixel manipulation is in implementing \"filters.\"  A filter, in this\nprogram, is an operation that modifies an image by replacing the color of each \npixel with a weighted average of the colors of a 3-by-3 square of pixels.\nA \"Blur\" filter for example, uses equal weights for all pixels in the average, \nso the color of a pixel is changed to the simple average of the colors of that \npixel and its neighbors.  Using different weights for each pixel can produce some \nstriking effects.", "The pixel manipulation in the sample program produces effects that can't be achieved\nwith pure ", ".  I encourage you to learn more by looking at\nthe ", ".\nYou might also take a look at the live demos in the  ", ",\nwhich implement the same effects using ", " graphics."], "chapter_title": "Two-Dimensional Graphics", "id": 2.5}, {"section_title": "Some Linear Algebra", "chapter_id": "Chapter 3", "section_id": "Section 3.5", "content": ["Linear algebra is a branch of mathematics that is fundamental to\ncomputer graphics.  It studies ", ",\n", ",\nand ", ".  We have already encountered\nthese topics in ", " in a two-dimensional\ncontext.  In this section, we look at them more closely and extend\nthe discussion to three dimensions.", "It is not essential that you know the mathematical details that\nare covered in this section, since they can be handled internally in\nOpenGL or by software libraries.  However, you will need to be familiar\nwith the concepts and the terminology. This is especially true for\nmodern OpenGL, which leaves many of the details up to your programs.\nEven when you have a software library to handle the details, you still\nneed to know enough to use the library.  You might want to skim\nthis section and use it later for reference.", "A vector is a quantity that has a length and a direction.  A vector can be visualized\nas an arrow, as long as you remember that it is the length and direction of the\narrow that are relevant, and that its specific location is irrelevant.\nVectors are often used in computer graphics to represent directions, such as\nthe direction from an object to a light source or the direction in which a surface\nfaces.  In those cases, we are more interested in the direction of a vector than\nin its length.", "If we visualize a 3D vector ", " as an arrow starting at the origin, (0,0,0), and ending\nat a point ", ", then we can, to a certain extent, identify ", "\nwith ", "\u2014at least as long as we remember that an arrow starting\nat any other point could also be used to represent ", ".\nIf ", " has coordinates (", "), we can use the same coordinates\nfor ", ".  When we think of (", ") as a vector, ", " represents\nthe ", " in the ", "-coordinate between the starting point of the arrow and\nits ending point, ", " is the change in the ", "-coordinate, and ", " is\nthe change in the ", "-coordinate.  For\nexample, the 3D point (", ") = (3,4,5) has the\nsame coordinates as the vector (", ") = (3,4,5).\nFor the point, the coordinates (3,4,5) specify a position in space\nin the ", " coordinate system.  For the vector, the coordinates (3,4,5)\nspecify the change in the ", ", ", ", and ", " coordinates along\nthe vector.  If we represent the vector with an arrow that starts\nat the origin (0,0,0), then the head of the arrow will be at (3,4,5).\nBut we could just as well visualize the vector as an arrow that starts at\nthe point (1,1,1), and in that case the head of the arrow would be at\nthe point (4,5,6).", "The distinction between a point and a vector is subtle.  For some\npurposes, the distinction can be ignored; for other purposes, it is important.\nOften, all that we have is a sequence of numbers, which we can treat \nas the coordinates of either a vector or a point, whichever is more appropriate in the context.", "One of the basic properties of a vector is its ", ".\nIn terms of its coordinates, the length of a 3D vector (", ")\nis given by ", "(", "+", "+", ").\n(This is just the Pythagorean theorem in three dimensions.)  If ", " is\na vector, its length is denoted by\u00a0", ".\nThe length of a vector is also called its ", ".\n(We are considering 3D vectors here, but concepts and formulas are similar for other dimensions.)\n", "Vectors of length 1 are particularly important.  They are called\n", ".  If ", " = (", ")\nis any vector other than (0,0,0), then there is exactly one unit vector\nthat points in the same direction as ", ".  That vector is given by", "where ", " is the length of ", ".  Dividing a vector by its\nlength is said to ", " the vector: The result\nis a unit vector that points in the same direction as the original\nvector.", "Two vectors can be added.  Given two vectors ", " = (", ") and\n", " = (", "), their sum is defined as", "The sum has a geometric meaning:", "\n", "Multiplication is more complicated.  The obvious definition of the product of two vectors,\nsimilar to the definition of the sum, does not have geometric meaning and is rarely used.\nHowever, there are three kinds of vector multiplication that are used: the scalar\nproduct, the dot produt, and the cross product.", "If ", " = (", ") is a vector and ", " is a number, then the ", "\nof ", " and ", " is defined as", "Assuming that ", " is positive and ", " is not zero, ", " is a vector that points in the same\ndirection as ", ", whose length is ", " times the length of ", ".  If ", " is negative,\n", " points in the opposite direction from ", ", and its length is ", "\ntimes the length of ", ".  This type of product is called a scalar product because a number like\n", " is also referred to as a \"scalar,\" perhaps because multiplication by ", " scales ", "\nto a new length.", "Given two vectors ", " = (", ") and\n", " = (", "), the ", "\nof ", " and ", " is denoted by ", "\u00b7", " and is defined\nby", "Note that the dot product is a number, not a vector.\nThe dot product has several very important geometric meanings.  First of\nall, note that the length of a vector ", " is just the square root of\n", "\u00b7", ".  Furthermore, the dot product of two non-zero\nvectors ", " and ", " has the property that\n", "where ", " is the measure of the angle between ", " and ", ".  In\nparticular, in the case of two unit vectors, whose lengths are 1, the dot product of\ntwo unit vectors is simply the cosine of the angle between them.  Furthermore,\nsince the cosine of a 90-degree angle is zero, two non-zero vectors are perpendicular\nif and only if their dot product is zero.  Because of these properties,\nthe dot product is particularly important in lighting calculations, where the\neffect of light shining on a surface depends on the angle that the light makes \nwith the surface.", "The scalar product and dot product are defined in any dimension.  For vectors in 3D, there is\nanother type of product called the ", ", which also\nhas an important geometric meaning. For vectors ", " = (", ") and\n", " = (", "), the cross product of ", "\nand ", " is denoted ", "\u00d7", " and is the vector defined by\n", "If ", " and ", " are non-zero vectors, then ", "\u00d7", "\nis zero if and only if ", " and ", " point in the same direction or in\nexactly opposite directions.  Assuming ", "\u00d7", " is non-zero, then\nit is perpendicular both to ", " and to ", "; furthermore, \nthe vectors ", ", ", ", ", "\u00d7", " follow the\n", "; that is, if you curl the fingers of your right hand from \n", " to ", ", then your thumb points in the direction of ", "\u00d7", ". If\n", " and ", " are perpendicular unit vectors, then the cross product\n", "\u00d7", " is also a unit vector, which is perpendicular both\nto ", " and to ", ".", "Finally, I will note that given two points ", " = (", ") and\n", " = (", "), the difference ", "\nis defined by", "This difference is a vector that can be visualized as an arrow that starts at ", "\nand ends at ", ".", "Now, suppose that ", ", ", ", and ", "\nare vertices of a polygon.  Then the vectors ", " and\n", " lie in the plane of the polygon, and so the cross product", "is a vector that is perpendicular to the polygon.", "\n", "This vector is said\nto be a ", " for the polygon.  A normal vector of length one\nis called a ", ".  Unit normals will be important in lighting\ncalculations, and it will be useful to be able to calculate a unit normal for a polygon\nfrom its vertices.", "A matrix is just a two-dimensional array of numbers.  A matrix with ", " rows and\n", " columns is said to be an ", "-by-", " matrix.  If ", " and ", "\nare matrices, and if the number of columns in ", " is equal to the number of\nrows in ", ", then ", " and ", " can be multiplied to give the matrix\nproduct ", ".  If ", " is an ", "-by-", " matrix and ", " is\nan ", "-by-", " matrix, then ", " is an ", "-by-", " matrix.\nIn particular, two ", "-by-", " matrices can be multiplied to give another\n", "-by-", " matrix.", "An ", "-dimensional vector can be thought of an ", "-by-", " matrix.  If\n", " is an ", "-by-", " matrix and ", " is a vector in ", " dimensions,\nthought of as an ", "-by-", " matrix, then the product ", " is again an\n", "-dimensional vector (though in this case thought of as a ", "-by-", " matrix).\nThe product of a 3-by-3 matrix ", " and a 3D vector ", " = (", ")\nis often displayed like this:", "\n", "Note that the ", "-th coordinate in the product ", " is simply the dot product of the\n", "-th row of the matrix ", " and the vector\u00a0", ".", "Using this definition of the multiplication of a vector by a matrix, a matrix defines a\ntransformation that can be applied to one vector to yield another vector.\nTransformations that are defined in this way are ", ",\nand they are the main object of study in linear algebra.  A linear transformation ", " has\nthe properties that for two vectors ", " and ", ", ", ",\nand for a number ", ", ", ".", "Rotation and scaling are linear transformations, but translation is ", "\na linear transformaton.\nTo include translations, we have to widen our view of transformation to include\n", ".\nAn affine transformation can be defined, roughly, as a linear transformation followed by a \ntranslation.  Geometrically, an affine transformation is a transformation that preserves\nparallel lines; that is, if two lines are parallel, then their images under an affine\ntransformation will also be parallel.\nFor computer graphics, we are interested in affine transformations in\nthree dimensions.  However\u2014by what seems at first to be a very odd trick\u2014we\ncan narrow our view back to the linear by moving into the fourth dimension.\n", "Note first of all that an affine transformation in three dimensions transforms a vector\n(", ") into a vector (", ") given by\nformulas", "These formulas express a linear transformation given by multiplication by the 3-by-3 matrix", "\n", "followed by translation by ", " in the ", " direction, ", " in the ", "\ndirection and ", " in the ", " direction.  The trick is to replace each three-dimensional\nvector (", ") with the four-dimensional vector\n(", "1), adding a \"1\" as the fourth coordinate.   And instead of \nthe 3-by-3 matrix, we use the 4-by-4 matrix\n", "\n", "If the vector (", ",1) is multiplied by this 4-by-4 matrix,\nthe result is  precisely the vector (", "1).  That is,\ninstead of applying an ", " transformation to the 3D vector (", "),\nwe can apply a ", " transformation to the 4D vector (", ",1).", "This might seem pointless to you, but nevertheless, that is what is done in OpenGL and\nother 3D computer graphics systems:  An\naffine transformation is represented as a 4-by-4 matrix in which the bottom row is\n(0,0,0,1), and a three-dimensional vector is changed into a four dimensional vector\nby adding a 1 as the final coordinate.  The result is that all the affine transformations\nthat are so important in computer graphics can be implemented as multiplication of\nvectors by matrices.", "The identity transformation, which leaves vectors unchanged, corresponds to multiplication\nby the ", ", which has ones along its descending diagonal and\nzeros elsewhere. The OpenGL function ", "() sets the current matrix to\nbe the 4-by-4 identity matrix.  An OpenGL transformation function, such as ", "(", "),\nhas the effect of multiplying the current matrix  by the 4-by-4 matrix that\nrepresents the transformation.  Multiplication is on the right; that is, if ", " is\nthe current matrix and ", " is the matrix that represents the transformation, then\nthe current matrix will be set to the product matrix\u00a0", ". For the record,\nthe following illustration shows the identity matrix and the matrices\ncorresponding to various OpenGL transformation functions:", "\n", "It is even possible to use an arbitrary transformation matrix in OpenGL, using the\nfunction ", "(", ") or ", "(", "). The parameter, ", ",\nis an array of numbers of type ", " or ", ",\nrepresenting a transformation matrix.  The array is a one-dimensional array of length 16.\nThe items in the array are the numbers from the transformation matrix, stored in ", ",\nthat is, the numbers in the fist column, followed by the numbers in the second\ncolumn, and so on.  These function multiply the current matrix by the matrix ", ",\non the right.  You could use them, for example, to implement a ", ",\nwhich is not easy to represent as a sequence of scales, rotations, and translations.", "We finish this section with a bit of mathematical detail about the implementation of transformations.\nThere is one common transformation in computer graphics that is not an affine transformation:\nIn the case of a perspective projection, the projection transformation is not affine.\nIn a perspective projection, an object will appear to get smaller as it moves farther away\nfrom the viewer, and that is a property that no affine transformation can express, since\naffine transforms preserve parallel lines and parallel lines will seem to converge in the\ndistance in a perspective projection.\n", "Surprisingly, we can still represent a perspective projection as a 4-by-4 matrix, provided\nwe are willing to stretch our use of coordinates even further than we have already.  We\nhave already represented 3D vectors by 4D vectors in which the fourth coordinate is 1.\nWe now allow the fourth coordinate to be anything at all.  When the fourth coordinate, ", ",\nis non-zero, we consider the coordinates (", ") to\nrepresent the three-dimensional vector (", ").  Note that this\nis consistent with our previous usage, since it considers (", ")\nto represent (", "), as before.   When the fourth coordinate is zero,\nthere is no corresponding 3D vector, but it is possible to think of (", ",0)\nas representing a 3D \"point at infinity\" in the direction of (", "),\nas long as at least one of ", ", ", ", and ", " is non-zero. \n", "Coordinates (", ") used in this way are referred to\nas ", ".  If we use homogeneous coordinates, then any\n4-by-4 matrix can be used to transform three-dimensional vectors, including matrices whose\nbottom row is not (0,0,0,1). Among the transformations\nthat can be represented in this way is the projection transformation for a perspective\nprojection.  And in fact, this is what OpenGL does internally.  It represents all three-dimensional\npoints and vectors using homogeneous coordinates, and it represents all transformations as\n4-by-4 matrices.  You can even specify vertices using homogeneous coordinates.  For example, the\ncommand\n", "with a non-zero value for ", ", generates the 3D point (", ").  Fortunately, you will almost never\nhave to deal with homogeneous coordinates directly.  The only real exception to this is\nthat homogeneous coordinates are used, surprisingly, when configuring OpenGL lighting, as\nwe'll see in the ", "."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.5}, {"section_title": "Hierarchical Modeling", "chapter_id": "Chapter 2", "section_id": "Section 2.4", "content": ["In this section, we look at how complex scenes can be built\nfrom very simple shapes.  The key is hierarchical\nstructure.  That is, a complex object can be made up of\nsimpler objects, which can in turn be made up of even\nsimpler objects, and so on until it bottoms out with\nsimple ", "\nthat can be drawn directly.  This is called\n", ".  We will see\nthat the ", "\nthat were studied in the ", "\nplay an important role in hierarchical modeling.", "Hierarchical structure is the key to dealing with complexity\nin many areas of computer science (and in the rest of reality),\nso it be no surprise that it plays an important role in\ncomputer graphics.", "A major motivation for introducing a new coordinate system is that it should be\npossible to use the coordinate system that is most natural to the scene that you want to\ndraw.  We can extend this idea to individual objects in a scene:  When drawing an object,\nuse the coordinate system that is most natural for the object.\n", "Usually, we want an object in its natural coordinates to be centered at the origin, (0,0),\nor at least to use the origin as a convenient reference point.  Then, to place it in\nthe scene, we can use a ", " transform, followed by a ", ", \nfollowed by a ", " to set its size, orientation, and position in the scene.  \nRecall that transformations used in this way are called \n", ".\nThe transforms are often applied in the order scale, then rotate, then translate,\nbecause scaling and rotation leave the reference point, (0,0), fixed.  Once the object\nhas been scaled and rotated, it's easy\nto use a translation to move the reference point to any desired point in the scene.\n(Of course, in a particular case, you might not need all three operations.) Remember that in the code,\nthe transformations are specified in the opposite order from the order in which they are\napplied to the object and that the transformations are specified before drawing the\nobject.  So in the code, the translation would come first, followed by the rotation and\nthen the scaling.  Modeling transforms are not always composed in this order, but\nit is the most common usage.", "The modeling transformations that are used to place an object in the scene should not\naffect other objects in the scene.  To limit their application to just the one object,\nwe can save the current transformation before starting work on the object and restore it\nafterwards.  How this is done differs from one graphics ", " to another, \nbut let's suppose here that there are subroutines ", "() and\n", "() for performing those tasks.  That is, ", "\nwill make a copy of the modeling transformation that is currently in effect and store\nthat copy.  It does not change the current transformation; it merely saves a copy.\nLater, when ", " is called, it will retrieve that copy and will\nreplace the current modeling transform with the retrieved transform.  Typical code\nfor drawing an object will then have the form:", "Note that we don't know and don't need to know what the saved transform does.\nPerhaps it is simply the so-called ", ", which\nis a transform that doesn't modify the coordinates to which it is applied.\nOr there might already be another transform in place, such as a coordinate transform that affects the scene as a whole.\nThe modeling transform for the object is effectively applied in addition to any other transform that\nwas specified previously.  The modeling transform moves the object from its natural coordinates into its\nproper place in the scene.  Then on top of that, a coordinate transform that is applied to the scene as a whole\nwould carry the object along with it.", "Now let's extend this idea.  Suppose that the object that we want to draw is itself a complex \npicture, made up of a number of smaller objects.  Think, for example, of a potted flower made up of\npot, stem, leaves, and bloom.  We would like to be able to draw the smaller component objects in their\nown natural coordinate systems, just as we do the main object.  For example, we would like to specify\nthe bloom in a coordinate system in which the center of the bloom is at (0,0).\nBut this is easy:  We draw each small component object, such as the bloom,\nin its own coordinate system, and use a modeling transformation to move the sub-object\ninto position ", ".  We are composing the complex object in its\nown natural coordinate system as if it were a complete scene.", "On top of that, we can apply ", " modeling\ntransformation to the complex object as a whole, to move it into the actual scene; \nthe sub-objects of the complex object are carried along with it.  That is,\nthe overall transformation that applies to a sub-object consists of a modeling transformation\nthat places the sub-object into the complex object, followed by the transformation that\nplaces the complex object into the scene.", "In fact, we can build objects that are made up of smaller objects which in turn\nare made up of even smaller objects, to any level. For example, we could draw the bloom's petals in\ntheir own coordinate systems, then apply modeling transformations to place the petals into the\nnatural coordinate system for the bloom.  There will be another\ntransformation that moves the bloom into position\non the stem, and yet another transformation that places the entire potted flower into the scene.\nThis is hierarchical modeling.", "Let's look at a little example.  Suppose that we want to draw a simple 2D image of a cart with\ntwo wheels.", "\n", "This cart is used as one part of a complex scene in an example below.\nThe body of the cart can be drawn as a pair of rectangles.  For the wheels, suppose that we\nhave written a subroutine", "that draws a wheel.  This subroutine draws the wheel in its own natural coordinate system. \nIn this coordinate system, the wheel is centered at (0,0) and has radius\u00a01.", "In the cart's coordinate system, I found it convenient to use the midpoint of the base of the\nlarge rectangle as the reference point.  I \nassume that the positive direction of the ", "-axis points upward, which is the common\nconvention in mathematics.  The rectangular body of the cart has\nwidth 6 and height 2, so the coordinates of the lower left corner of the rectangle are (-3,0),\nand we can draw it with a command such as ", "(-3,0,6,2).\nThe top of the cart is a smaller red rectangle, which can be drawn in a similar way.\nTo complete the cart, we need\nto add two wheels to the object.  To make the size of the wheels fit the cart, they need to be scaled.\nTo place them in the correct positions relative to body of the cart, one wheel must be translated\nto the left and the other wheel, to the right.  When I coded this example, I had to play\naround with the numbers to get the right sizes and positions for the wheels, and I found\nthat the wheels looked better if I also moved them down a bit.  Using the usual techniques of\nhierarchical modeling, we save the current transform before drawing each wheel, and we restore it after\ndrawing the wheel. This restricts the effect of the modeling transformation for the wheel\nto that wheel alone, so that it does not affect any other part of the cart.\nHere is pseudocode for a  subroutine that draws the cart in its own coordinate system:", "It's important to note that the same subroutine is used to draw both wheels.  The reason that\ntwo wheels appear in the picture in different positions is that different modeling transformations are in effect for the\ntwo subroutine calls.", "Once we have this cart-drawing subroutine, we can use it to add a cart to a scene.\nWhen we do this, we apply another modeling transformation to the cart as a whole.  Indeed, we could add several carts\nto the scene, if we wanted, by calling the ", " subroutine several times with different modeling transformations.\n", "You should notice the analogy here:  Building up a complex scene out of objects is similar to\nbuilding up a complex program out of subroutines.  In both cases, you can work on pieces of the\nproblem separately, you can compose a solution to a big problem from solutions to smaller problems,\nand once you have solved a problem, you can reuse that solution in several places.\n", "Here is a demo that uses\nthe cart in an animated scene:", "\n", "\n", "You can probably guess how hierarchical modeling is used to draw \nthe three windmills in this example.\nThere is a ", " method that draws a windmill in its own coordinate system.  Each of the\nwindmills in the scene is then produced by applying a different modeling transform to the standard\nwindmill.  Furthermore, the windmill is itself a complex object that is constructed from several\nsub-objects using various modeling transformations.", "It might not be so easy to see how different parts of the scene can be animated.  In fact, animation\nis just another aspect of modeling.  A computer ", " consists of a sequence of frames.  Each frame\nis a separate image, with small changes from one frame to the next.  From our point of view, each frame\nis a separate scene and has to be drawn separately.  The same object can appear in many frames.  To\nanimate the object, we can simply apply a different modeling transformation to the object in each\nframe.  The parameters used in the transformation can be computed from the current time or from the frame number.\nTo make a cart move from left to right, for example, we might apply a modeling transformation", "to the cart, where ", " is the frame number.\nIn each frame, the cart will be 0.1 units farther to the right than in the previous\nframe.  (In fact, in the actual program, the translation that is applied to the cart is\n", "which moves the reference point of the cart from -3 to 13 along the horizontal axis\nevery 300 frames.  In the coordinate system that is used for the scene, the x-coordinate\nranges from 0 to 7, so this puts the cart outside the scene for much of the loop.)", "The really neat thing is that this type of animation works with hierarchical modeling.  For example,\nthe ", " method doesn't just draw a windmill\u2014it draws an ", " windmill,\nwith turning vanes.  That just means that the rotation applied to the vanes depends on the frame \nnumber.  When a modeling transformation is applied to the windmill, the rotating vanes are scaled and\nmoved as part of the object as a whole.  This is an example of hierarchical modeling.\nThe vanes are sub-objects of the windmill.  The rotation of the vanes is part of the modeling\ntransformation that places the vanes into the windmill object.  Then a further modeling transformation\nis applied to the windmill object to place it in the scene.\n", "The file ", " contains\nthe complete source code for a Java version of this example.  The ", "\nof this book covers graphics programming in Java.  Once you are familiar with that, you should take\na look at the source code, especially the ", "() method, which draws the entire scene.", "Logically, the components of a complex scene form a structure.  In this structure,\neach object is associated with the sub-objects that it contains.  If the scene is hierarchical,\nthen the structure is hierarchical.  This structure is known as a\n", ".  A scene graph is a tree-like structure,\nwith the root representing the entire scene, the children of the root representing the\ntop-level objects in the scene, and so on.  We can visualize the scene graph for our\nsample scene:", "\n", "In this drawing, a single object can have several connections to one or\nmore parent objects.  Each connection represents one occurrence of the object in its\nparent object.  For example, the \"filled square\" object occurs as a sub-object\nin the cart and in the windmill.  It is used twice in the cart and once in the\nwindmill.  (The cart contains two red rectangles, which are created as squares\nwith a non-uniform scaling; the pole of the windmill is made as a scaled square.)\nThe \"filled circle\" is used in the sun and is used twice in the wheel.  The \"line\"\nis used 12 times in the sun and 12 times in the wheel; I've drawn one thick arrow, marked\nwith a 12, to represent 12 connections.  The wheel, in turn, is used twice in\nthe cart.  (My diagram leaves out, for lack of space, two occurrences of the filled\nsquare in the scene: It is used to make the road and the line down the middle of the road.)", "Each arrow in the picture can be associated with a modeling transformation\nthat places the sub-object into its parent object.  When an object contains several\ncopies of a sub-object, each arrow connecting the sub-object to the object will have\na different associated modeling transformation.  The object is the same for each copy;\nonly the transformation differs.", "Although the scene graph exists conceptually, in some applications it exists\nonly implicitly.  For example, the Java version of the program that was\nmentioned above draws the image \"procedurally,\" that is, by calling subroutines.\nThere is no data structure to represent the scene graph.\nInstead, the scene graph is implicit in\nthe sequence of subroutine calls that draw the scene.  Each node in the graph is a subroutine,\nand each arrow is a subroutine call.  The various objects are drawn using different\nmodeling transformations.  As discussed in ", ",\nthe computer only keeps track of a \"current transformation\" that represents all\nthe transforms that are applied to an object.  When an object is drawn by a subroutine,\nthe program saves the current transformation before calling the subroutine.\nAfter the subroutine returns, the saved transformation is restored.  \nInside the subroutine, the object is drawn in its own\ncoordinate system, possibly calling other subroutines to draw sub-objects with their\nown modeling transformations. Those extra transformations will have no effect outside of the subroutine,\nsince the transform that is in effect before the subroutine is called will be restored\nafter the subroutine returns.", "It is also possible for a scene graph to be represented by an actual data structure in the program.\nIn an object-oriented approach, the graphical objects in the scene are represented by\nprogram objects.  There are many ways to build an object-oriented scene graph ", ".\nFor a simple example implemented in Java, you can take a look at\n", ".  This program draws\nthe same animated scene as the previous example, but it represents the scene with\nan object-oriented data structure rather than procedurally.   The same scene graph API\nis implemented in ", " in the live demo shown\nearlier on this page, and you might take a look at that after you read about\n", " graphics in ", ".", "In the example\nprogram, both in Java and in JavaScript, a node in the scene graph is\nrepresented by an object belonging to a class named ", ".\n", " is an abstract class, and actual nodes in the scene graph are defined by \nsubclasses of that class.  For example, there is a subclass named\n", " to represent a complex graphical object that is\nmade up of sub-objects.  A variable, ", ", of type ", " includes \na method ", "(", ") for adding a sub-object to the compound object.", "When implementing a scene graph as data structure made up of objects,\na decision has to be made about how to handle transforms. \nOne option is to allow transformations to be associated with any node in the scene graph.  In this case, however,\nI decided to use special nodes to represent transforms as\nobjects of type  ", ".\nA ", " is a ", " that\ncontains a link to another ", " and also contains a\nmodeling transformation that is to be applied to that object.\nThe modeling transformation is given in terms of scaling, rotation, and translation amounts\nthat are instance variables in the object.  It is worth noting that these are always applied\nin the order scale, then rotate, then translate, no matter what order the instance variables\nare set in the code. If you want to do a translation followed by a rotation, you will need\ntwo ", " to implement it, since a translation plus a rotation\nin the same ", " would be applied in the order rotate-then-translate.\nIt is also worth noting that the setter methods for the scaling,\nrotation, and translation have a return value that is equal to the object.  This makes it possible\nto chain calls to the methods into a single statement such as", "and even say things like", "This type of chaining can make for\nmore compact code and can eliminate the need for a lot of extra temporary variables.", "Another decision has to be made about how to handle color.  One possibility would be\nto make a ", " class similar to\n", ".  However, in this case I just added\na ", "() method to the main ", "\nclass.  A color that is set on a compound object is inherited by any sub-objects,\nunless a different color is set on the sub-object.  In other words, a color on a compound object\nacts as a default color for its sub-objects, but color can be overridden on the sub-objects.", "In addition to compound objects and transformed objects, we need scene graph\nnodes to represent the basic graphical objects that occupy the bottom level of the scene graph.\nThese are the nodes that do the actual drawing in the end.", "For those who are familiar with data structures, I will note that\na scene graph is actually an example of a \"directed acyclic graph\" or \"dag.\"\nThe process of drawing the scene involves a traversal of this dag.  The\nterm \"acyclic\" means that there can't be cycles in the graph.  For a scene graph,\nthis is the obvious requirement that an object cannot be a sub-object, either\ndirectly or indirectly, of itself.", "Suppose that you write a subroutine to draw an object. At the beginning of the\nsubroutine, you use a routine such as ", "() to save a copy of the current transform.\nAt the end of the subroutine, you call ", "() to reset the current\ntransform back to the value that was saved.  Now, in order for this to work correctly\nfor hierarchical graphics, these routines must actually use a ", " of transforms.\n(Recall that a stack is simply a list where items can be added, or \"pushed,\" onto\none end of the list and removed, or \"popped,\" from the same end.)  The problem is that\nwhen drawing a complex object, one subroutine can call other subroutines.  This means that\nseveral drawing subroutines can be active at the same time,\neach with its own saved transform.  When a transform is saved after\nanother transform has already been saved, the system needs\nto remember both transforms.  When ", "() is called, it is the\nmost recently saved transform that should be restored.", "A stack has exactly the structure that is needed to implement these operations.\nBefore you start drawing an object, you would push the current transform onto the stack.  After drawing\nthe object, you would pop the transform from the stack.  Between those two operations, if the object is\nhierarchical, the transforms for its sub-objects will have been pushed onto and popped from the\nstack as needed.", "Some graphics APIs come with transform stacks already defined.\nFor example, the original OpenGL API includes the functions\n", "() and ", "() for using a stack of transformation matrices\nthat is built into OpenGL.  The Java 2D graphics API does not include a built-in\nstack of transforms, but it does have methods for getting and setting the current\ntransform, and the get and set methods can be used with an explicit stack data structure\nto implement the necessary operations.\nWhen we turn to the HTML canvas API for 2D graphics, we'll see that it includes\nfunctions named ", "() and ", "() that are actually ", "\nand ", " operations on a stack.  These functions are essential to implementing\nhierarchical graphics for an HTML canvas.", "Let's try to bring this all together by considering how it applies to a simple object\nin a complex scene: one of the filled circles that is part of the front wheel on the cart\nin our example scene.  Here, I have rearranged part of the scene graph for that scene, and I've added\nlabels to show the modeling transformations that are applied to each object:", "\n", "The rotation amount for the wheel and the translation amount for the cart are shown as\nvariables, since they are different in different frames of the animation.  When the computer\nstarts drawing the scene, the modeling transform that is in effect is the\n", ", that is, no transform at all.  As it prepares to draw\nthe cart, it saves a copy of the current transform (the identity) by pushing it onto the stack.\nIt then modifies the current transform by multiplying it by the modeling transforms for the cart,\n", "(0.3,0.3) and ", "(dx,0).  When it comes to drawing the wheel, it\nagain pushes the current transform (the modeling transform for the cart as a whole) onto the\nstack, and it modifies the current transform to take the wheel's modeling transforms into\naccount.  Similarly, when it comes to the filled circle, it saves the modeling transform\nfor the wheel, and then applies the modeling transform for the circle.", "When, finally, the\ncircle is actually drawn in the scene, it is transformed by the combined transform.\nThat transform places the circle directly into the scene, but it has been composed\nfrom the transform that places the circle into the wheel, the one that places the wheel \ninto the cart, and the one that places the cart into the scene.  After drawing the circle,\nthe computer replaces the current transform with one it pops from the stack.  That will be the \nmodeling transform for the wheel as a whole, and that transform will be used for any further parts of the\nwheel that have to be drawn.  When the wheel is done, the transform for the cart is popped.\nAnd when the cart is done, the original transform, the identity, is popped.  When the computer\ngoes onto the next object in the scene, it starts the whole process again, with the identity\ntransform as the starting point.", "This might sound complicated, but I should emphasize that it something that the computer\ndoes for you.  Your responsibility is simply to design the individual objects, in their\nown natural coordinate system. As part of that, you specify the modeling transformations that are applied\nto the sub-objects of that object.  You construct the scene as a whole in a similar way.\nThe computer will then put everything together for you, taking into account the many layers\nof hierarchical structure.  You only have to deal with one component of the structure at\na time.  That's the power of hierarchical design; that's how it helps you deal with complexity."], "chapter_title": "Two-Dimensional Graphics", "id": 2.4}, {"section_title": "Using GLUT and JOGL", "chapter_id": "Chapter 3", "section_id": "Section 3.6", "content": ["OpenGL is an ", " for graphics only, with no support for things like\nwindows or events.  OpenGL depends on external mechanisms to\ncreate the drawing surfaces on which it will draw.  Windowing APIs\nthat support OpenGL often do so as one library among many others that\nare used to produce a complete application.  We will look at\ntwo cross-platform APIs that make it possible to use OpenGL\nin applications, one for C/C++ and one for Java.", "For simple applications written in C or C++, one possible\nwindowing API is ", " (OpenGL Utility Toolkit).  GLUT is a very small\nAPI.  It is used to create windows that serve as\nsimple frames for OpenGL drawing surfaces.  It has support for\nhandling mouse and keyboard events, and it can do basic animation.\nIt does not support controls such as buttons or input fields,\nbut it does allow for a menu that pops up in response to\na mouse action.  You can find information about the GLUT API at", "\n", "\n", "If possible, you should use FreeGLUT, which is compatible with GLUT but has\na few extensions and a fully open source license.   See", "\n", "\n", "\n", " (Java OpenGL) is a collection of classes that make it\npossible to use OpenGL in Java applications.  JOGL is integrated\ninto Swing and AWT, the standard Java graphical user interface APIs.\nWith JOGL, you can create Java GUI components on which\nyou can draw using OpenGL.  These OpenGL components can be\nused in any Java application, in much the same way that you\nwould use a ", "\nor ", " as a drawing surface.\nLike many things Java, JOGL is immensely complicated.  We will use it\nonly in fairly simple applications.\nJOGL is not a standard part of Java.  It's home web site is", "\n", "\n", "This section contains information to get you started using GLUT and JOGL, assuming\nthat you already know how the basics of programming with C and Java.  It also briefly\ndiscusses ", ", a JavaScript library that I have written to simulate the subset\nof OpenGL 1.1 that is used in this book.", "To work with GLUT, you will need\na C compiler and copies of the OpenGL and GLUT (or FreeGLUT)\ndevelopment libraries.  I can't tell you exactly that means on\nyour own computer.  On my computer, which runs Linux Mint, for example,\nthe free C compiler gcc is already available.  To do OpenGL\ndevelopment, I installed several packages, including\n", " and ", ".\n(Mesa is a Linux implementation of OpenGL.)  If ", " contains\na complete C program that uses GLUT, I can compile it using a command such as\n", "The \"-o glutprog\" tells the compiler to use \"glutprog\" as the\nname of its output file, which can then be run as a normal executable file;\nwithout this option, the executable file would be named \"a.out\".\nThe \"-lglut\" and \"-lGL\" options tell the compiler to link the program with the GLUT and OpenGL libraries.\n(The character after the \"-\" is a lower case \"L\".)\nWithout these options, the compiler won't recognize any GLUT or OpenGL functions.  If the program\nalso uses the ", " library, compiling it would require the option \"-lGLU, and if it uses\nthe math library, it would need the option \"-lm\".  If a program requires additional .c files,\nthey should be included as well.  For example, the sample program\n", " depends on ", ", and it\ncan be compiled with the Linux gcc compiler using the command:", "The sample program ", " can be used as a starting\npoint for writing programs that use GLUT.  While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "The GLUT library makes it easy to write basic OpenGL applications in\u00a0C.  GLUT\nuses event-handling functions.  You write functions to handle events that occur\nwhen the display needs to be redrawn or when the user clicks the mouse or presses a key\non the keyboard.", "To use GLUT, you need to include the header file ", " (or ", ")\nat the start of any source code file that uses it, along with the general OpenGL header file,\n", ".  The header files should be installed in a standard location, in a folder named ", ".\nSo, the program usually begins with", "On my computer, saying ", " actually includes the subset\nof FreeGLUT that corresponds to GLUT.  To get access to all of FreeGLUT, I would\nsubstitute ", ".  Depending on the features that it uses,\na program might need other header files, such as ", " \nand ", ".", "The program's ", "() function must contain some code to initialize GLUT, to\ncreate and open a window, and to set up event handling by registering the functions that\nshould be called in response to various events.  After this setup, it must\ncall a function that runs the GLUT event-handling loop.  That function\nwaits for events and processes them by calling the functions that have been registered\nto handle them.  The event loop runs until the program ends, which happens when\nthe user closes the window or when the program calls the standard ", "() function.", "To set up the event-handling functions,\nGLUT uses the fact that in C, it is possible to pass a function name as a parameter\nto another function.  For example, if ", "() is the function that \nshould be called to draw the content of the window, then the\nprogram would use the command", "to install this function as an event handler for\ndisplay events. A display event occurs when the contents of the window need to be redrawn, including\nwhen the window is first opened.\nNote that ", " must have been previously defined, as a function with no parameters:", "Keep in mind that it's not the name of this function that makes it an OpenGL display\nfunction.  It has to be set as the display function by calling ", "(", ").\nAll of the GLUT event-handling functions work in a similar way (except many of them do need\nto have parameters).", "There are a lot of possible event-handling functions, and I will only cover some of\nthem here.  Let's jump right in and look at a possible ", "() routine for a GLUT\nprogram that uses most of the common event handlers:", "The first five lines do some necessary initialization, the next seven lines install event\nhandlers, and the call to ", "() runs the GLUT event loop.  I will discuss all of\nthe functions that are used here.  The first GLUT function call must be ", ",\nwith the parameters as shown.  (Note that ", " and ", "\nrepresent command-line arguments for the program.  Passing them to ", " allows\nit to process certain command-line arguments that are recognized by GLUT.  I won't discuss those\narguments here.)  The functions ", " and ", "\ndo the obvious things; size is given in pixels, and \nwindow position is given in terms of pixel coordinates on the computer\nscreen, with (0,0) at the upper left corner of the screen.  The function ", "\ncreates the window, but note that nothing can happen in that window until ", "\nis called.  Often, an additional, user-defined function is called in ", "() to do\nwhatever initialization of global variables and OpenGL state is required by the program.\nOpenGL initialization can be done after calling ", " and before\ncalling ", ".  Turning to the other functions used in ", "(),", "\n", " \u2014 Must be called to\ndefine some characteristics of the OpenGL drawing context.  The parameter specifies\nfeatures that you would like the OpenGL context to have.  The features are represented by\nconstants that are OR'ed together in the parameter.  ", " says that a depth buffer\nshould be created; without it, the depth test won't work.  If you are doing 2D graphics, you\nwouldn't include this option.  ", " asks for ", ", \nwhich means that drawing is actually done off-screen, and the\noff-screen copy has to copied to the screen to be seen.  The copying is done by\n", ", which ", " be called at the end of the display function.\n(You can use ", " instead of ", " to get ", "; \nin that case, you have to call ", "() at the end of the display function instead\nof ", "().  However, all of the examples in this book use ", ".)", "\n", " \u2014 The display function\nshould contain OpenGL drawing code that can completely redraw the scene.  This is\nsimilar to ", "() in Java.\nThe display function can have any name, but it must be declared as a void\nfunction with no parameters: ", "().", "\n", " \u2014 The reshape function\nis called when the user changes the size of the window.  Its parameters tell the\nnew width and height of the drawing area:", "For example, you might use this method to set up the projection transform, if the\nprojection depends only on the window size.  A reshape function is not required, but\nif one is provided, it should always set the\nOpenGL ", ", which is the part of the window that\nis used for drawing.  Do this by calling", "The viewport is set automatically if no reshape function is specified.", "\n", " \u2014 The keyboard function is\ncalled when the user types a character such as 'b' or 'A' or a space.  It is not called\nfor special keys such as arrow keys that do not produce characters when pressed.\nThe keyboard function has a parameter of type ", " which\nrepresents the character that was typed.  It also has two ", " parameters \nthat give the location of the mouse when the key was pressed, in pixel coordinates\nwith (0,0) at the upper left corner of the display area.  So, the definition of\nthe key function must have the form:", "Whenever you make any changes to the program's data that require the display to be redrawn,\nyou should call ", "().  This is similar to calling ", "() in\nJava.  It is better to call ", "()\nthan to call the display function directly.  (I also note that it's possible to\ncall OpenGL drawing commands directly in the event-handling functions, but it probably only makes\nsense if you are using single buffering; if you do this, call ", "()\nto make sure that the drawing appears on the screen.)", "\n", " \u2014 The \"special\"\nfunction is called when the user presses certain special keys, such as an arrow\nkey or the Home key.  The parameters are an integer code for the key that was pressed, plus the\nmouse position when the key was pressed:", "GLUT has constants to represent the possible key codes, including\n", ", ", ", ", ", and ", "\nfor the arrow keys and ", " for the Home key. For example,\nyou can check whether the user pressed the left arrow key by testing\n", "\u00a0", ".", "\n", " \u2014 The mouse function is\ncalled both when the user presses and when the user releases a button on the mouse, with a parameter to tell\nwhich of these occurred.  The function will generally look like this:", "The first parameter tells which mouse button was pressed or released; its\nvalue is the constant ", " for the left, ", " for the \nmiddle, and ", " for the right mouse button.  The other\ntwo parameters tell the position of the mouse.  The mouse position\nis given in pixel coordinates with (0,0) in the top left corner of the display area and\nwith y increasing from top to bottom.", "\n", " \u2014 The motion function\nis called when the user moves the mouse while dragging, that is, while a mouse button\nis pressed.  After the user presses the mouse in the OpenGL window, this function will\ncontinue to be called even if the mouse moves outside the window, and the mouse\nrelease event will also be sent to the same window.  The function has two parameters\nto specify the new mouse position:", "\n", " \u2014 The idle function is called by the\nGLUT event loop whenever there are no events waiting to be processed.  The\nidle function has no parameters.  It is called as often as possible, not at\nperiodic intervals.  GLUT also has a timer function, which schedules some function to be\ncalled once, after a specified delay.  To set a timer, call", "and define ", " as", "The parameter to ", " when it is called will be the same integer that was passed as\nthe third parameter to ", ".  If you want to use ", "\nfor animation, then ", " should end with another call to ", ".", "A GLUT window does not have a menu bar, but it is possible to add a hidden popup menu to the window.\nThe menu will appear in response to a mouse click on the display.  You can set whether it\nis triggered by the left, middle, or right mouse button.", "A menu is created using the function ", ",\nwhere the parameter is the name of a function that will be called when the user\nselects a command from the menu.  The function must be defined with a parameter of\ntype ", " that identifies the command that was selected:", "Once the menu has been created, commands are added to the menu by calling the function\n", "(", ").  The first parameter is the string that\nwill appear in the menu.  The second is an ", " that identifies the\ncommand; it is the integer that will be passed to the menu-handling function when\nthe user selects the command from the menu.", "Finally, the function ", "(", ") attaches the menu to the\nwindow.  The parameter specifies which mouse button will trigger the menu.  Possible\nvalues are ", ", ", ", and ", ".\nAs far as I can tell, if a mouse click is used to trigger the popup menu, than the same\nmouse click will ", " also produce a call to the mouse-handler function.", "Note that a call to ", " doesn't mention the menu, and a\ncall to ", " doesn't mention either the menu or the window.\nWhen you call ", ", the menu that is created becomes the \"current\nmenu\" in the GLUT state.  When ", " is called, it adds a command\nto the current menu.  When ", " is called, it attaches the current\nmenu to the current window, which was set by a call to ", ".\nAll this is consistent with the OpenGL \"state machine\" philosophy, where functions\nact by modifying the current state.", "As an example, suppose that we want to let the user set the background color for\nthe display.  We need a function to carry out commands that we will add to the menu.  For example,\nwe might define", "We might have another function to create the menu.  This function would be called\nin ", "(), after calling ", ":", "It's possible to have submenus in a menu.  I won't discuss the procedure here, but you can look\nat the sample program ", " for an example of using submenus.", "In addition to window and event handling, GLUT includes some functions for drawing basic 3D shapes\nsuch as spheres, cones, and ", ".  \nIt has two functions for each shape, a \"solid\" version that draws\nthe shape as a solid object, and a ", " version that draws \nsomething that looks like it's made of wire mesh.  (The wireframe is produced by drawing \njust the outlines of the polygons that make up the object.)  For example, the function", "draws a solid sphere with the given radius, centered at the origin.  Remember that this is\njust an approximation of a sphere, made up of polygons.  For the approximation, the sphere is divided by\nlines of longitude, like the slices of an orange, and by lines of latitude, like a stack of disks.\nThe parameters ", " and ", " tell how many subdivisions to use.  Typical values\nare 32 and 16, but the number that you need to get a good approximation for a sphere depends on the\nsize of the sphere on the screen.  The function ", " has the same parameters but\ndraws only the lines of latitude and longitude.  Functions for a cone, a cylinder, \nand a ", " (doughnut) are similar:", "For a torus, the ", " is the size of the doughnut hole.  The function", "draws a cube of a specified size.\nThere are functions for the other regular polyhedra that have no parameters and draw the \nobject at some fixed size:  ", "(), ", "(),\n", "(), and ", "().\nThere is also ", "(", ") that draws a famous object that is often used as an\nexample.  Here's what the teapot looks like:", "\n", "Wireframe versions of all of the shapes are also available.  For example,\n", "(", ") draws a wireframe teapot.  Note that \nGLUT shapes come with ", " that\nare required for lighting calculations.  However, except for the teapot, they do\nnot come with ", ", which are required for applying\ntextures to objects. ", "GLUT also includes some limited support for drawing text in an OpenGL drawing\ncontext.  I won't discuss that possibility here.  You can check the API\ndocumentation if you are interested, and you can find an example in the\nsample program ", ".", "JOGL is a framework for using OpenGL in Java programs.  It is a large and complex API that\nsupports all versions of OpenGL, but it is fairly easy to use for basic applications.\nIn my examples and discussion, I will be using JOGL\u00a02.3, the latest version\nas of March, 2015.  Note that version 2.3 is not fully compatible with earlier versions.\n(Hopefully, later versions will remain compatible with 2.3.)", "The sample program ", " can be used as a starting\npoint for writing OpenGL programs using JOGL. While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "To use JOGL, you will need two .jar files containing the Java classes for JOGL:\n", " and ", ".  In addition, you will\nneed two native library files.  A native library is\na collection of routines that can be called from Java but are not written in Java.  Routines\nin a native library will work on only kind of computer; you need a different native library\nfor each type of computer on which your program is to be used.  The native libraries for\nJOGL are stored in additional .jar files, which are available in several versions for\ndifferent computers.  For example, for 64-bit Linux, you need\n", " and ", ".\nFor 32-bit Linux, the files are\n", " and ", ".\nIt is unfortunate that there are different versions for 64 and 32 bit operating systems, since\nmany people don't know which they are using.  However, if you are in doubt, you can get\nboth; JOGL will figure out which of the two to use.\nFor Mac\u00a0OS, you need\n", " and ", ".\nFor 64-bit Windows, the files are\n", " and ", ".", "You can get the jar files from the JOGL web site, ", ".\nI extracted them from the very large (54 megabyte) archive file", "\n\n", "\n\n", "I have also made the necessary files available on my own web site, at", "\n\n", "\n\n", "JOGL is open-source, and the files are freely redistributable, according to their\n", ".", "To do JOGL development, you should create a directory somewhere on your computer to hold the jar\nfiles.  Place the two JOGL jar files in that directory, along with the two native library jar files\nfor your platform.  (Having extra native library jar files doesn't hurt, as long as you have\nthe ones that you need.)", "It is possible to do JOGL development on the command line.  You have to tell the\n", " command where to find the two JOGL jar files. You do that in the\nclasspath (\"-cp\") option to the ", " command.  For example, if you are working\nin Linux or MacOS, and if the jar\nfiles happen to be in the same directory where you are working, you might say:", "It's similar for Windows, except that the classpath uses a \";\" instead of a \":\" to\nseparate the items in the list:", "There is an essential period at the end of the classpath, which makes it possible for Java to\nfind .java files in the current directory.\nIf the jar files are not in the current directory,\nyou can use full path names or relative path names to the files.  For example,", "Running a program with the ", " command is exactly similar. For example:", "Note that you don't have to explicitly reference the native library jar files.\nThey just have to be in the same directory with the JOGL jar files.", "I do most of my Java development using the Eclipse IDE (", ").\nTo do development with JOGL in Eclipse, you will have to configure Eclipse\nwith information about the jar files.  To do that, start up Eclipse.  You want to\ncreate a \"User Library\" to contain the jar files:\nOpen the Eclipse Preferences window, and select \"Java\" / \"Build\u00a0Path\" / \"User\u00a0Libraries\"\non the left.  Click the \"New\" button on the right.  Enter \"JOGL\" (or any name you like) as the\nname of the user library.  Make sure that the new user library is selected in the\nlist of libraries, then click the \"Add External Jars\" button.  In the file selection box,\nnavigate to the directory that contains the JOGL jar files, and select the two jar files that\nare needed for JOGL, ", " and ", ".  \n(Again, you do not need to add the native libraries; they just need to be in the same directory\nas the JOGL jar files.)  Click \"Open.\"  The selected\njars will be added to the user library. (You could also add them one at a time, if you don't\nknow how to select multiple files.)  It should\nlook something like this:", "\n", "Click \"OK.\"  The user library has been created. You will only have to do this\nonce, and then you can use it in all of your JOGL projects.", "Now, to use OpenGL in a project, create a new Java project as usual in Eclipse.\nRight-click the project in the Project Explorer view, and select \"Build\u00a0Path\" /\n\"Configure\u00a0Build\u00a0Path\" from the menu.  You will see the project Properties\ndialog, with \"Build Path\" selected on the left.  (You can also access this through the\n\"Properties\" command in the \"Project\" menu.)  Select \"Libraries\" at the top of the\nwindow, and then click the \"Add\u00a0Library\" button.  In the popup window, select \"User\u00a0Library\"\nand click \"Next.\"  In the next window, select your JOGL User Library and click \"Finish.\"\nFinally, click \"OK\" in the main Properties window.  Your project should now be set up\nto do JOGL development.  You should see the JOGL User Library listed as part of the\nproject in the Project Explorer.  Any time you want to start a new JOGL project, you can go through\nthe same setup to add the JOGL User Library to the build path in the project.", "With all that setup out of the way, it's time to talk about actually\nwriting OpenGL programs with Java.  With JOGL,\nwe don't have to talk about mouse and keyboard handling or animation, since that can be done\nin the same way as in any Java program.  You will only need to know about a few classes from\nthe JOGL API.", "First, you need a GUI component on which you can draw using OpenGL.  For that, you\ncan use ", ", which is a subclass of ", ".\n(", " is for use in programs based on the Swing API; an alternative\nis ", ", which is a subclass of the older AWT class\n", ".)  The class is defined in the package ", ".\nAll of the other classes that we will need for basic OpenGL programming \nare in the package ", ".", "JOGL uses Java's event framework to manage OpenGL drawing contexts, and it defines a\ncustom event listener interface, ", ", to manage\nOpenGL events.  To draw on a ", " with OpenGL, you need to\ncreate an object that implements the ", " interface, and\nregister that listener with your ", ".  The ", "\ninterface defines the following methods:\n", "The ", " parameter in these methods tells which OpenGL drawing surface\nis involved.  It will be a reference to the ", ".\n(", "  is an interface that is implemented by\n", " and other OpenGL drawing surfaces.)\nThe ", "() method is a place to do OpenGL initialization.  (According to the\ndocumentation, it can actually be called several times, if the OpenGL context\nneeds to be recreated for some reason. So ", "() should not be used to\ndo initialization that shouldn't be done more than once.) The \n", "() method will be called to give you a chance to\ndo any cleanup before the OpenGL drawing context is destroyed.\nThe ", "() method is called when the window first opens and\nwhenever the size of the ", " changes.\nOpenGL's ", "() function is called automatically before ", "()\nis called, so you won't need to do it yourself.  Usually, you won't need to write\nany code in ", "() or ", "(), but they have to be there to\nsatisfy the definition of the ", " interface.", "The ", "() method is where the actual drawing is done and where you\nwill do most of your work.  It should ordinarily clear the drawing area and completely redraw the scene.\nTake a minute to study an outline for a minimal JOGL program.  It creates a\n", " which also serves as the\n", ":", "At this point, the only other thing you need to know is how to use OpenGL\nfunctions in the program.  In JOGL, the OpenGL\u00a01.1 functions are collected into\nan object of type ", ".  (There are different classes\nfor different versions of OpenGL;  ", " contains\nOpenGL\u00a01.1 functionality, along with later versions that are compatible with 1.1.)\nAn object of type ", " is an OpenGL graphics context,\nin the same way that an object of type ", "\nis a graphics context for ordinary Java 2D drawing.  The statement\n", "in the above program obtains the drawing context for\nthe ", ", that is, for the\n", " in that program.  The name of the\nvariable could, of course, be anything, but ", " or ", " is conventional.", "For the most part, using OpenGL functions in JOGL is the same as in C,\nexcept that the functions are now methods in the object ", ".  For example,\na call to ", "(", ") becomes", "The redundant \"gl.gl\" is a little annoying, but you get used to it.  OpenGL constants\nsuch as ", " are static members of ", ", so that, for\nexample, ", " becomes ", " in JOGL.\nParameter lists for OpenGL functions\nare the same as in the C API in most cases.  One exception is for functions such as ", "()\nthat take an array/pointer parameter in C.  In JOGL, the parameter becomes an ordinary\nJava array, and an extra integer parameter is added to give the position of the data in\nthe array.  Here, for example, is how one might draw a triangle in JOGL, with all the\nvertex coordinates in one array:", "The biggest change in the JOGL API is the use of ", "\ninstead of arrays in functions such as ", ".  This is discussed\nin ", ".  We will see in ", " that texture images also get special\ntreatment in JOGL.", "The JOGL API includes a class named ", " that makes GLUT's\nshape-drawing functions available in Java.  (Since you don't need GLUT's window or event functions\nin Java, only the shape functions are included.)  Class ", "\nis defined in the package ", ".\nTo draw shapes using this class, you need\nto create an object of type GLUT.  It's only necessary to make one of these for use in a program:", "The methods in this object include all the shape-drawing functions from the GLUT C API,\nwith the same names and parameters.  For example:", "(I don't know why these are instance methods in an object rather than\nstatic methods in a class; logically, there is no need for the object.)", "The GLU library is available through the class ", ",\n and it works similarly to GLUT.   That is, you have to create an object of type\n ", ", and the GLU functions will be available as methods\n in that object.  We have encountered GLU only for the functions ", "\n and ", ", which are discussed in ", ".\n For example,", "The JavaScript library ", " was written to accompany and support this textbook.\nIt implements the subset of OpenGL 1.1 that is discussed in ", " and\n", ", except for display lists (", ").\nIt is used in the demos that appear in\nthose chapters.  Many of the sample programs that are discussed in those chapters are available\nin JavaScript versions that use glsim.js.", "If you would like to experiment with OpenGL 1.1, \nbut don't want to go through the trouble of setting up a C or Java environment that supports \nOpenGL programming, you can consider writing your programs as web pages using glsim.js.\nNote that glsim is meant for experimentation and practice only, not for serious applications.", "The OpenGL API that is implemented by glsim.js is essentially the same as the C API, although \nsome of the details of semantics are different.  Of course the techniques for creating a\ndrawing surface and an OpenGL drawing context are specific to JavaScript and differ from\nthose used in GLUT or JOGL.", "To use glsim.js, you need to create an ", " document with a <canvas> element\nto serve as the drawing surface.  The HTML file has to import the script; if glsim.js is in the\nsame directory as the HTML file, you can do that with", "To create the OpenGL drawing context, use the JavaScript command", "where ", " is either a string giving the ", " of the <canvas> element or\nis the JavaScript ", " object corresponding to the <canvas> element. Once you\nhave created the drawing context in this way, any OpenGL commands that you give will apply to\nthe canvas.  To run the program, you just need to open the HTML document in a web browser that\nsupports ", ".", "The easiest way to get started programming is to modify a program that already exists.\nThe sample program ", ", from ", "\nis a very minimal example of using glsim.js.\nThe sample web page ", " can be used as a starting\npoint for writing longer programs that use glsim.js.  It provides a framework for doing OpenGL drawing,\nwith support for animation and mouse and keyboard events.  The code contains comments that tell \nyou how to use it.  Some documentation for the glsim.js library can be found in\n", "."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.6}, {"section_title": "Shapes and Colors in OpenGL 1.1", "chapter_id": "Chapter 3", "section_id": "Section 3.1", "content": ["This section introduces some of the core features\nof OpenGL.  Much of the discussion in this section is limited\nto 2D.  For now, all you need to\nknow about 3D is that it adds a third direction to the\n", " and ", " directions that are used in 2D.\nBy convention, the third direction is called\u00a0", ".\nIn the default coordinate system, the positive direction of\nthe ", "-axis points in a direction perpendicular to\nthe image.", "In the default coordinate system for OpenGL, the image\nshows a region of 3D space in which ", ", ", ", and\n", " all range from minus one to one.  To show a \ndifferent region, you have to apply a \n", ".  For now,\nwe will just use coordinates that lie between -1 and\u00a01.", "A note about programming: OpenGL can be implemented in many different programming\nlanguages, but the API specification more or less assumes that\nthe language is\u00a0C.  For the most part, the C specification\ntranslates directly into other languages.  The main differences\nare due to the special characteristics of arrays in the C language.\nMy examples will follow the C syntax, with a few notes about how\nthings can be different in other languages.  Since I'm following\nthe C API, I will refer to \"functions\" rather than \"subroutines\"\nor \"methods.\"  ", " \nexplains in detail how to write OpenGL programs in C and in Java.\nYou will need to consult that section before you can do any\nactual programming.  The live OpenGL 1.1 demos for\nthis book are written using a ", " simulator that implements\na subset of OpenGL 1.1.  That simulator is also discussed in\n", ".", "OpenGL can draw only a few basic shapes, including points, lines, and\ntriangles.  There is no built-in support for curves or curved surfaces;\nthey must be approximated by simpler shapes.  The basic shapes are\nreferred to as ", ".\nA primitive in OpenGL is defined by its ", ".\nA vertex is simply a point in 3D, given by its ", ", ", ", and\n", " coordinates.  Let's jump right in and see how to draw a \ntriangle.  It takes a few steps:", "Each vertex of the triangle is specified by a call to the function\n", ".  Vertices must be specified between calls to ", "\nand ", ".  The parameter to ", " tells which type of primitive\nis being drawn.  The ", " primitive allows you to draw more than\none triangle: Just specify three vertices for each triangle that you want to draw.", "(I should note that these functions actually just send commands to the ", ". OpenGL\ncan save up batches of commands to transmit together, and the drawing won't actually\nbe done until the commands are transmitted.  To ensure that that happens, the\nfunction ", "() must be called.  In some cases, this function might be\ncalled automatically by an OpenGL API, but you might well run into times when\nyou have to call it yourself.)", "For OpenGL, vertices have three coordinates.  The function ", "\nspecifies the ", " and ", " coordinates of the vertex, and the ", " coordinate \nis set to zero.  There is also a function ", " that specifies\nall three coordinates.  The \"2\" or \"3\" in the name tells how many parameters are\npassed to the function.  The \"f\" at the end of the name indicates that the parameters\nare of type ", ".  In fact, there are other \"glVertex\" functions,\nincluding versions that take parameters of type ", " or ", ",\nwith named like ", " and ", ".  There are even versions that\ntake four parameters, although it won't be clear for a while why they should exist.\nAnd, as we will see later, there are versions that take an array of numbers instead of individual numbers\nas parameters.  The entire set of vertex functions is often referred to as \"glVertex*\", \nwith the \"*\" standing in for the parameter specification.  (The proliferation of\nnames is due to the fact that the C programming language doesn't support overloading\nof function names; that is, C distinguishes functions only by their names and not by\nthe number and type of parameters that are passed to the function.)", "OpenGL 1.1 has ten kinds of primitive.  Seven of them still exist in modern OpenGL; the\nother three have been dropped.  The simplest primitive is ", ",\nwhich simply ", " a point at each vertex of the primitive.\nBy default, a point is rendered as a single pixel.  The size of point primitives can be\nchanged by calling", "where the parameter, ", ", is of type ", " and specifies the\ndiameter of the rendered point, in pixels.  By default,\npoints are squares.  You can get circular points by calling", "The functions ", " and ", " change the OpenGL \"state.\"\nThe state includes all the settings that affect rendering.  We will encounter\nmany state-changing functions.  The functions ", " and ", "\ncan be used to turn many features on and off.  In general, the rule is that any\nrendering feature that requires extra computation is turned off by default.  If you want\nthat feature, you have to turn it on by calling ", " with the appropriate\nparameter.", "There are three primitives for drawing line segments:  ", ",\n", ", and ", ".  ", " draws\ndisconnected line segments; specify two vertices for each segment that you\nwant to draw.  The other two primitives draw connected sequences of line\nsegments.  The only difference is that ", " adds an extra\nline segment from the final vertex back to the first vertex.  Here is what\nyou get if use the same six vertices with the four primitives we have\nseen so far:", "\n", "The points A, B, C, D, E, and F were specified in that order.  In this illustration,\nall the points lie in the same plane, but keep in mind that in general, points can\nbe anywhere in 3D space.", "The width for line primitives can be set by calling ", "(", ").\nThe line width is always specified in pixels.  It is ", " subject to scaling by\ntransformations.", "Let's look at an example.  OpenGL does not have a circle primitive, but we can\napproximate a circle by drawing a polygon with a large number of sides.  To draw\nan outline of the polygon, we can use a ", " primitive:", "This draws an approximation for the circumference of a circle of radius 0.5 with\ncenter at (0,0). \nRemember that to learn how to use examples like this one in\na complete, running program, you will have to read ", ".\nAlso, you might have to make some changes to the code, depending on which OpenGL\nimplementation you are using.", "The next set of primitives is for drawing triangles.  There are three of them:\n", ", ", ", and ", ".", "\n", "The three triangles on the left make up one ", " primitive, with nine vertices.  \nWith that primitive, every set of three vertices makes a separate triangle.  For a\n", " primitive, the first three vertices produce a triangle.  After that,\nevery new vertex adds another triangle to the strip, connecting the new vertex to the\ntwo previous vertices.  Two ", " primitives are shown on the right.\nAgain for a ", ", the first three vertices make a triangle, and every vertex\nafter that adds anther triangle, but in this case, the new triangle is made by connecting\nthe new vertex to the previous vertex and to the very first vertex that was specified (vertex\n\"A\"\u00a0in the picture).  Note that ", " can be used for drawing\nfilled-in polygons.  In this picture, by the way, the dots and lines are not part of the\nprimitive; OpenGL would only draw the filled-in, green interiors of the figures.", "The three remaining primitives, which have been removed from modern OpenGL, are\n", ", ", ", and ", ".  The name \"quad\" is short\nfor quadrilateral, that is, a four-sided polygon.  \nA quad is determined by four vertices.  In order for a quad to be ", " correctly in\nOpenGL, all vertices of the quad must lie in the same plane.  The same is true for polygon\nprimitives.  Similarly, to be rendered correctly, quads and polygons must be\n", " (see ", ").  Since OpenGL doesn't check whether\nthese conditions are satisfied, the use of quads and polygons is error-prone. Since the same shapes can easily\nbe produced with the triangle primitives, they are not really necessary, but here for\nthe record are some examples:", "\n", "The vertices for these primitives are specified in the order A, B, C, ....\nNote how the order differs for the two quad primitives:  For ", ", the vertices for\neach individual quad should be specified in counterclockwise order around the quad;\nfor ", ", the vertices should alternate from one side of the strip\nto the other.", "OpenGL has a large collection of functions that can be used to specify colors for\nthe geometry that we draw.  These functions have names of the form ", ",\nwhere the \"*\" stands for a suffix that gives the number and type of the parameters.\nI should warn you now that for realistic 3D graphics, OpenGL has\na more complicated notion of color that uses a different set of functions.  You will\nlearn about that in the ", ", but \nfor now we will stick to ", ".", "For example, the function ", " has three parameters of\ntype ", ".  The parameters give the red, green, and blue components of the color as\nnumbers in the range 0.0 to\u00a01.0.  (In fact, values outside this range are allowed,\neven negative values.  When color values are used in computations, out-of-range values\nwill be used as given.  When a color actually appears on the screen, its component\nvalues are clamped to the range\u00a00\u00a0to\u00a01.  That is, values less than zero\nare changed to zero, and values greater than one are changed to one.)", "You can add a fourth component to the color by using ", "().  The fourth\ncomponent, known as ", ", is not used in the default \ndrawing mode, but it is possible to configure\nOpenGL to use it as the degree of transparency of the color, similarly to the use of the\nalpha component in the 2D graphics APIs that we have looked at.  You need two commands\nto turn on transparency:", "The first command enables use of the alpha component.  It can be disabled by\ncalling ", "(", "). When the ", " option is\ndisabled, alpha is simply ignored. \nThe second command tells how the alpha component of a\ncolor will be used.  The parameters shown here are the\nmost common; they implement transparency in the usual way.  I should note that \nwhile transparency works fine in 2D, it is much more difficult to use transparency correctly\nin 3D.", "If you would like to use integer color values in the range 0 to 255, you can use\n", "() or ", " to set the color.  In these function names,\n\"ub\" stands for \"unsigned byte.\"  ", " is an eight-bit\ndata type with values in the range 0 to 255.  Here are some examples of commands for setting \ndrawing colors in OpenGL:", "Using any of these functions sets the value of a \"current color,\" which \nis part of the OpenGL state.  When you\ngenerate a vertex with one of the ", " functions, the current color\nis saved along with the vertex coordinates, as an ", "\nof the vertex.  We will see that vertices can have other kinds of attribute as well\nas color.  One interesting point about OpenGL is that colors are associated with\nindividual vertices, not with complete shapes.  By changing the current color\nbetween calls to ", "() and ", "(), you can get a shape in which\ndifferent vertices have different color attributes.  When you do this, OpenGL\nwill compute the colors of pixels inside the shape by\n", " the colors of the vertices.  (Again, since\nOpenGL is extremely configurable, I have to note that interpolation of colors is just the\ndefault behavior.)  For example, here is a triangle in which the three vertices\nare assigned the colors red, green, and blue:", "\n", "This image is often used as a kind of \"Hello World\" example for\nOpenGL.  The triangle can be drawn with the commands", "Note that when drawing a primitive, \nyou do ", " need to explicitly set a color for each vertex, as was done here.\nIf you want a shape that is all one color, you just have to set the current color\nonce, before drawing the shape (or just after the call to ", "().  For\nexample, we can draw a solid yellow triangle with", "Also remember that the color for a vertex is specified ", " the call to\n", " that generates the vertex.", "Here\nis an interactive demo that draws the basic OpenGL triangle, with different\ncolored vertices.  You can control the colors of the vertices to see how the\ninterpolated colors in the interior of the triangle are affected. This is our\nfirst OpenGL example.  The demo actually uses WebGL, so you can use it as\na test to check whether your web browser supports WebGL.", "\n", "\n", "The sample program ", " draws the\nbasic OpenGL triangle using Java.  The program ", "\ndoes the same using the C programming language.  And ", "\nis a version that uses my JavaScript simulator, which implements just the parts of\nOpenGL 1.1 that are covered in this book.\nAny of those programs could be used to experiment with 2D drawing in OpenGL.\nAnd you can use them to test your OpenGL programming environment.", "A common operation is to clear the drawing area by filling it with some\nbackground color.  It is be possible to do that by drawing a\nbig colored rectangle, but OpenGL has a potentially more efficient way to do it.  \nThe function", "sets up a color to be used for clearing the drawing area.\n(This only sets the color; the color isn't used until you actually give\nthe command to clear the drawing area.)\nThe parameters are floating point values in the range 0 to\u00a01.\nThere are no variants of this function; you must provide all four color\ncomponents, and they must be in the range 0 to 1.\nThe default clear color is all zeros, that is, black with an alpha\ncomponent also equal to zero.  The command to do the actual clearing is:", "The correct term for what I have been calling the drawing\narea is the ", ", where \"buffer\" is a general term\nreferring to a region in memory.  OpenGL uses several buffers in addition to the\ncolor buffer.  We will encounter the \"depth buffer\" in just a moment.\nThe ", " command can be used to clear several different\nbuffers at the same time, which can be more efficient than clearing\nthem separately since the clearing can be done in parallel.\nThe parameter to ", " tells it which buffer or buffers to clear.\nTo clear several buffers at once, combine the\nconstants that represent them with an arithmetic OR operation.  For example,", "This is the form of ", " that is generally used in 3D graphics,\nwhere the depth buffer plays an essential role.  For 2D graphics, the\ndepth buffer is generally not used, and the appropriate parameter for\n", " is just ", ".", "We have see that there are versions of ", " and ", "\nthat take different numbers and types of parameters.  There are also versions\nthat let you place all the data for the command in a single array parameter.\nThe names for such versions end with \"v\".  For example:  ", ",\n", ", ", ", and ", ".  The \"v\"\nactually stands for \"", ",\" meaning essentially a one-dimensional \narray of numbers.  For example, in the function call\n", "(", "), ", " would be an array\ncontaining at least three floating point numbers.", "The existence of array parameters in OpenGL forces some differences between\nOpenGL implementations in different programming languages.  Arrays in Java are\ndifferent from arrays in C, and arrays in JavaScript are different from both.\nLet's look at the situation in C first, since that's the language of the\noriginal OpenGL ", ".", "In C, arrays variables are a sort of variation on pointer variables, and\narrays and pointers can be used interchangeably in many circumstances.\nIn fact, in the C API, array parameters are actually specified as pointers.\nFor example, the parameter for ", " is of type \"pointer to float.\"\nThe actual parameter in a call to ", " can be an array variable,\nbut it can also be any pointer that points to the beginning of\na sequence of three floats.  As an example, suppose that we want to draw\na square.  We need two coordinates for each vertex of the square.\nIn C, we can put all 8 coordinates into one array and use ", "\nto pull out the coordinates that we need:", "This example uses \"pointer arithmetic,\" in which ", " represents\na pointer to the N-th element of the array.  An alternative notation would be &", "[", "],\nwhere \"&\" is the address operator, and &", "[", "] means \n\"a pointer to ", "[", "]\".\nThis will all seem very alien to people who are only familiar with Java or JavaScript.\nIn my examples, I will avoid using pointer arithmetic, but I will occasionally use address operators.", "As for Java, the people who designed ", " wanted to preserve\nthe ability to pull data out of the middle of an array.  However, it's not possible to work with pointers\nin Java.  The solution was to replace a pointer parameter in the C API with a pair\nof parameters in the JOGL API\u2014one parameter to specify the array that contains the data\nand one to specify the starting index of the data in the array.  For example, here is how\nthe square-drawing code translates into Java:", "Really not much difference in the parameters, although the zero in the first ", " is a little\nannoying.  The main difference is the prefixes \"gl2\" and \"GL2\", which are required by the object-oriented\nnature of the JOGL API.  I won't say more about JOGL here, but if you need to translate my\nexamples into JOGL, you should keep in mind the extra parameter that is required when working\nwith arrays.", "For the record, here are the ", " and ", " functions that I will use\nin this book.  This is not the complete set that is available in OpenGL:", "For ", ", keep in mind that the \"ub\" variations require integers in the range\n0 to 255, while the \"f\" and \"d\" variations require floating-point numbers in the range\n0.0 to\u00a01.0.", "An obvious point about viewing in 3D is that one object can be behind\nanother object.  When this happens, the back object is hidden from the viewer\nby the front object.  When we create an image of a 3D world, we have to\nmake sure that objects that are supposed to be hidden behind other objects\nare in fact not visible in the image.  This is the \n", ".", "The solution might seem simple enough:  Just draw the objects in order from\nback to front.  If one object is behind another, the back object will be covered\nup later when the front object is drawn.  This is called the\n", ".  It's essentially what you\nare used to doing in 2D.  Unfortunately, it's not so easy to implement.\nFirst of all, you can have objects that intersect, so that part of each\nobject is hidden by the other.  Whatever order you draw the objects in,\nthere will be some points where the wrong object is visible. To fix this, you would have to cut\nthe objects into pieces, along the intersection, and treat the pieces as\nas separate objects.  In fact, there can be problems even if there\nare no intersecting objects:  It's possible to have three non-intersecting\nobjects where the first object hides part of the second, the second hides\npart of the third, and the third hides part of the first. The painter's\nalgorithm will fail regardless of the order in which the three objects are drawn.\nThe solution again is to cut the objects into pieces, but now it's not\nso obvious where to cut.", "Even though these problems can be solved, there is another issue.\nThe correct drawing order can change when the point of view is changed or when a geometric\ntransformation is applied, which means that the correct drawing order has to be\nrecomputed every time that happens.  In an animation, that would mean for every\nframe.", "So, OpenGL does not use the painter's algorithm.  Instead, it uses a technique\ncalled the ", ".  The depth test solves the\nhidden surface problem no matter what order the objects are drawn in, so you can\ndraw them in any order you want!  The term \"depth\" here has to do with the\ndistance from the viewer to the object.  Objects at greater depth are farther\nfrom the viewer.  An object with smaller depth will hide an object with greater\ndepth.  To implement the depth test algorithm, OpenGL stores\na depth value for each pixel in the image.  The extra memory that is used\nto store these depth values  makes up the\n", " that I mentioned earlier.\nDuring the drawing process, the depth buffer is used to keep track of what is currently\nvisible at each pixel.  When a second object is drawn at that pixel, the\ninformation in the depth buffer can be used to decide whether the new object\nis in front of or behind the object that is currently visible there.  If the\nnew object is in front, then the color of the pixel is changed to show the\nnew object, and the depth buffer is also updated.  If the new object is\nbehind the current object, then the data for the new object is discarded and\nthe color and depth buffers are left unchanged.", "By default, the depth test is ", " turned on, which can lead to very\nbad results when drawing in 3D.  You can enable the depth test by calling", "It can be turned off by calling ", "(", ").\nIf you forget to enable the depth test when drawing in 3D, the image that\nyou get will likely be confusing and will make no sense physically.\nYou can also get quite a mess if you forget to clear the depth buffer,\nusing the ", " command shown earlier in this section, \nat the same time that you clear the color buffer.", "Here is a demo \nthat lets you experiment with the depth test.  It also lets\nyou see what happens when part of your geometry extends outside the visible range\nof ", "-values.", "\n", "\n", "Here are are a few details about the implementation of the depth test:\nFor each pixel, the depth buffer stores a\nrepresentation of the distance from the viewer to the point that is currently\nvisible at that pixel.  This value is essentially the ", "-coordinate\nof the point, after any transformations have been applied.  (In fact, the\ndepth buffer is often called the \"z-buffer\".)\nThe range of possible ", "-coordinates is scaled to the range 0 to\u00a01.\nThe fact that there is only a limited range of depth buffer values means\nthat OpenGL can only display objects in a limited range of distances from\nthe viewer.  A depth value of 0 corresponds to the minimal distance;\na depth value of 1 corresponds to the maximal distance.  When you clear\nthe depth buffer, every depth value is set to 1, which can be thought\nof as representing the background of the image.", "You get to choose the range of ", "-values that is visible in the image, \nby the transformations that you apply.  The default range, in the absence of any\ntransformations, is -1\u00a0to\u00a01.  Points with ", "-values outside the\nrange are not visible in the image.  It is a common problem to use too small\na range of ", "-values, so that objects are missing from the scene, or \nhave their fronts or backs cut off, because they lie outside of the visible range.\nYou might be tempted to use a huge range, to make sure that the\nobjects that you want to include in the image are included within the range.\nHowever, that's not a good idea:  The depth buffer has a limited number of bits\nper pixel and therefore a limited amount of accuracy.  The larger the range of\nvalues that it must represent, the harder it is to distinguish between objects\nthat are almost at the same depth.  (Think about what would happen if all objects\nin your scene have depth values between 0.499999 and 0.500001\u2014the depth buffer\nmight see them all as being at exactly the same depth!)", "There is another issue with the depth buffer algorithm.  It \ncan give some strange results when two objects\nhave exactly the same depth value.  Logically, it's not even\nclear which object should be visible, but the real problem with the depth\ntest is that it might show one object at some points and the second object\nat some other points.  This is possible because numerical calculations are\nnot perfectly accurate.  Here an actual example:", "\n", "In the two pictures shown here, a gray square was drawn, followed\nby a white square, followed by a black square.  The squares all lie in the same\nplane.  A very small rotation was applied, to force the computer do some calculations\nbefore drawing the objects.\nThe picture on the left was drawn with the depth test disabled, so that, for example,\nwhen a pixel of the white square was drawn, the computer didn't try to figure out whether it\nlies in front of or behind the gray square; it simply colored the pixel white.\nOn the right, the depth test was enabled, and you can see the strange result.", "Finally, by the way, note that the discussion here assumes that there are no transparent\nobjects.  Unfortunately, the depth test does not handle transparency correctly, since\ntransparency means that two or more objects can contribute to the color of the pixel,\nbut the depth test assumes that the pixel color is the color of the object nearest\nto the viewer at that point.  To handle 3D transparency correctly in OpenGL, you pretty much have\nto resort to implementing the painter's algorithm by hand, at least for the transparent\nobjects in the scene."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.1}, {"section_title": "Polygonal Meshes and glDrawArrays", "chapter_id": "Chapter 3", "section_id": "Section 3.4", "content": ["We have drawn only very simple shapes with OpenGL.  In this section,\nwe look at how more complex shapes can be represented in a way that\nis convenient for rendering in OpenGL, and we introduce a new,\nmore efficient way to draw OpenGL primitives.", "OpenGL can only directly render points, lines, and polygons.  \n(In fact, in modern OpenGL, the only polygons that are used are triangles.)\nA ", " can be represented exactly, since a polyhedron\nhas faces that are polygons.  On the other hand, if only polygons are available, then \na curved surface, such as the surface of a sphere, can only be approximated.\nA polyhedron can be represented, or a curved surface can be approximated,\nas a ", ",\nthat is, a set of polygons that are connected along their edges.  If the\npolygons are small, the approximation can look like a curved surface.  (We will\nsee in the ", " how lighting effects\ncan be used to make a polygonal mesh look more like a curved surface and\nless like a polyhedron.)", "So, our problem is to represent a set of polygons\u2014most often a set\nof triangles.  We start by defining a convenient way to represent such a\nset as a data structure.", "The polygons in a polygonal mesh are also referred to as\n\"faces\" (as in the faces of a polyhedron), and one of the\nprimary means for representing a polygonal mesh is as\nan ", ", or IFS.\n", "The data for an IFS includes a list of all the\nvertices that appear in the mesh, giving the coordinates\nof each vertex.  A vertex can then be identified by\nan integer that specifies its index, or position, in the list.\nAs an example, consider this \"house,\" a polyhedron with\n10 vertices and 9 faces:\n", "\n", "The vertex list for this polyhedron has the form", "The order of the vertices is completely arbitrary.  The purpose is simply\nto allow each vertex to be identified by an integer.", "To describe one of the polygonal faces of a mesh, we just have to list its vertices, \nin order going around the polygon. For an IFS, we\ncan specify a vertex by giving its index in the list.  For example,\nwe can say that one of the triangular faces of the pyramid is the polygon\nformed by vertex #3, vertex #2, and vertex #4.  So, we can complete our\ndata for the mesh by giving a list of vertex indices for each face.\nHere is the face data for the house.  Remember that the numbers in parenthese\nare indices into the vertex list:", "Again, the order in which the faces are listed in arbitrary.\nThere is also some freedom in how the vertices for a face are listed.\nYou can start with any vertex.  Once you've picked a starting vertex,\nthere are two possible orderings, corresponding to the two possible\ndirections in which you can go around the circumference of the polygon.\nFor example, starting with vertex 0, the first face in the list could\nbe specified either as (0,1,2,3) or as (0,3,2,1).  However, the\nfirst possibility is the right one in this case, for the following reason.\nA polygon in 3D can be viewed from either side; we can think of it\nas having two faces, facing in opposite directions.  It turns out that\nit is often convenient to consider one of those faces to be the\n\"front face\" of the polygon and one to be the \"back face.\"\nFor a polyhedron like the house, the front face is the one that faces\nthe outside of the polyhedron.  The usual rule is that the\nvertices of a polygon should be listed in counter-clockwise order\nwhen looking at the front face of the polygon.  When looking at the\nback face, the vertices will be listed in clockwise order.  This is\nthe default rule used by OpenGL.", "\n", "The vertex and face data for an indexed face set can be represented as\na pair of two-dimensional arrays.  For the house, in a version for Java, we could use", "In most cases, there will be additional data for the IFS.  For example, if we\nwant to color the faces of the polyhedron, with a different color for each face,\nthen we could add another array, ", ", to hold the color data.\nEach element of ", " would be an array of three ", "\nvalues in the range 0.0 to 1.0, giving the ", " components for\none of the faces.  With this setup, we could use the following code to draw\nthe polyhedron, using Java and ", ":", "Note that every vertex index is used three or four times in the face data.\nWith the IFS representation, a vertex is represented in the face list by a single\ninteger.  This representation uses\nless memory space than the alternative, which would be to write out\nthe vertex in full each time it occurs in the face data.  For the house example,\nthe IFS representation uses 64 numbers to represent the vertices and faces of the polygonal mesh,\nas opposed to 102 numbers for the alternative representation.", "Indexed face sets have another advantage.  Suppose that we want to modify the shape of\nthe polygon mesh by moving its vertices.  We might do this in each frame of\nan animation, as a way of \"morphing\" the shape from one form to another.\nSince only the positions of the vertices are changing, and not the way\nthat they are connected together, it will only be necessary to update the\n30 numbers in the vertex list.  The values in the face list will remain\nunchanged.", "There are other ways to store the data for an IFS.\nIn C, for example, where two-dimensional arrays are more problematic, we might use one\ndimensional arrays for the data.  In that case, we would store all the vertex coordinates\nin a single array.  The length of the vertex array would\nbe three times the number of vertices, and the data for vertex number ", " will\nbegin at index 3*", " in the array.  For the face list, we have to deal with the\nfact that not all faces have the same number of vertices.  A common solution is to\nadd a -1 to the array after the data for each face.  In C, where it is not possible\nto determine the length of an array, we also need variables to store the number of\nvertices and the number of faces.  Using this representation,\nthe data for the house becomes:", "After adding a ", " array to hold color data for the faces,\nwe can use the following C code to draw the house:", "Note the use of the C address operator, &.  For example,\n", " is a pointer to element number ", " in\nthe ", " array.  That element is the first of the three color\ncomponent values for face number\u00a0", ".  This matches the parameter\ntype for ", " in C, since the parameter is a pointer type.", "We could easily draw the edges of the polyhedron instead of the\nfaces simply by using ", " instead of ", "\nin the drawing code (and probably leaving out the color changes).\nAn interesting issue comes up if we want to draw both the faces and\nthe edges.  This can be a nice effect, but we run into a problem with\nthe depth test:  Pixels along the edges lie at the same depth as\npixels on the faces.  As discussed in ", ", the depth\ntest cannot handle this situation well.  However, OpenGL has a solution:\na feature called \"polygon offset.\"  This feature can \nadjust the depth, in clip coordinates, of a polygon, in order to avoid having two\nobjects exactly at the same depth.  To apply polygon offset, you need to\nset the amount of offset by calling", "The second parameter gives the amount of offset, in units determined by the first parameter.\nThe meaning of the first parameter is somewhat obscure; a value of 1 seems to work\nin all cases.  You also have to enable the ", " feature while\ndrawing the faces.  An outline for the procedure is\n", "There is a sample program that can draw the house and a number of other\npolyhedra.  It uses drawing code very similar to what we have looked at here,\nincluding polygon offset.  The program is also an example of using the\ncamera and trackball API that was discussed in ", ",\nso that the user can rotate a polyhedron by dragging it with the mouse.\nThe program has menus that allow the user to turn rendering of edges \nand faces on and off, plus some other options.  The Java version of the\nprogram is ", ", and the C version is\n", ".  To get at the menu in the C version,\nright-click on the display.  The data for the polyhedra is\ncreated in ", " and \n", ".  And here is live demo version of the program\nfor you to try:", "\n", "\n", "All of the OpenGL commands that we have seen so far were part of the original\nOpenGL\u00a01.0.  OpenGL\u00a01.1 added some features to increase performance.  One\ncomplaint about the original OpenGL was the large number of function calls needed to\ndraw a ", " \nusing functions such as ", " and ", " with\n", ".  To address this issue, OpenGL\u00a01.1 introduced the\nfunctions ", " and ", ".  We will look at ", "\nfirst.  There are some differences between the C and the Java versions of the API.\nWe consider the C version first and will deal with the changes necessary for the\nJava version in the next subsection.", "When using ", ",\nall of the data needed to draw a primitive, including vertex coordinates, colors,\nand other vertex ", ", can be packed into\narrays.  Once that is done, the primitive can be drawn with a single call to\n", ".  Recall that a primitive such\nas a ", " or a ", " can include a large number of\nvertices, so that the reduction in the number of function calls can be \nsubstantial.", "To use ", ", you must store all of the vertex coordinates\nfor a primitive in a single one-dimensional array.  You can use an array of\n", ", ", ", or ", ", and you can have 2, 3, or\n4 coordinates for each vertex.  The data in the array are the same numbers\nthat you would pass as parameters to a function such as ", ",\nin the same order.  You need to tell OpenGL where to find the data by\ncalling", "The ", " parameter is the number of coordinates per vertex.  \n(You have to provide the same number of coordinates\nfor each vertex.)   The ", " is a constant that tells the data\ntype of each of the numbers in the array.  The possible values are\n", ", ", ", and ", ".  This parameter is\nnecessary because the array can contain different types of data.\nThe constant that you provide here must match the type of the array.\nThe ", " is usually 0, meaning that the data\nvalues are stored in consecutive locations in the array; if that is\nnot the case, then ", " gives the distance ", " between\nthe location of the data for one vertex and location for the next vertex.\n(This would allow you to store other data, along with the vertex coordinates,\nin the same array.) The final parameter is the array that contains the data.\nIt is listed as being of type \"", "\", which is a C data type for a\npointer that can point to any type of data.  (Recall that an array variable\nin C is a kind of pointer, so you can just pass an array variable as the\nfourth parameter.)  For example, suppose that we want to draw a square in \nthe ", "-plane.  We can set up the vertex array with", "In addition to setting the location of the vertex coordinates, you have to enable\nuse of the array by calling", "OpenGL ignores the vertex pointer except when this state is enabled.  You can use\n", " to disable use of the vertex array.  Finally,\nin order to actually draw the primitive, you would call the function", "This function call corresponds to one use of glBegin/glEnd.\nThe ", " tells which primitive type is being drawn,\nsuch as ", " or ", ".\nThe same ten primitive types that can be used with ", " can\nbe used here.  The parameter ", " is the number of the first\nvertex that is to used for drawing the primitive.  Note that the position\nis given in terms of vertex number; the corresponding array index would\nbe the vertex number times the number of coordinates per vertex, which\nwas set in the call to ", ".\nThe ", " parameter is the number of vertices to be used,\njust as if ", " were called ", " times.\nOften, ", " will be zero, and ", "\nwill be the total number of vertices in the array.  The command for\ndrawing the square in our example would be", "Often there is other data associated with each vertex in addition to\nthe vertex coordinates.  For example, you might want to specify a different\ncolor for each vertex.  The colors for the vertices can be put into\nanother array.  You have to specify the location of the data by calling", "which works just like ", ".  And you need to enable\nthe color array by calling", "With this setup, when you call ", ", OpenGL\nwill pull a color from the color array for each vertex at the\nsame time that it pulls the vertex coordinates from the vertex array.\nLater, we will encounter other kinds of vertex data besides coordinates\nand color that can be dealt with in much the same way.", "Let's put this together to draw the standard OpenGL red/green/blue triangle,\nwhich we drew using ", " in ", ".\nSince the vertices of the triangle have different colors, we will use a color array\nin addition to the vertex array.", "In practice, not all of this code has to be in the same place.  The function\nthat does the actual drawing, ", ", must be in the display routine\nthat draws the image.  The rest could be in the display routine, but could also\nbe done, for example, in an initialization routine.", "The function ", " is similar to ", ", but it is\ndesigned for use with data in a format similar to an indexed face set.\nWith ", ", OpenGL pulls data from the enabled arrays in order,\nvertex 0, then vertex 1, then vertex 2, and so on.  With ", ",\nyou provide a list of vertex numbers.  OpenGL will go through the list of\nvertex numbers, pulling data for the specified vertices from the arrays.\nThe advantage of this comes, as with indexed face sets, from the fact that the\nsame vertex can be reused several times.", "To use ", " to draw a primitive, you need an array to store the vertex numbers.\nThe numbers in the array can be 8, 16, or 32 bit integers.  (They are supposed to be unsigned\nintegers, but arrays of regular positive integers will also work.)  You also need\narrays to store the vertex coordinates and other vertex data, and you must enable \nthose arrays in the same way as for ", ", using functions such as\n", " and ", ".  To actually draw the primitive,\ncall the functions", "Here, ", " is one of the ten primitive types such as ", ",\n", " is the number of vertices to be drawn, ", " specifies the\ntype of data in the array, and ", " is the array that holds the list of\nvertex numbers.  The ", " must be given as one of the constants\n", ", ", ", or ", " to specify\n8, 16, or 32 bit integers respectively.", "As an example, we can draw a cube.  We can draw all six faces of the cube as one\nprimitive of type ", ".  We need the vertex coordinates in one array\nand the vertex numbers for the faces in another array.  I will also use a color\narray for vertex colors.  The vertex colors will be interpolated to pixels on the\nfaces, just like the red/green/blue triangle.\nHere is code that could be used to draw the cube.\nAgain, this would not necessarily be all in the same part of a program:", "Note that the second parameter is the number of vertices, not the number of quads.", "The sample program ", " uses this code\nto draw a cube.  It draws a second cube using ", ".  The Java\nversion is ", ", but you need to \nread the next subsection before you can understand it.  There is also a\nJavaScript version, ", ".", "Ordinary Java arrays are not suitable for use with ", " and\n", ", partly because of the format in which data is stored in them and\npartly because of inefficiency in transfer of data between Java arrays and the ", ".\nThese problems are solved by using ", ".\nThe term \"nio\" here refers to the package ", ", which contains classes for\ninput/output.  A \"buffer\" in this case is an object of the class ", "\nor one of its subclasses, such as ", " or ", ".\nFinally, \"direct\" means that the buffer is optimized for direct transfer of data between memory\nand other devices such as the GPU.  Like an array, an nio buffer is a numbered sequence of\nelements, all of the same type. A ", ", for example, contains\na numbered sequence of values of type ", ".  There are subclasses of\n", " for all of Java's primitive data types except ", ".", "Nio buffers are used in JOGL in several places where\narrays are used in the C API.  For example, JOGL has the following\n", " method in the ", " class:", "Only the last parameter differs from the C version.  The buffer can be of type\n", ", ", ",\nor ", ".  The type of buffer must match the ", "\nparameter in the method.   Functions such as ", " work the same way,\nand ", " takes the form", "where the ", " can be of type ", ",\n", ", or ", "\nto match the ", " ", ", ", ",\nor ", ".", "The class ", " contains static\nutility methods for working with direct nio buffers.  The easiest to use are\nmethods that create a buffer from a Java array.  For example, the method\n", "(", ") takes a ", "\narray as its parameter and creates a ", " of the\nsame length and containing the same data as the array.  These methods are\nused to create the buffers in the sample program ", ".\nFor example,", "The buffers can then be used when drawing the cube:", "There are also methods such as ", "(", "),\nwhich creates a ", " of length ", ".  Remember that\nan nio ", ", like an array, is simply a linear sequence of elements of a given type.\nIn fact, just as for an array, it is possible to refer to items in a buffer by\ntheir index or position in that sequence.  Suppose that ", " is a variable\nof type ", ", ", " is an ", " and\n", " is a ", ".  Then", "copies the value of ", " into position number ", " in the buffer.\nSimilarly, ", "(", ") can be used to retrieve the value at\nindex ", " in the buffer.  These methods make it possible to work with buffers\nin much the same way that you can work with arrays.", "All of the OpenGL drawing commands that we have considered so far have an unfortunate\ninefficiency when the same object is going be drawn more than once:  The commands and\ndata for drawing that object must be transmitted to the GPU each time the object is\ndrawn.  It should be possible to store information on the GPU, so that it can be reused\nwithout retransmitting it.  We will look at two techniques for doing this:\n", " and \n", " (VBOs).  Display lists\nwere part of the original OpenGL 1.0, but they are not part of the modern OpenGL API.\nVBOs were introduced in OpenGL 1.5 and are still important in modern OpenGL; we will discuss\nthem only briefly here and will consider them more fully when we get to ", ".", "Display lists are useful when the same sequence of OpenGL commands will be used\nseveral times.  A display list is a list of graphics commands and the data used by those commands.\nA display list can be stored in a GPU.\nThe contents of the display list only have to be transmitted once\nto the GPU.  Once a list has been created, it can be \"called.\"\nThe key point is that calling a list requires only one OpenGL\ncommand.  Although the same list of commands still has to be executed, only one\ncommand has to be transmitted from the CPU to the graphics card, and then the full power\nof hardware acceleration can be used to execute the commands at the highest possible\nspeed.", "Note that calling a display list twice can result in two different effects, since the effect\ncan depend on the OpenGL state at the time the display list is called.  For\nexample, a display list that generates the geometry for a sphere can draw\nspheres in different locations, as long as different modeling transforms are\nin effect each time the list is called.  The list can also produce spheres\nof different colors, as long as the drawing color is changed between calls to the\nlist.", "If you want to use a display list,\nyou first have to ask for an integer that will identify that list to the GPU.\nThis is done with a command such as", "The return value is an ", " which will be the identifier for the list.\nThe parameter to ", " is also an ", ", which is usually\u00a01.  \n(You can actually ask for\nseveral list IDs at once; the parameter tells how many you want.\nThe list IDs will be consecutive integers, so that if\n", " is the return value from ", "(3), then the identifiers for\nthe three lists will be ", ", ", "\u00a0+\u00a01, and ", "\u00a0+\u00a02.)", "Once you've allocated a list in this way, you can store commands into it.  If ", "\nis the ID for the list, you would do this with code of the form:", "The parameter ", " means that you only want to store commands into the list,\nnot execute them.  If you use the alternative parameter ", ",\nthen the commands will be executed immediately as well as stored in the list for later reuse.", "Once you have created a display list in this way, you can call the list with the command", "The effect of this command is to tell the GPU to execute a list that it\nhas already stored.  You can tell the graphics card that a list is no longer\nneeded by calling", "The second parameter in this method call plays the same role as the parameter in\n", "; that is, it allows you delete several sequentially numbered lists.\nDeleting a list when you are through with it allows the GPU to reuse the \nmemory that was used by that list.", "Vertex buffer objects take a different approach to reusing information.\nThey only store data, not commands.  A VBO is similar to an array.  In fact, it\nis essentially an array that can be stored on the GPU for efficiency of reuse. \nThere are OpenGL commands to\ncreate and delete VBOs and to transfer data from an array on the CPU side\ninto a VBO on the GPU.  You can configure ", "() and ", "()\nto take the data from a VBO instead of from an ordinary array (in C) or from an nio Buffer (in JOGL).\nThis means that you can send the data once to the GPU and use it any number of times.", "I will not discuss how to use VBOs here, since it was not a part of OpenGL 1.1.\nHowever, there is a sample program that lets you compare different techniques for\nrendering a complex image.  The C version of the program is\n", ", and the Java version is\n", ".  The program draws 1331 spheres,\narranged in an 11-by-11-by-11 cube.  The spheres are different colors, with the amount\nof red in the color varying along one axis, the amount of green along a second axis,\nand the amount of blue along the third.  Each sphere has 66 vertices, whose coordinates can\nbe computed using the math functions ", " and ", ".  The program allows\nyou to select from five different rendering methods, and it shows the time that\nit takes to render the spheres using the selected method.  (The Java version has a drop-down\nmenu for selecting the method; in the C version, right-click the image to get the menu.\nYou can use your mouse to rotate the cube of spheres, both to get a better view and to generate more\ndata for computing the average render time.)  The five rendering techniques are:", " In my own experiments, I found, as expected, that display \nlists and VBOs gave the shortest rendering times, with little difference between the two.\nThere were some interesting differences between the results for the C version and the\nresults for the Java version, which seem to be due to the fact that function calls in C\nare more efficient than method calls in Java.  You should try the program on your own computer, \nand compare the rendering times for the various rendering methods."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.4}, {"section_title": "Projection and Viewing", "chapter_id": "Chapter 3", "section_id": "Section 3.3", "content": ["In the ", ", we looked at the\nmodeling transformation, which transforms from object coordinates to world\ncoordinates.  However, when working with OpenGL 1.1, you need to know about\nseveral other coordinate systems and the transforms between them.  We discuss\nthem in this section.", "We start with an overview of the various coordinate systems.  Some of\nthis is review, and some of it is new.", "The coordinates that you actually use for drawing an object are called\n", ".  The object coordinate system is chosen to be\nconvenient for the object that is being drawn.   A ", "\ncan then be applied to set the size, orientation, and position of the object\nin the overall scene (or, in the case of hierarchical modeling, in the\nobject coordinate system of a larger, more complex object).  The modeling transformation\nis the first that is applied to the vertices of an object.", "The coordinates in which you build the complete scene are called\n", ".  These are the coordinates for\nthe overall scene, the imaginary 3D world that you are creating.\nThe modeling transformation maps from object coordinates to world coordinates.\n", "In the real world, what you see depends on where you are standing\nand the direction in which you are looking.  That is, you can't make a picture\nof the scene until you know the position of the \"viewer\" and where the\nviewer is looking\u2014and, if you think about it, how the viewer's head\nis tilted.  For the purposes of OpenGL, we imagine that the\nviewer is attached to their own individual coordinate system, which is known\nas ", ".\nIn this coordinate system, the viewer is at the origin, (0,0,0), looking\nin the direction of the negative ", "-axis, the positive direction of\nthe ", "-axis is pointing straight up, and the  ", "-axis is pointing\nto the right.  This is a viewer-centric coordinate\nsystem.   In other words, eye coordinates are (almost) the coordinates\nthat you actually want to ", " for drawing on the screen.\nThe transform from world coordinates to eye coordinates is called the \n", ". ", "If this is confusing, think of it this way: We are free to use any\ncoordinate system that we want on the world.  Eye coordinates are the\nnatural coordinate system for making a picture of the world as seen by\na viewer.  If we used a different coordinate system (world coordinates) when building the world,\nthen we have to transform those coordinates to eye coordinates to find\nout what the viewer actually sees.  That transformation is the viewing transform.\n", "Note, by the way, that OpenGL doesn't keep track of separate modeling\nand viewing transforms.  They are combined into a single transform, which is known as the\n", ".  In fact, even though world coordinates\nmight seem to be the most important and natural coordinate system, OpenGL doesn't have any\nrepresentation for them and desn't use them internally.  For OpenGL, only\nobject and eye coordinates have meaning.  OpenGL goes directly from object coordinates to\neye coordinates by applying the modelview transformation.", "We are not done.  The viewer can't see the entire 3D world,\nonly the part that fits into the ", ", which is the\nrectangular region of the screen or other display device where the\nimage will be drawn.  We say that the scene is\n\"clipped\" by the edges of the viewport.  Furthermore, in OpenGL,\nthe viewer can see only a limited range of ", "-values in the eye coordinate system.  \nPoints with larger\nor smaller ", "-values are clipped away and are not rendered into the image.\n(This is not, of course, the way that viewing works in the real world, but it's\nrequired by the use of the ", " in OpenGL.  See ", ".)\nThe volume of space that\nis actually rendered into the image is called the ", ".\nThings inside the view volume make it into the image; things that are not\nin the view volume are clipped and cannot be seen.  For purposes of drawing,\nOpenGL applies a coordinate transform that maps the view volume\nonto a ", ".  The cube is centered at the origin and extends from -1 to\n1 in the x-direction, in the y-direction, and in the z-direction.  The coordinate\nsystem on this cube is referred to as ", ".\nThe transformation from eye coordinates to clip coordinates is\ncalled the ", ".  At this point, we\nhaven't quite projected the 3D scene onto a 2D surface, but we can now do\nso simply by discarding the z-coordinate.  (The z-coordinate, however, is still\nneeded to provide the depth information that is needed for the depth test.)\n", "We ", " aren't done.  In the end, when things are actually drawn, there are\n", ", the 2D coordinate system in which the\nactual drawing takes place on a physical display device such as the computer screen.\nOrdinarily, in  device coordinates, the pixel is the unit of measure.\nThe drawing region is a rectangle of pixels.  This is the rectangle that is called\nthe viewport.  The ", "\ntakes x and y from the clip coordinates and scales them to fit\nthe viewport.", "Let's go through the sequence of transformations one more time.  Think of a \n", ", such\nas a line or triangle, that is part of the scene and that might appear in the image\nthat we want to make of the scene.  The primitive goes through the following sequence of\noperations:\n", "\n", "We need to consider these transforms in more detail and see how to use them in\nOpenGL\u00a01.1.", "The simplest of the transforms is the viewport transform.  It transforms\n", " and ", " clip coordinates to the coordinates that are used on\nthe display device.  To specify the viewport transform, it is only necessary\nto specify the rectangle on the device where the scene will be rendered.\nThis is done using the ", " function.", "OpenGL must be provided with a drawing surface by the environment in which\nit is running, such as ", " for Java or the ", " library for C.  \nThat drawing surface is a rectangular grid of pixels, with a horizontal size\nand a vertical size.  OpenGL uses a coordinate system on the drawing surface\nthat puts (0,0) at the lower left, with y increasing from bottom to top and\nx increasing from left to right.  When the drawing surface is first given to\nOpenGL, the viewport is set to be the entire drawing surface.  However,\nit is possible for OpenGL to draw to a different rectangle by calling", "where (", ") is the lower left corner of the viewport, in the\ndrawing surface coordinate system, and ", " and ", "\nare the size of the viewport.  Clip coordinates from -1 to 1 will then\nbe mapped to the specified viewport.  Note that this means in particular\nthat drawing is limited to the viewport.  It is not an error for the\nviewport to extend outside of the drawing surface, though it would be\nunusual to set up that situation deliberately.", "When the size of the drawing surface changes, such as when the user resizes\na window that contains the drawing surface, OpenGL does not automatically\nchange the viewport to match the new size.  However, the environment in which OpenGL\nis running might do that for you.  (See ", " for information \nabout how this is handled by JOGL and GLUT.)", "\n", " is often used to draw several different scenes, or several views\nof the same scene, on the same drawing surface.  Suppose, for example, that\nwe want to draw two scenes, side-by-side, and that the drawing surface is 600-by-400 pixels.\nAn outline for how to do that is very simple:\n", "The first ", " command establishes a 300-by-400 pixel viewport with\nits lower left corner at (0,0).  That is, the lower left corner of the viewport is\nat the lower left corner of the drawing surface.  This viewport fills the left\nhalf of the drawing surface.  Similarly, the second viewport, with its lower left\ncorner at (300,0), fills the right half of the drawing surface.", "We turn next to the projection transformation.  Like any transform, the projection\nis represented in OpenGL as a ", ".  OpenGL keeps track of the \nprojection matrix separately from the matrix that represents the modelview\ntransformation.  The same transform functions, such as ", ", can be applied to both\nmatrices, so OpenGL needs some way to know which matrix those functions apply to.\nThis is determined by an OpenGL state property called the ", ".\nThe value of the matrix mode is a constant such as ", " or ", ".\nWhen a function such as ", " is called, it modifies a matrix; which matrix\nis modified depends on the current value of the matrix mode.\nThe value is set by calling the function ", ".  The initial value is\n", ".  This means that if you want to work on the projection matrix,\nyou must first call", "If you want to go back to working on the modelview matrix, you must call", "In my programs, I generally set the matrix mode to ", ", set up\nthe projection transformation, and and then immediately set the matrix mode\nback to ", ".  This means that anywhere else in the program, I can\nbe sure that the matrix mode is ", ".", "To help you to understand projection, remember that\na 3D image can show only a part of the infinite 3D world.  The view volume\nis the part of the world that is visible in the image.  The view volume\nis determined by a combination of the viewing transformation and the projection\ntransformation.  The viewing transform determines where the viewer is located and\nwhat direction the viewer is facing,  but it doesn't say how much of the world the \nviewer can see.  The projection transform does that:  It specifies the shape and \nextent of the region that is in view.   Think of the viewer as a camera, with \na big invisible box attached to the front of the camera that encloses\nthe part of the world that that camera has in view.  The inside of the box is the\nview volume.  As the camera moves around in the world, the box moves with it,\nand the view volume changes.  But the shape and size of the box don't change.\nThe shape and size of the box correspond to the projection transform.\nThe position and orientation of the camera correspond to the viewing\ntransform.", "This is all just another way of saying that, mathematically, \nthe OpenGL projection transformation transforms eye coordinates\nto clip coordinates, mapping the view volume onto the 2-by-2-by-2 clipping cube that contains\neverything that will be visible in the image.  To specify a projection just means\nspecifying the size and shape of the view volume, relative to the viewer.", "There are two general types of projection, ", "\nand ", ".  Perspective projection is more\nphysically realistic.  That is, it shows what you would see if the OpenGL display rectangle\non your computer screen were a window into an actual 3D world (one that\ncould extend in front of the screen as well as behind it).  It shows a view that\nyou could get by taking a picture of a 3D world with a camera.  In a perspective\nview, the apparent size of an object depends on how far it is away from the\nviewer.  Only things that are in front of the viewer can be seen.  In fact,\nignoring clipping in the ", "-direction for the moment,\nthe part of the world that is in view is an infinite pyramid, with the\nviewer at the apex of the pyramid, and with the sides of the pyramid\npassing through the sides of the viewport rectangle.", "However, OpenGL can't actually show everything in this pyramid, because of\nits use of the ", " to solve the ", ".  Since the\ndepth buffer can only store a finite range of depth values, it can't\nrepresent the entire range of depth values for the infinite pyramid that \nis theoretically in view.  Only objects in a certain range of distances from \nthe viewer can be part of the image.  That range of distances \nis specified by two values,  ", " and ", ".  \nFor a perspective transformation, both of these values must be positive numbers, and ", " must be greater \nthan ", ".  Anything that is closer to the viewer than the ", " distance\nor farther away than the ", " distance is discarded and does not appear\nin the rendered image.  The volume of space that is represented in the image\nis thus a \"truncated pyramid.\"  This pyramid is the view volume for a perspective\nprojection: \n", "\n", "The view volume is bounded by six planes\u2014the four sides plus the top and bottom\nof the truncated pyramid.  These\nplanes are called clipping planes because anything that lies on the wrong side\nof each plane is clipped away. The projection transformation maps the six sides of the\ntruncated pyramid in eye coordinates to the six sides of the clipping cube in clip coordinates.", "In OpenGL, setting up the projection transformation is equivalent to defining\nthe view volume.  For a perspective transformation, you have to set up a view volume\nthat is a truncated pyramid.  A rather obscure term for this shape is a ", ".\nA perspective transformation can be set up with the ", " command:", "The last two parameters specify the ", " and ", " distances from the\nviewer, as already discussed.  The viewer is assumed to be at the origin, (0,0,0), facing\nin the direction of the negative z-axis.  (This is the eye coordinate system.)  So,\nthe near clipping plane is at ", " = \u2212", ", and the far clipping\nplane is at ", " = \u2212", ".  (Notice the minus signs!)\nThe first four parameters specify the sides of the pyramid: \n ", ", ", ", ", ", and ", " specify the\nhorizontal and vertical limits of the view volume ", ".\nFor example, the coordinates of the upper-left corner of the small end of the\npyramid are (", ",\u00a0", ",\u00a0-", ").  The ", "\nand ", " limits at the far clipping plane are larger, usually much larger, than\nthe limits specified in the ", " command.", "Note that ", " and ", " limits in ", " are usually symmetrical\nabout zero.  That is, ", " is usually equal to the negative of ", " and ", " \nis usually equal to the negative of ", ".  However, this is not required.  It is possible to have\nasymmetrical view volumes where the z-axis does not point directly down the center of the view.", "Since the matrix mode must be set to ", " to work on the projection transformation,\n", " is often used in a code segment of the form", "The call to ", " ensures that the starting point is the ", ".\nThis is important since ", " modifies the existing projection matrix rather than replacing\nit, and although it is theoretically possible, you don't even want to try to think about what would happen if you\ncombine several projection transformations into one.", "Compared to perspective projections, orthographic projections are easier to understand: \nIn an orthographic projection, the 3D world is projected onto\na 2D image by discarding the ", "-coordinate of the eye-coordinate system.  This type of projection\nis unrealistic in that it is not what a viewer would see.  For example, the apparent size of\nan object does not depend on its distance from the viewer.  Objects in back of the viewer\nas well as in front of the viewer can be visible in the image.  \nOrthographic projections are still useful, however, especially in interactive modeling \nprograms where it is useful to see true sizes and angles, undistorted by perspective.", "In fact, it's not really clear what\nit means to say that there is a viewer in the case of orthographic projection.\nNevertheless, for orthographic projection in OpenGL, there is considered to be a viewer.\nThe viewer is located at the eye-coordinate \norigin, facing in the direction of the negative z-axis.  Theoretically, a rectangular corridor\nextending infinitely in both directions, in front of the viewer and in back, would be in view.\nHowever, as with perspective projection, only a finite segment of this infinite corridor can\nactually be shown in an OpenGL image.  This finite view volume is a parallelepiped\u2014a\u00a0rectangular\nsolid\u2014that is cut out of the infinite corridor by a ", " clipping plane and a ", "\nclipping plane.  The value of ", " must be greater than ", ", but for an orthographic\nprojection, the value of ", " is allowed to be negative, putting the \"near\" clipping plane\nbehind the viewer, as shown in the lower section of this illustration: \n", "\n", "Note that a negative value for ", " puts the near clipping plane on the ", "\n", "-axis, which is behind the viewer.", "An orthographic projection can be set up in OpenGL using the ", " method, which is\nhas the following form:\n", "The first four parameters specify the ", "- and ", "-coordinates of the left, right, bottom, and\ntop of the view volume.  Note that the last two parameters are ", " and ", ",\nnot ", " and ", ".  In fact, the minimum z-value for the view volume is\n\u2212", " and the maximum z-value is \u2212", ".  However, it is often\nthe case that ", " = \u2212", ", and if that is true then the minimum and\nmaximum z-values turn out to be ", " and ", " after all!", "As with ", ", ", " should be called when the matrix mode is\n", ".  As an example, suppose that we want the view volume to be the\nbox centered at the origin containing ", ", ", ", and ", " values in\nthe range from -10 to 10.  This can be accomplished with", "Now, as it turns out, the effect of ", " in this simple case is exactly the\nsame as the effect of ", "(0.1,\u00a00.1,\u00a0-0.1), since the projection\njust scales the box down by a factor of 10.  But it's usually better to think of projection\nas a different sort of thing from scaling.  (The minus sign on the ", " scaling\nfactor is there because projection reverses the direction of the ", "-axis, transforming\nthe conventionally right-handed eye coordinate system into OpenGL's left-handed default\ncoordinate system.)", "The ", " method is not particularly easy to use.  There is a library\nknown as ", " that contains some utility functions for use with OpenGL.\nThe GLU library includes the method ", " as an easier way to set up a perspective\nprojection.  The command", "can be used instead of ", ".  The ", " is the vertical\nangle, measured in degrees, between the upper side of the view volume pyramid and the lower side.  \nTypical values are in the range 30 to 60 degrees.  The ", " parameter is the\naspect ratio of the view, that is, the width of a cross-section of the pyramid\ndivided by its height.  The value of ", " should\ngenerally be set to the aspect ratio of the viewport.  The ", " and ", "\nparameters in ", " have the same meaning as for ", ".", "\"Modeling\" and \"viewing\" might seem like very different things, conceptually, but OpenGL\ncombines them into a single transformation.  This is because there is no way to distinguish\nbetween them in principle; the difference is purely conceptual.  That is,\na given transformation can be considered to be either a modeling transformation or a\nviewing transformation, depending on how you think about it.  (One significant difference, conceptually,\nis that the viewing transformation usually replies to an entire scene as a whole, while modeling\ntransformations are applied to individual objects.  But this is not a difference in principle.) \nWe have already seen something\nsimilar in 2D graphics (", "), but let's think about how it works in 3D.", "For example, suppose that there is a model of a house at the origin, facing towards the\ndirection of the positive ", "-axis.  Suppose the viewer is on the positive ", "-axis,\nlooking back towards the origin.  The viewer is looking directly at the front of the\nhouse. Now, you might apply a modeling transformation to the house, to rotate it by\n90 degrees about the ", "-axis.  After this transformation, the house is facing in\nthe positive direction of the ", "-axis, and  the viewer is looking directly\nat the ", " side of the house.  On the other hand, you might rotate the\n", " by ", " 90 degrees about the ", "-axis.  This would put the viewer on\nthe negative ", "-axis, which would give it a view of the ", " side of the house.\nThe net result after either transformation is that the viewer ends up with exactly the\nsame view of the house.  Either transformation can be implemented in OpenGL with the\ncommand\n", "That is, this command represents either a modeling transformation that rotates\nan object by 90 degrees or a viewing transformation that rotates the viewer by -90\ndegrees about the ", "-axis.  Note that the effect on the viewer is the inverse of the effect on the object.\nModeling and viewing transforms are always related in this way.  For example, if you\nare looking at an object, you can move yourself 5 feet to the ", " (viewing transform), or you can move\nthe object 5 feet to the ", " (modeling transform).  In either case, you end up with\nthe same view of the object.  Both transformations would be represented in OpenGL as", "This even works for scaling:  If the viewer ", ", it will look to the\nviewer exactly the same as if the world is expanding, and vice-versa.", "Although modeling and viewing transformations are the same in principle, they remain\nvery different conceptually, and they are typically applied at different points in the code.\nIn general when drawing a scene, you will do the following:  (1)\u00a0Load the identity matrix,\nfor a well-defined starting point; (2)\u00a0apply the viewing transformation; and (3)\u00a0draw\nthe objects in the scene, each with its own modeling transformation.  Remember that OpenGL keeps\ntrack of several transformations, and that this must all be done while the modelview transform\nis current; if you are not sure of that then before step \n(1), you should call ", "(", ").\nDuring step (3), you will probably use ", "() and ", "() to limit\neach modeling transform to a particular object.", "After loading the identity matrix, the viewer is in the default position, at the origin,\nlooking down the negative ", "-axis, with the positive ", "-axis pointing upwards in the\nview.  Suppose, for example, that we would like to move the viewer from its\ndefault location at the origin back along the positive z-axis \nto the point (0,0,20).  This operation has exactly the same effect as moving the world,\nand the objects that it contains, 20 units in the negative direction along the z-axis.  Whichever\noperation is performed, the viewer ends up in exactly the same position relative to the\nobjects.  Both operations are implemented by the same OpenGL\ncommand, ", "(0,0,-20).  For another example,\nsuppose that we use two commands", "to establish the viewing transformation.  As a modeling transform, these commands would\nfirst translate an object 10 units \nin the positive ", "-direction, then rotate the object 90 degrees about the ", "-axis.  (Remember that\nmodeling transformations are applied to objects in the order opposite to their order in the code.)\nWhat do these commands do as a viewing transformation?  The effect on the view is the inverse\nof the effect on objects.  The inverse of \"translate 90 then rotate 10\" is \"rotate -10\nthen translate -90.\"  That is, to do the inverse, you have to undo the rotation ", "\nyou undo the translation.   The effect as a viewing transformation is first to rotate the viewer\nby -90 degrees about the ", "-axis, then to translate the viewer by -10 along the ", "-axis.\n(You should think about how the two interpretations affect the view of an object that starts out\nat the origin.)  Note that the order in which viewing transformations are applied is the\n", " the order in which they occur in the code.", "\nHere is a demo that illustrates the equivalence between modeling and viewing.\nThe translucent gray box in the lower images represents the view volume that is used\nto create the image that is shown in the upper left.  In this case, the projection\nis a perspective projection, and the view volume is a frustum.\nRead the help text in the demo for more information.\n\n", "\n", "\n", "It can be difficult to set up a view by combining rotations, scalings, and translations,\nso OpenGL provides an easier way to set up a typical view.\nThe command is not part of OpenGL itself but is part of the GLU library.", "The GLU library provides the following convenient method for\nsetting up a viewing transformation:", "This method places the viewer at the point (", ",", ",", "),\nlooking towards the point (", ",", ",", ").\nThe viewer is oriented so that the vector (", ",", ",", ")\npoints upwards in the viewer's view.  For example, to position the viewer on the positive\n", "-axis, 10 units from the origin, looking back at the origin, with the positive\ndirection of the ", "-axis pointing up as usual, use", "With all this, we can give an outline for a typical display routine for drawing\nan image of a 3D scene with OpenGL\u00a01.1:", "Projection and viewing are often discussed using the analogy of a ", ".\nA real camera is used to take a picture of a 3D world.  For 3D graphics, it useful to\nimagine using a virtual camera to do the same thing.  Setting up the viewing transformation is like\npositioning and pointing the camera.  The projection transformation determines the properties\nof the camera:  What is its field of view, what sort of lens does it use?  \n(Of course, the analogy breaks for OpenGL in at least one respect,\nsince a real camera doesn't do clipping in its ", "-direction.)", "I have written a camera utility to implement this idea.  The camera is meant to\ntake over the job of setting the projection and view.  Instead of doing that by hand, you set\nproperties of the camera.  The API is available for both C and Java.  The two versions are\nsomewhat different because the Java version is object-oriented.  I will discuss the\nC implementation first. (See ", " for basic information about\nprogramming OpenGL in C and Java.  For an example of using a camera in a program, see\nthe polyhedron viewer example in the ", ".\nNote also that there is a version of the camera for use with my JavaScript simulator for OpenGL;\nit is part of the simulator library ", " and has an API almost identical\nto the Java API.)", "In C, the camera is defined by the sample .c file, ", " and\na corresponding header file, ", ".  Full documentation for the API can\nbe found in the header file.  To use the camera, you should ", " at\nthe start of your program, and when you compile the program, you should include ", " in\nthe list of files that you want to compile.  The camera depends on the GLU library and on\nC's standard math library, so you have to make sure that those libraries are available\nwhen it is compiled.  To use the camera, you should call", "to set up the projection and viewing transform before drawing the scene.\nCalling this function replaces the usual code\ncode for setting up the projection and viewing transformations. It leaves the\nmatrix mode set to ", ".", "The remaining functions in the API are used to configure the camera.  This would usually\nbe done as part of initialization, but it is possible to change the configuration at any time.\nHowever, remember that the settings are not used until you call ", ".\nAvailable functions include:", "In many cases, the default settings are sufficient.  Note in particular how\n", " and ", " work together to set up the view and\nprojection.  The parameters to ", " represent three points in\nworld coordinates.  The view reference point, (", "),\nshould be somewhere in the middle of the scene that you want to render.\nThe parameters to ", " define a box about that view reference\npoint that should contain everything that you want to appear in the image.", "For use with ", " in Java, the camera API is implemented as\na class named ", ", defined in the file\n", ".  The camera\nis meant for use with a ", " or\n", " that is being used as an OpenGL drawing surface.\nTo use a camera, create an object of type ", " as\nan instance variable:", "Before drawing the scene, call", "where ", " is the OpenGL drawing context of type ", ".\n(Note the presence of the parameter ", ", which was not necessary in\u00a0C; it is\nrequired because the OpenGL drawing context in JOGL is implemented as an object.)\nAs in the C version, this sets the viewing and projection transformations and\ncan replace any other code that you would use for that purpose.\nThe functions for configuring the camera are the same in Java as in C,\nexcept that they become methods in the ", " object, and true/false\nparameters are ", " instead of ", ":", "The camera comes with a simulated \"trackball.\"\nThe trackball allows the user to rotate the view by clicking and dragging the mouse\non the display.  To use it with ", " in C, you just need to install a mouse function and\na mouse motion function by calling", "The functions\n", " and ", " are defined\nas part of the camera API and are declared and documented in ", ".\nThe trackball works by modifying the viewing transformation associated with\nthe camera, and it only works if ", "() is called at the\nbeginning of the display function to set the viewing and projection\ntransformations.  To install a trackball for use with a ", "\nobject in JOGL, call", "where ", " is the component on which the camera\nis used."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.3}, {"section_title": "WebGL Extensions", "chapter_id": "Chapter 7", "section_id": "Section 7.5", "content": ["WebGL is designed to run on a wide variety of devices, including mobile devices\nthat have relatively limited graphical capabilities.  Because of this, only a minimal\nset of features is required of all WebGL implementations.  However, WebGL has\na mechanism for activating additional, optional features.  The optional features\nare defined in ", ".\nA web page that requires a WebGL extension is not guaranteed to work in every implementation of\nWebGL.  However, in many cases, it is fairly easy to write a page that can work with or \nwithout the extension, though perhaps with some missing feature when the extension is not available.\nThere are about two dozen extensions whose definitions have been standardized.\nThese standard extensions are documented at\n", ".\n", "Standard OpenGL also has an extension mechanism.  Historically, many features from extensions in\none version of OpenGL have become required features in later versions.  The same is likely to be\ntrue for WebGL extensions.  In fact, some of the WebGL 1.0 extensions have been incorporated as\nrequired features in the upcoming WebGL\u00a02.0.", "This section covers the WebGL extension mechanism, and it discusses a few of the standard\nextensions.", "We start with a simple extension that can improve the appearance of textures in some scenes.\nThe standard filtering methods for ", " an image texture give poor results when the\ntexture is viewed at an oblique angle.  In that case, a pixel on the surface corresponds\nto a trapezoidal region in the texture, and the standard ", "\nand ", " filter rules such as ", " don't\nhandle that case very well.  (Filtering was covered in ", ".)\nA better result can be obtained, at the cost of additional computation, using something called\n", ", which samples the texture taking the trapezoidal shape\ninto account.  Many ", " can do anisotropic filtering.  It is not a required feature\nin WebGL implementations, but it is commonly available as an extension.", "The sample program ", " shows how to use anisotropic\nfiltering in WebGL.  It shows a large plane textured with a brick image that can be viewed from a sharp,\noblique angle.  If the extension is available, then the user can turn anisotropic filtering on and off.\nIf it is not available, the program will still draw the scene, but only using standard filtering.\nHere are two images from the program.  Anisotropic filtering is used in the image on the right.\nOn the left, without anisotropic filtering, the texture is blurred even at moderate distanced from\nthe viewer:", "\n", "Each WebGL extension has a name.  The function ", "(", ") is used to activate an\nextension, where ", " is a string containing the name of the extension.  The return value of the function is ", "\nif the extension is not available, and you should always check the return value before attempting to \nuse the extension.  If the return value is not null, then it is a JavaScript object.  The object might contain,\nfor example, constants that are meant to be passed to WebGL API functions in order to make use of the functionality\nof the extension.", "The name of the anisotropic filtering extension is \"EXT_texture_filter_anisotropic.\"  (Names are\nnot case-sensitive.) To test for\nthe availability of the extension and to activate it, a program can use a statement such as", "If ", " is ", ", then the extension is not available.  If it is not\nnull, then the object has a property named ", " that can be used\nas a parameter to ", " to set the level, or amount, of anisotropic filtering that\nwill be applied to the texture.  For example, after creating and binding a texture, a program might say", "The third parameter is the anisotropic filtering level.  Setting the level to 1 will turn off\nanisotropic filtering.  Higher values give better results.  There is an implementation-dependent\nmaximum level, but asking for a level greater than the maximum is not an error\u2014you will simply get the maximum\nlevel.  To find out the maximum, you can use", "It is recommended to use ", " as the minification filter and\n", " as the magnification filter when using anisotropic filtering.  A texture\nwould typically be configured using code similar to the following:", "If the extension is not available, the texture might not look as good as it might\nhave, but it will still work (and only a very observant user is likely to notice).", "As a second example, we consider a pair of extensions named \"OES_texture_float\"\nand \"WEBGL_color_buffer_float\".  The first of these makes it possible to use textures\nin which color component values are floating-point numbers, instead of eight-bit integers.\nThe second makes it possible to render to such a texture by using it as the color buffer in a\n", ".", "Why would someone want to do this?  Eight-bit integers are fine for representing colors visually,\nbut they don't have enough precision for doing accurate calculations.  \nFor applications that do significant numerical processing with color components, floating-point values are\nessential.", "As an example, consider finding the average color value of an image, which requires\nadding up the color values from a large number of pixels.  This is something that can be\nspeeded up by using the parallel processing power of a GPU.  My technique for doing so uses\ntwo framebuffers, with two textures serving as color buffers.  \nI assume that the image width and height are powers of two.  Start by drawing\nthe image to the first texture.  Think of the image as divided in half, horizontally and vertically,\ngiving four equal-sizes rectangles.  As a first step, compute a half-size image that is the \naverage of those four rectangles.  That is, the color of a pixel in the half-size image\nis the average of the colors of four pixels in the original.\nThe averaged image can be computed by drawing a half-size rectangle\nto the second framebuffer, using multiple samples from the image in the first texture.  Here is \na fragment shader that does the work:", "In this first pass, the square with vertices at (0,0) and (0.5,0.5) is rendered, and\n", " is 0.5.  The drawing is done in a coordinate system in which the square with\nvertices (0,0) and (1,1) covers the entire drawing area.  In that coordinate system, the\nsquare with vertices at (0,0) and (0.5,0.5) covers the lower left quarter of the drawing area.\nThe first sample in the fragment shader comes from that quarter of the texture image, \nand the other three samples come from corresponding points in the other three quarters of the the image.", "In a second pass, the roles of the two framebuffers are swapped, and a square with vertices\nat (0,0) and (0.25,0.25) is drawn, using the same fragment shader with ", " equal\nto 0.25.  Since the framebuffers were swapped, the second pass is sampling the half-sized image \nthat was produced in the first pass.  The result is a quarter-sized image \nthat is the average of four rectangles that cover the half-sized image\u2014and therefore of 16 rectangles\nthat cover the original image.  This can be\nrepeated, with smaller and smaller squares, until the resulting image is small enough that\nits colors can be efficiently read back into the CPU and averaged there.  The result is\na color value that is the average of all the pixels from the original image.  We expect that,\nbecause a lot of the work is done in parallel by the GPU, we can get the answer much faster using\nthis technique than if we had simply done all the computations on the CPU.", "The point here is that for an accurate result, we want the color components to be represented\nas floating point values in the GPU, not as eight-bit integers.", "I use this technique in the sample program ", ".\nIn that program, the problem is to find the average ", " in color between two\nimages.  I start by drawing the two images to two textures. I then\nrender a difference image, in which the color of a pixel is the\nabsolute value of the difference between the colors of the same pixel in the two textures.\nThis is done with another special-purpose shader program.  I then apply the above averaging\nprocess to the difference image.", "The actual point of the sample program is to try to \"evolve\" an approximation to a given image,\nusing a \"genetic algorithm.\" (It was inspired by two students from my Fall, 2015 class,\nFelix Taschbach and Pieter Schaap, who worked on a similar program for their final project, \nthough they didn't use the GPU.)  In the program, the average difference between the original\nimage and an approximation is used as a measure of how good the approximation is.\nI used a very simple grayscale image as the goal, with\napproximations made from triangles.  You don't need to know anything about the genetic algorithm, \nespecially since the program has no practical purpose.  However,\nthe source code is heavily commented if you want to try to understand it.  Here is a screenshot\nfrom one run of the program, showing the original image and the best approximation produced after running\nthe genetic algorithm for several hundred generations:", "\n", "(See also ", ", from generation number 15,000.)", "But what interests us here is how the program uses the WebGL floating-point color extensions.\nThe program attempts to activate the extensions during initialization using the following code:", "The program requires the extensions, so an exception is thrown if they can't be activated.\nThe extension objects, ", " and ", ", dont't have any properties that are needed\nin this program; however, it is still necessary to call ", " to activate\nthe extensions.  (I only test that OES_texture_float is available.  Historically, that extension included the\nfunctionality that I need from WEBGL_color_buffer_float, and a browser that has support for\nOES_texture_float can probably run the program whether or not it says that it supports the\nother extension.  Officially, new programs are supposed to activate both extensions explicitly.)", "The program creates two floating-point textures that are attached to framebuffers for use as\ncolor buffers.  (See ", ".)  Here is the code that creates one of those\ntextures:", "The parameter ", " in the last line specifies that the data type for the color components in the texture\nis ", ".  That data type would be an error if the extensions had not been activated.", "When the GPU does the averaging computation with these textures, it is doing floating-point\ncalculations.  The program computes a series of smaller and smaller averaged images, stopping\nwith a 4-by-4 pixel image.  It then reads the 16 pixel colors back from the texture using the\nfollowing code:", "The call to ", " reads the color data for the 16 pixels into the\narray, ", ".  Again, the ", " parameter specifies the data type, and that parameter\nvalue is legal in ", " only because the extensions have been activated.", "I will discuss one more WebGL extension, one that is useful for an important\nrendering technique called ", ".  I don't have a sample\nprogram for deferred rendering, and I will only discuss it in general terms.", "Deferred shading is used as\nan optimization when rendering complex scenes, and it is often used to speed up rendering\nin video games.  It is most closely associated with lighting, since it can be used to\nrender scenes with large numbers of light sources, but it can also be useful for other effects.", "Recall that the number of lights that can be represented in OpenGL or in a WebGL shader\nis limited.  But scenes with many lights can be rendered using a ", ":\nEach pass computes the contribution of one light, or a small number of lights, and the results\nof the passes are added together to give the complete scene.  The problem is that, if the rendering in each pass\nis done in the normal way, then there are a lot of things that have to be recomputed, in exactly\nthe same way, in each pass.  For example, assuming that ", " is used, that includes computing \n", " properties and a ", " vector for each pixel in the image.  Deferred shading\naims to avoid the duplicated effort.", "In deferred shading, a first pass is used to compute material properties, normal vectors, and whatever other\ndata is needed, for each pixel in the image.  All of that data is saved, to be used in additional passes that will compute\nlighting and possibly other effects.  For a given pixel, only the data for the object that is actually visible at the pixel is\nsaved, since data for hidden surfaces is not needed to render the scene.  The first pass uses the geometry and attributes\nof objects in the scene.  Everything that the later passes need to know about geometry and attributes is\nin the saved data.", "The saved data can be stored in texture objects.  (Floating point textures are ideal for this, since the\ndata will be used in further calculations.)  In this case, the values in the textures don't necessarily\nrepresent images.  For example, the RGB color components in one texture might represent the x, y, and\nz coordinates of a normal vector.  And if a depth value is needed in later passes, it might be stored in\nthe alpha color component of the same texture.  Another texture might hold a diffuse color, while\na third holds a specular color in its RGB components and a shininess value in its alpha component.\nShader programs are free to interpret data in a texture however they like.", "A WebGL shader can write data to a texture, using a framebuffer.  But standard WebGL can only write\nto one framebuffer at a time.  Now, it would be possible to use a separate pass for each texture that\nwe need to compute, but that would involve a lot of redundant calculations, which is what we are\ntrying to avoid.  What we need is a WebGL extension that makes it possible for a shader to write\nto several framebuffers simultaneously.  The extension that we need is named \"WEBGL_draw_buffers\".\nWhen that extension is activated, it becomes possible to attach several textures (or ", ")\nto a framebuffer, and it becomes possible for a shader to write data to all of the attached\nshaders simultaneously.  The extension is relatively complicated to use.  It must be activated,\nas usual, with a statement of the form", "Assuming that the extension is available, the maximum number of color buffers that can\nbe used in a shader is given by ", ", which will be at least\nfour.  With the extension in place, you can attach multiple textures as color buffers for \na framebuffer, using code of the form", "and so on, using using constants such as ", " from the\nextension object to specify the attachment points.", "Usually in a fragment shader, the color that is output to the color buffer is specified by \nassigning a value to the special variable ", ".  That changes when multiple\ncolor buffers are used.  In that case, instead of ", ", the fragment shader\nhas a special variable ", " which is an array of ", ", one for each\npossible color buffer.  Colors are output to the color buffers by assigning values to\n", "[0], ", "[1], ....  Because this is a change in the legal\nsyntax of the shader, the extension must also be activated in the fragment shader source code\nby adding the line", "to the beginning of the code.  Suppose, for example, that we want to store\na normal vector, a diffuse color, a specular color, and object coordinates in the color\nbuffers.  Let's say that these values are input to the fragment shader as varying\nvariables or uniform variables, except for the diffuse color, which is sampled from\na texture.  Then the fragment shader might take the form", "The final requirement for using the extension is to specify the correspondence between the indices that are\nused in ", " and the color buffers that have been attached to the \nframebuffer.  It seems like the correspondence should be automatic, but it's not.\nYou have to specify it using the JavaScript function, ", "\nfrom the extension object.  This function takes an array as parameter, and the values\nin the array are chosen from the constants ", ",\n", ", ....  These are the same constants that\nare used to specify the color buffer attachment points in a framebuffer.  For example,\nif for some reason you wanted a fragment shader to output to the color buffers that\nare attached at attachment points 2 and 3, you would call", "With all that setup, you are ready to do the first pass for deferred shading.\nFor the subsequent passes, you would use a different shader, with a single color\nbuffer (and with the blend function set to do additive blending).  For those passes,\nyou want to run the fragment shader once for each pixel in the image.  The fragment\nshader will use the pixel data that was saved in the first pass, together with\nother information such as light properties, to compute the output color for the pixel.\nYou can trigger a call to the fragment shader for each pixel simply by drawing a\nsingle rectangle that covers the image.", "The theory behind deferred shading is not all that complicated, but there are\na lot of details to get right in the implementation.  Deferred shading is just\none of many tricks that are used by video game programmers to improve the rendering\nspeed for their games."], "chapter_title": "3D Graphics with WebGL", "id": 7.5}, {"section_title": "Other Features", "chapter_id": "Chapter 5", "section_id": "Section 5.3", "content": ["We will finish this chapter with a look at several additional features\nof ", ".  In the process, you will learn about some new aspects\nof 3D graphics.", "We start with a simple example: ", ".\nAnaglyph refers to 3D images that are meant to be viewed through red/cyan (or red/green\nor red/blue) glasses.  The image contains two copies of the scene, one as viewed from the\nleft eye, drawn using only red, and one as viewed from the right eye, drawn using only\ngreen and blue.  When the image is viewed through red/cyan glasses, the left eye\nsees only the left-eye view of the scene and the right eye sees only the right-eye\nview of the scene.  The result is what looks like a real 3D scene, with depth perception.\nThe result isn't perfect, because screen colors and the color filters in the glasses\naren't perfect.  You will see some \"ghosts,\" where part of the left-eye image gets through\nto the right eye or vice versa.  Anaglyph stereo works best for monochrome images\nthat contain only shades of gray.  Color images are more problematic.  There are \nseveral ways of separating the colors into left and right-eye images, none of them \nperfect. The one used in ", " works pretty well except for pure shades of red,\nwhich will only be visible to one eye.", "Here is an example, from the sample program ", ",\nwhich is identical to ", ", except for the\nuse of anaglyph rendering.  For a proper 3D view, you need to look at this image\nthrough red/cyan glasses:", "\n", "In ", ", anaglyph stereo is implemented by the class\n", ".  This class is not defined\nin the main ", " JavaScript file; it's defined in a separate file named\n", ", \nwhich can be found in the ", " download in the folder\n", ".  To use it, you need to include that file in\na ", " element in your HTML file.", "The class is very easy to use.  The constructor for an object of type\n", " takes an ordinary\n", " as a parameter. The size of the\ndrawing area is also specified in the constructor (or call the object's \n", " method later to specify the size). For example,:", "Once you have an ", ",\nyou can use it in place of the ", "\nto render the image:", "That's all there is to it!  The scene will be rendered in anaglyph stereo.", "Most real programs require some kind of user interaction.  For a web application,\nof course, the program can get user input using HTML widgets such as buttons and text input boxes.\nBut direct mouse interaction with a 3D world is more natural in many programs.", "The most basic example is using the mouse to rotate the scene.  In ", ",\nrotation can be implemented using the class ", " or the\nclass ", ".  The main difference is that with ", ",\nthe rotation is constrained so that the positive ", "-axis is always the up direction\nin the view.  ", ", on the other hand, allows completely free\nrotation.  These classes are used in my ", " examples and demos that\nimplement mouse rotation.", "The two control classes are not part of the main ", " JavaScript file.\nTo use one of them, you need the JavaScript file ", "\nor ", ", which can be\nfound in the folder ", " in the ", " download.", "The two classes are used in a similar way.  I will discuss ", ".\nIn my examples, I create a camera and move it away from the origin. I usually add a light object\nto the camera object, so that the light will move along with the camera, providing some\nillumination to anything that is visible to the camera.  The ", "\nobject is used to rotate the camera around the scene.  The constructor for the\ncontrol object has two parameters, the camera and the canvas on which the scene is rendered.\nHere is typical setup:", "The constructor installs listeners on the ", " so that the controls can respond\nto mouse events.  It is also necessary to call", "just before each rendering of the scene.  This method adjusts the camera rotation.\nThe controls are designed to be used in a scene that is constantly being animated,\nso that the render function is being called regularly.  No change will be visible until\nthe render function is called.  (In my programs that don't use animation, I add an extra\nlistener to make sure that ", "() and ", "() are called whenever\nthe mouse is dragged.  See, for example, the source code for \n", " to see how it's done.)", "The controls can also do \"panning\" (dragging the scene in the plane of the screen)\nwith the right mouse button and \"zooming\" (scaling the scene) with the middle mouse button\nor scroll wheel.  I generally turn off those features by setting", "A much more interesting form of mouse interaction is to let the user select objects\nin the scene by clicking on them.  The problem is to determine which object the\nuser is clicking.  The general procedure is something like this:  Follow a ray from\nthe camera through the point on the screen where the user clicked and find the first\nobject in the scene that is intersected by that ray.  That's the object that is visible\nat the point where the user clicked.  Unfortunately, the procedure involves a lot\nof calculations.  Fortunately, ", " has a class that can do the work for\nyou:  ", ".", "A ", " can be used to find intersections of a ray with\nobjects in a scene.  (A ray is just half of a line, stretching from some given starting\npoint in a given direction towards infinity.)  You can make one raycaster object to use\nthroughout your program:", "To tell it which ray to use, you can call", "where both of the parameters are of type ", ".\nTheir values are in terms of ", ", the same coordinate\nsystem that you use for the scene as a whole.  The ", " must\nbe a ", ", with length equal to one.\nFor example, suppose that you want to fire a laser gun....  The \n", " is the location of the gun, and the ", "\nis the direction that the gun is pointing.  Configure the raycaster \nwith those parameters, and you can use it to find out what object is struck\nby the laser beam.", "Alternatively, and more conveniently\nfor processing user input, you can express the ray in terms of the camera and\na point on the screen:", "The ", " are given as a ", " expressed\nin ", ".  This means the horizontal coordinate ranges from\n\u22121 on the left edge of the viewport to 1 on the right, and the vertical coordinate ranges from\n\u22121 at the bottom to 1 on the top.  (Clip coordinates are called \"normalized device\ncoordinates\" in ", ".)  So, we need to convert from pixel coordinates on a\ncanvas to clip coordinates.  Here's one way to do it, given a mouse event, ", ":", "Once you have told the raycaster which ray to use, it is ready to find\nintersections of that ray with objects in the scene.  This can be\ndone with the function", "The first parameter is an array of ", ".\nThe raycaster will search for intersections of its current ray with objects in the array.\nIf the second parameter is ", ", it will also search descendants of those\nobjects in the ", "; it it is ", " or is omitted, then only the\nobjects in the array will be searched.  For example, to search for intersections\nwith all objects in the scene, use", "The return value from ", " is an array of JavaScript objects.\nEach item in the array represents an intersection of the ray with an ", ".\nThe function finds all such intersections, not just the first.  If no intersection is\nfound, the array is empty.  The array is sorted by increasing distance from the starting \npoint of the ray.  If you just want the first intersection, use the first element of the array.", "An element in the array is an object whose properties contain information about the\nintersection.  Suppose that ", " is one of the array elements.  Then the most useful\nproperties are:  ", ", which is the ", " that \nwas intersected by the ray; and ", ", which  is the point of intersection, \ngiven as a ", " in world coordinates.  That information is\nenough to implement some interesting user interaction.", "The following demo\nuses some basic mouse interaction to let the user edit a scene.  The scene shows\na number of tapered yellow cylinders standing on a green base.  The user can\ndrag the cylinders, add and delete cylinders, and rotate the scene.  A set of \nradio buttons lets the user select which action should be performed by the mouse.", "\n", "\n", "Let's look at how the actions are implemented.   The only objects are the base and the\ncylinders.  In the program, the base is referred to as ", ", and all the objects\nare children of an ", " named ", ".  (I use the ", "\nobject to make it easy to rotate the set of all visible objects without moving the camera or lights.)\nFor all drag, add, and delete actions, I look for intersections of these objects with\na ray that extends from the camera through the mouse position:", "If ", " is zero, there are no intersections, and there is nothing\nto do.  Otherwise, I look at ", "[0], which represents an intersection with\nthe object that is visible at the mouse position.  So, ", "[0]", "\nis the object that the user clicked, and ", "[0]", " is the\npoint of intersection.", "The Delete action is the simplest to implement:  When the user clicks a cylinder, the cylinder should be \nremoved from the scene.  If the first intersection is with the ", ", then \nnothing is deleted.  Otherwise, the clicked object was a cylinder and should be deleted:", "For an Add action, we should add a cylinder only if the user clicked the ground. In that case,\nthe point of intersection tells where the cylinder should be added.  An interesting issue\nhere is that we get the point of intersection in world coordinates, but in order to\nadd the cylinder as a child of ", ", I need to know the point of intersection\nin the local coordinate system for ", ".  The two coordinate systems will be different\nif the world has been rotated.  Fortunately, every ", "\nhas a method ", "(", ") that can be used to transform a ", ",\n", ", from world coordinates to local coordinates for that object.  This method does not\nreturn a value; it modifies the coordinates of the vector\u00a0", ".  (There is also a\n", " method.)  So, the Add action can be implemented like this:", "For a Drag action, we can determine which cylinder was clicked using the same test as for delete.\nHowever, the problem of moving the cylinder as the user drags the mouse raises a new issue:\nhow do we know where to put the cylinder?  We somehow have to transform a new mouse\nposition into a new position for the cylinder.   For that, we can use the raycaster\nagain.  My first thought was to use a ray from the camera through the new mouse position,\nto find the intersection of that ray with the ground, and then to move the cylinder to that \npoint of intersection.  Unfortunately, this puts the ", " of the cylinder at the\nmouse position, and it made the cylinder jump to the wrong position as soon as I started moving\nthe mouse.  I realized that I didn't want to track the intersection with the ground; I needed to\ntrack the intersection with a plane that lies at the same height as the original point of intersection.\nTo implement this, I add an invisible plane at that height just during dragging,\nand I use intersections with that plane instead of intersections with the ground.\n(You can have invisible objects in ", "\u2014just set the ", " \nproperty of the material to ", ".)", "One thing that has been missing in our 3D images is shadows.  Even if you didn't\nnotice the lack consciously, it made many of the images look wrong.\nShadows can add a nice touch of realism to a scene, but OpenGL, including WebGL, \ncannot generate shadows automatically.  There are ways to compute shadows that \ncan be implemented in OpenGL, but they are tricky to use and they are not completely\nrealistic physically.  One method, which is called ", ",\nis implemented in ", ".  Shadow mapping in ", " is certainly not trivial to use,\nbut it is easier than trying to do the same thing from scratch.", "Here is\na demo that\nshows a ", " scene that uses shadow mapping.  The lights that\ncast the shadows can be animated, so you can watch the shadows change\nas the lights move.", "\n", "\n", "The basic idea of shadow mapping is fairly straightforward: To tell what parts\nof a scene are in shadow, you have to look at the scene from the point of view of\nthe light source.  Things that are visible from the point of view of the light\nare illuminated by that light.  Things that are not visible from the light are\nin shadow.  (This is ignoring the possibility of transparency and indirect, reflected\nlight, which cannot be handled by shadow mapping.)  To implement this idea, place a camera at the light source and take a picture.\nIn fact, you don't need the picture.  What you need is the ", ".  After the\npicture has been rendered, the value stored in the depth buffer for a given pixel\ncontains, essentially, the distance from the light to the object that is visible from the\npoint of view of the light at that pixel.  That object is illuminated by the light. If an object\nis at greater depth than the value stored in the depth buffer, then that object is\nin shadow.  The depth buffer is the shadow map.  Now, go back to the point of view of the camera,\nand consider a point on some object as it is rendered from the camera's point of view.  Is that\npoint in shadow or not?  You just have to transform that point from the camera's viewing coordinates\nto the light's viewing\ncoordinates and check the depth of the transformed point.  If that depth is greater than\nthe corresponding value in the shadow map, then the point is in shadow.  Note that if there\nare several lights, each light casts its own shadows, and you need a shadow map for each light.", "It is computationally expensive to compute shadow maps and to apply them, and\nshadows are disabled by default in ", ".  To get shadows, you need to do\nseveral things.  You need to enable shadow computations in the WebGL renderer by\nsaying", "Only ", " and\n", " can cast shadows.  To get\nshadows from a light, even after enabling shadows in the renderer, you have to set\nthe light's ", " property to ", ":", "Furthermore, shadows have to be enabled for each object that\nwill cast or receive shadows.  \"Receiving\" a shadow means that shadows will\nbe visible on that object.  Casting and receiving are enabled separately for\nan object.", "Even this might not make any shadows show up, and if they do they might\nlook pretty bad.  The problem is that you usually have to configure the\ncameras that are used to make the shadow maps.", "Each ", " or\n", " has its own\nshadow camera, which is used to create the shadow\nmap from the point of view of that light.  The shadow camera for a directional\nlight uses an ", ".  An orthographic projection is \nconfigured by view volume limits\n", ", ", ", ", ", ", ", ", ", and\n", " (see ", ").   \nFor a directional light, ", ", these limits correspond to\nthe properties ", ", ", ", ", ",\n", ", ", ",  and ", ".\nThese values are relative to ", ".  It is important to make sure that\nall the objects in your scene, or at least those that cast shadows, are within the view volume of\nthe shadow camera.  Furthermore,\nyou don't want the limits to be too big: If the scene occupies only a small part\nof the camera's view volume, then only a small part of the shadow map will contain\nuseful information\u2014and since there is so little information about shadows, your\nshadows won't be very accurate.  The default values are set for a very large scene.\nFor a relatively small scene, you might set:", "The shadow camera for a spotlight uses a ", ".  (The use of\na camera with a limited view is why you can have shadows from spotlights but not from\npoint lights.)  For a spotlight ", ", the shadow camera is configured by the\nproperties ", ", ", ",\nand  ", " (where \"fov\" is the vertical\nfield of view angle, given in degrees rather than radians).  The default value\nfor fov is probably OK, except that if you change the spotlight's cutoff angle,\nyou will want to change the fov to match.  But you should be sure to set appropriate values\nfor near and far, to include all of your scene and as little extra as is practical.\nAgain, near and far are distances from ", ".", "Both types of light have a ", " property, with a value between 0 and\n1, that determines how dark the shadows are.  The default value, 0.5, gives fairly light\nshadows, and you might want to increase it.  Finally, you might want to increase\nthe size of the shadow map.  The shadow map is a kind of texture image which by default is 512 by 512\npixels.  You can increase the accuracy of the shadows by using a larger shadow map.\nTo do that for a light, ", ", set the values of the properties\n", " and ", ".  For example,", "I'm not sure whether power-of-two values are absolutely required here, but\nthey are commonly used for textures.", "We have created and viewed simple scenes, shown on a solid-colored background.\nIt would be nice to put our scenes in an \"environment\" such as the interior of a building,\na nature scene, or a public square.  It's not practical to build representations of\nsuch complex environments out of geometric primitives, but we can get a reasonably\ngood effect using textures.  The technique that is used is called a\n", ".  A skybox is a large cube where a\ndifferent texture is applied to each face of the cube.  The textures are images\nof some environment.  For a viewer inside the cube, the six texture images on the cube fit together to provide\na complete view of the environment in every direction.  The six texture images\ntogether make up what is called a ", ".\nThe images must match up along the edges of the cube to form a seamless view of\nthe environment.", "A cube map of an actual physical environment can be made by taking six pictures\nof the environment in six directions: left, right, up, down, forward, and back.  (More realistically,\nit is made by taking enough photographs to cover all directions, with overlaps, and then using software to \"stitch\"\nthe images together into a complete cube map.)  The six\ndirections are referred to by their relation to the coordinate axes as:\npositive\u00a0x, negative\u00a0x, positive\u00a0y, negative\u00a0y, positive\u00a0z, and negative\u00a0z,\nand the images must be listed in that order when you specify the cube map.\nHere is an example.  The first picture shows the six images of a cube map laid\nout next to each other.  The positive y image is at the top, the negative y image is at the\nbottom.  In between are the negative x, positive z, positive x, and negative z images\nlaid out in a row.  The second picture shows the images used to texture a cube, viewed here from \nthe outside.  You\ncan see how the images match up along the edges of the cube:", "\n", "(This cube map, and others used in this section, are by Emil Persson, who has made a \nlarge number of cube maps available for download\nat ", "\nunder a creative commons license.)", "For a skybox, a very large cube is used.  The camera, lights, and any objects that are to be part\nof the scene are inside the cube.  The skybox cube itself should not be lit; any lighting\nof the environment is already part of the cube map texture images.", "One way to make a skybox is to load the cube map as a set of six separate texture images,\nand then use ", " to apply one texture image to\neach face of the cube.  The material for each face should be a ", ",\nwhich does not use lighting.  The six images can be loaded using ", " and\ncan be applied to a face using the ", " property of the material,\nas discussed in ", "  For example:", "(This code does not include a way to get the scene redrawn after\nthe textures have finished loading.  That's fine if an animation is running.\nIf not, you have to use a callback function in the texture loader to do the\nredrawing.)", "However, WebGL has built-in support for using cubemap textures directly.  It's possible\nto load the six images of a cube map into a single object representing the\ncubemap texture as a whole.  To use such a cubemap texture, ", "\nrequires a new kind of material called ", ".\nA shader material uses custom vertex and fragment shaders to render the image.\n(", " and ", "\nare programs that run on the ", ".  The are required for rendering images \nin WebGL, and we will start working with them directly in the ", ".) \nThe shaders that are required in this case are defined in a ", " shader library.\nHere's an example of making a skybox using a cubemap texture.  This code is copied\nfrom examples in the ", " download.  It uses the same ", " array\nas the previous example:", "The sample program ", "\nshows two WebGL scenes that use cube maps.  In the first, the texture is applied using\na ", ", and the cube is viewed from the\noutside.   In the second, the texture is a applied to a skybox cube as a cubemap texture, \nand the cube is viewed from the inside.", "A reflective surface shouldn't just reflect light\u2014it should reflect its\nenvironment.  ", " can use ", "\nto simulate reflection.  (Environment mapping is also called \"reflection mapping\").\nEnvironment mapping uses a cube map texture.  Given a\npoint on a surface, a ray is cast from the camera position to that point,\nand then the ray is reflected off the surface.  The point where that reflected\nray hits the cube determines which point from the texture maps to the point on the\nsurface.  For a simulation of perfect, mirror-like reflection, the surface\npoint is simply painted with the color from the texture.\nNote that the surface does not literally reflect other objects in the scene.\nIt reflects the contents of the cube map texture.  However,\nif the same cube map texture is used on a skybox, and if the skybox is\nthe only other object in the scene,  then it will look like\nthe surface is a mirror that perfectly reflects its environment.", "This type of reflection is very easy to do in ", ".  You only need to\nmake a ", " and set its\n", " property equal to the cubemap texture object.  For example,\nif ", " is the texture object obtained using\n", "(), as in the skybox examples above, we can make a sphere that\nperfectly reflects the texture by saying:", "For the effect to look good, you would want to use the\nsame texture on a skybox.  Note that no lighting would be necessary in the scene, since\nboth the skybox and the sphere use a ", ".\nThe colors seen on the sphere come entirely from the environment map and\nthe basic color of the sphere material.  The environment map color is multiplied\nby the basic color.  In this example, the basic ", " of the\nmaterial is white, and the sphere color is exactly equal to the color\nfrom the texture.  With a different base color, the environment map texture would be \"tinted\"\nwith that color.  You could even apply a regular texture map to the sphere, to be used in place of\nthe color, so that the reflection of the skybox would be combined with the texture.", "The sample program ", " demonstrates\nenvironment mapping.  It can show a variety of environment-mapped objects, with\na variety of skymap textures, and it has several options for the base color of the\nobject.  Here are two images from that demo.  The one on the left shows a reflective\narrowhead shape with a white base color.  On the right, the object is a model\nof a horse (taken from the ", " download) whose base color is pink:", "\n", "\nHere is a demo that is very similar to the sample program.  In the demo, you can choose\nto view just the skybox or just the reflective object, and you can see that the object only\nseems to be reflecting its actual environment.  Use your mouse to rotate the scene to see\nhow the reflection changes as the orientation of the object changes.", "\n", "\n", "\n", " can also do ", ".  Refraction\noccurs when light passes through a transparent or translucent object.  A ray of light will be\nbent as it passes between the inside of the object and the outside.  The amount\nof bending depends on the so-called \"indices of refraction\" of the material outside\nand the material inside the object.  More exactly, it depends on the ratio between\nthe two indices.  Even a perfectly transparent object will be visible because of\nthe distortion induced by this bending.", "In ", ", refraction is implemented using environment maps.  As with reflection,\na refracting object does not show its actual environment; it refracts the cubemap texture\nthat is used as the environment map.  For refraction, a special\n\"mapping\" must be used for the environment map texture.  The mapping is the second\nparameter to ", "().  Here is an example of\nloading a cubemap texture for use in refraction mapping", "In addition to this, the ", " property of the material\nthat is applied to the refracting object should be set.  The value is a number\nbetween 0 and\u00a01; the closer to 1, the less bending of light.  The default value\nis so close to 1 that the object will be almost invisible.  My example, below, uses\na value of 0.6:", "This gives a strong refractive effect.  If you set the material color to\nsomething other than white, you will get something that looks like tinted glass.\nAnther property that you might set is the ", ".  For a refractive\nobject, this value tells how much light is transmitted through the object rather than reflected\nfrom its surface.\nThe default value, 1, gives 100% transmission of light; smaller values make objects\nlook like they are made out of \"cloudy\" glass that blocks some of the light.", "The sample program ", "\nis a copy of ", " that has been modified to\ndo refraction instead of reflection.  The objects look like they are made of glass instead\nof mirrors.  An option has been added to make the glass look cloudy.  The following images are\nfrom that program.  A perfectly transmissive arrowhead is shown in the first image, and a cloudy\nsphere in the second.  Notice how the sphere shows an inverted image of the objects\nbehind it:", "\n", "In my reflection and refraction examples, the environment is a skybox, and there is a single\nobject that reflects or refracts that environment.  But what if a scene includes more than one\nobject?  The objects won't be in the cubemap texture.  If you use the cubemap texture on\nthe objects, they won't reflect or refract ", ".  There is no complete solution\nto this problem in WebGL.  However, you can make an object reflect or refract other objects\nby making an environment map that includes those objects.  If the objects are moving, this means\nthat you have to make a new environment map for every frame.  Recall that an environment map\ncan be made by taking six pictures of the environment.  ", " has a kind of camera that\ncan do just that, ", ".  I won't go into the\nfull details, but a CubeCamera can take a\nsix-fold picture of a scene from a given point of view and make a cubemap texture from\nthose images.  To use the camera, you have to place it at the location of an object\u2014and make the\nobject invisible so it doesn't show up in the pictures.  Snap the picture, and apply it\nas an environment map on the object.  For animated scenes, you have to do this in every frame,\nand you need to do it for every reflective/refractive object in the scene.  Obviously, this\ncan get very computationally expensive!  And the result still isn't perfect.  For one thing,\nyou won't see multiple reflections, where objects reflect back and forth on each other several\ntimes. For that, you need a different kind of rendering from the one used by OpenGL."], "chapter_title": "Three.js: A 3D Scene Graph API", "id": 5.3}, {"section_title": "Building Objects", "chapter_id": "Chapter 5", "section_id": "Section 5.2", "content": ["In ", ", a visible object is constructed from a\ngeometry and a material.  We have seen how to create simple\ngeometries that are suitable for point and line primitives,\nand we have encountered a variety of standard mesh geometries\nsuch as ", " and\n", ".  In this\nsection, we will see how to create new mesh geometries from\nscratch  We'll also look at some of the other support\nthat ", " provides for working with objects and\nmaterials.", "A mesh in ", " is what we called an ", " in\n", ".  In a ", " mesh, all the polygons are triangles.\nA geometry in ", " is an object of type ", ".\nAny geometry object contains an array of vertices, represented as objects of type\n", ".  For a mesh geometry, it also contains an\narray of faces, represented as objects of type ", ".  Each object\nof type ", " specifies one of the triangular faces of the\ngeometry.  The three vertices of the triangle are specified by three integers.  Each\ninteger is an index into the geometry's vertex array.  The three integers can\nbe specified as parameters to the ", " constructor.\nFor example,", "The three indices are stored as properties ", ", ", ", and\n", " of the face object. As an example, let's see how to directly create a ", " geometry\nfor this pyramid:", "\n", "Note that the bottom face of the pyramid, which is a square, has to be\ndivided into two triangles in order for the pyramid to be represented as a mesh geometry.\nIf ", " is the geometry object for this pyramid, then\n", " is the vertex array, and ", "\nis the face array.  With that in mind, we can define:", "Note that the order of the vertices on a face is not completely\narbitrary:  They should be listed in counterclockwise order as seen from in\nin front of the face, that is, looking at the face from the outside of the\npyramid.", "This pyramid geometry as given will work with a ", ",\nbut to work with lit materials such as ", "\nor ", ", the geometry needs ", ".\nIf the geometry has no normal vectors, Lambert and Phong materials will appear black.\nIt is possible to assign the normal vectors by hand, but you can also have ", " \ncompute them for you by calling methods in the geometry class.  For the pyramid,\nthis would be done by calling", "This method computes one normal vector for each face, where\nthe normal is perpendicular to the face.   This is sufficient if the\nmaterial is using ", "; that is, if the material's ", "\nproperty is set to ", ". The ", " property\nwas discussed in ", ".", "Flat shading is appropriate for the pyramid.\nBut when an object is supposed to look smooth rather than faceted, it needs a normal\nvector for each vertex rather than one for each face.  A ", " has an array of three\nvertex normals.  They can be set by hand, or ", " can compute\nreasonable vertex normals for a smooth surface by averaging the face normals\nof all faces that share a vertex.  Just call", "where ", " is the geometry object.\nNote that the face normals must already exist before ", " is called, \nso that usually you will call ", "() immediately after calling\n", "().  A geometry that has face normals but not vertex normals\nwill not work with a material whose ", " property has the default value,\n", ".  To make it possible to use ", " on a surface \nlike the pyramid, all of the vertex normals of each face should be set equal to its face normal.  In\nthat case, even with smooth shading, the pyramid's side will look flat.  Standard ", "\ngeometries such as ", " come with correct face and vertex\nnormals.", "The face normal for an object, ", ", of type ", "\nis stored in the property ", ".  The vertex normals are stored in\n", ", which is an array of three ", ".", "With a full set of normal vectors, the pyramid is ready to be used with any of the mesh\nmaterials that we have covered, but it looks a little boring with just one color.\nIt's possible to use several colors on one mesh.  One way to do that is\nto use a ", " on the object.  An object of \nthat type applies different materials to different faces.  Its constructor takes an\narray of mesh materials of the usual kind.  For example, here is how to make\na cube with different materials on its six sides:", "For a ", " to work with a geometry, each face\nof the geometry needs a \"material index.\"  The material index of a face is an integer\nthat is an index into the array of face materials.  The faces of a ", "\nhave appropriate indices.  Note that a box geometry has 12 faces, since each rectangular side\nis divided into two triangular faces.  The two triangles that make up a rectangular side\nhave the same material index.  (", " is the only standard\ngeometry that I can find that comes with non-zero material indices.  The default value\nfor the material index is zero.)", "Suppose that we want to use a ", " on the\npyramid that was created above.  For that to work, each face needs a material index,\nwhich is stored in a property of the face named ", ".  For the pyramid, the first two \nfaces in the array of faces make up the square\nbase of the pyramid.  They should probably have the same material index.  The following code\nassigns material index 0 to the first two faces and material indices 1, 2, 3, and 4 to\nthe other four faces:", "This code is from the sample program ", ".\nThe program displays a cube and a pyramid using ", ".  Here's what they\nlook like:", "\n", "There is another way to assign a different color to each face of a mesh object:  It is possible\nto store the colors as properties of the face objects in the geometry.  You can then use an ordinary\nmaterial rather than a ", ".  But you also have to tell the\nmaterial to use the colors from the geometry.", "There are several ways that color might be assigned to meshes.  One is to simply\nmake each face a different solid color.  Each face object has a ", " property that\ncan be used to implement this idea.  The value of the ", " property is an object of type\n", ", representing a color for the entire face.\nFor example, we can set the face colors of the pyramid with", "To use these colors, the ", " property\nof the material must be set to the value ", ";\nfor example:", "The default value of the property is ", ", which tells\nthe renderer to use the material's ", " property for every face.", "A second way to apply color to a face is to apply a different\ncolor to each vertex of the face.  WebGL will then interpolate the vertex colors to compute\ncolors for pixels inside the face.  Each face object has a property named ", "\nwhose value should be an array of ", " objects,\none for each vertex of the face.  To use these colors, the ", "\nproperty of the material has to be set to ", ".", "The following\ndemo uses vertex colors and face colors on a icosahedral approximation for a sphere.  The\ncolors can be animated.  In a color animation, each of the colors that is used on the\nobject cycles through the set of possible hues.  The positions of the vertices can also be\nanimated.", "\n", "\n", "In addition to letting you build indexed face sets, ", " has support for working\nwith curves and surfaces that are defined mathematically.  Some of the possibilities are\nillustrated in the sample program ", ", and I will\ndiscuss a few of them here.", "Parametric surfaces are the easiest to work with.  A parametric surface is defined by\na mathematical function ", "(", "), where ", " and ", " are numbers,\nand each value of the function is a point in space.  The surface consists of all the points\nthat are values of the function for ", " and ", " in some specified ranges.\nFor ", ", the function is a regular JavaScript function that returns values of\ntype ", ".  A parametric surface geometry \nis created by evaluating the function at a grid of ", " points.  This gives a\ngrid of points on the surface, which are then connected to give a polygonal approximation\nof the surface.  In ", ", the values of ", " and ", " are in the\nrange 0.0 to 1.0.  The geometry is created by a constructor", "where ", " is the JavaScript function and ", " and ", " determine\nthe number of points in the grid; ", " gives the number of subdivisions of the\ninterval from 0 to 1 in the ", " direction, and ", ", in the ", " direction.\nOnce you have the geometry, you can use it to make a mesh in the usual way.  Here is\nan example, from the sample program:", "\n", "This surface is defined by the function", "and the ", " mesh that represents the surface is created using", "Curves are more complicated in ", " (and, unfortunately, the API for working\nwith curves is not very consistent).  The class ", "\nrepresents the abstract idea of a parametric curve in two or three dimensions.  (It does not represent\na ", " geometry.)  A parametric curve is defined by a function of one numeric variable\n", ".  The value returned by the function is of type ", " for a 2D curve or \n", " for\na 3D curve.  For an object, ", ", of type ", ",\nthe method ", "(", ") should return the point on the curve corresponding\nto the value of the parameter ", ".  However, in the ", " class\nitself, this function is undefined.  To get an actual curve, you have to define it.\nFor example,", "Once ", " is defined, you have a usable curve.  One thing that you can do with\nit is create a tube geometry, which defines a surface that is a tube with a circular cross-section\nand with the curve running along the center of the tube. The sample program uses the ", " \ncurve, defined above, to create two tubes:", "\n", "The geometry for the wider tube is created with", "The second parameter to the constructor is the number of subdivisions of the surface along the\nlength of the curve.  The third is the radius of the circular cross-section of the tube, and the fourth is the\nnumber of subdivisions around the circumference of the cross-section.", "To make a tube, you need a 3D curve.  There are also several ways to make a surface from\na 2D curve.  One way is to rotate the curve about a line in 3D.  The surface consists of\nall the points that the curve passes through as it rotates.  The is called ", ".\nThis image from the sample program shows the surface generated by lathing a cosine curve.\nThe curve itself is shown above the surface:", "\n", "The surface is created in ", " using a ", "\nobject.  A ", " is constructed not from a curve but from\nan array of points that lie on the curve.  The points are objects of type \n", ", but they should lie in the ", "-plane, \nmeaning that they should have ", "-coordinate equal to zero.  The constructor takes\nthe form", "The first parameter is the array of ", ".  The second\nis the number of subdivisions of the surface along the circle generated when a point\nis rotated about the axis.  (The number\nof \"stacks\" for the surface is given by the length of the points array.)  In the sample\nprogram, I create the array of points from an object, ", ", of type\n", " by calling ", "(128).  This function\ncreates an array of 128 points on the curve, using values of the parameter that\nrange from 0.0 to 1.0.", "Another thing that you can do with a 2D curve is simply to fill in the inside of the curve, giving a \n2D filled shape.  To do that in ", ", you can use an object of type\n", ", which is a subclass of ", ".\nA ", " can be defined in the same way as a path in the \n2D Canvas API that was covered in ", ".  That is, an\nobject ", " of type ", " has methods\n", ", ", ", ", " and ", "\nthat can be used to define the path. See ", " for details of how\nthese functions work.  As an example, we can create a teardrop shape:", "To use the path to create a filled shape in ", ", we need a ", "\nobject:", "The second parameter to the constructor is a JavaScript object whose properties specify\noptions for the geometry.  The ", " option specifies the number of points\nfrom the path that are used to create the outline of the shape.  The 2D shape created with this geometry\nis shown on the left in this picture:", "\n", "The other two objects in the picture were created by ", "\nthe shape.  In extrusion, a filled 2D shape is moved along a path in 3D.  The points that the shape\npasses through make up a 3D solid.  In this case, the shape was extruded along a line segement\nperpendicular to the shape, which is the most common case.  The basic extruded shape is\nshown on the right in the illustration.  The middle object is the same shape with \"beveled\"\nedges.  For more details on extrusion, see the documentation for ", "\nand the source code for the \n", ".", "A ", " can be used to add visual interest and detail to an object.\nIn ", ", an image texture is represented by an object of type \n", ".  Since we are talking about web pages, the image\nfor a ", " texture is generally loaded from a web address.  Image textures are usually created by \nusing the ", " function from the class\n", ".  The function takes a\nURL (web address) as parameter and returns a ", " object:", "A texture in ", " is considered to be part of a material.  To apply\na texture to a mesh, just assign the ", " object to the\n", " property of the mesh material that is used on the mesh:\n", "The ", " property can also be set in the material constructor.\nAll three types of mesh material (Basic, Lambert, and Phong) can use a texture.\nIn general, the material color will be white, since the material color will\nbe multiplied by colors from the texture.  A non-white material color will add\na \"tint\" to the texture colors.  The ", " that are needed to\nmap the image to a mesh are part of the mesh\ngeometry.  The standard mesh geometries such as ", "\ncome with texture coordinates already defined.", "That's the basic idea\u2014create a texture object from an image URL and assign\nit to the ", " property of a material.  However, there are complications.\nFirst of all, image loading is \"asynchronous.\"  That is, calling\n", " only starts the process of loading the image,\nand the process can complete sometime after the function returns.  Using a texture\non an object before the image has finished loading does not cause an error, but\nthe object will be rendered as completely black.  Once the image\nhas been loaded, the scene has to be rendered again to show the image texture.\nIf an animation is running, this will happen automatically; the image will appear\nin the first frame after it has finished loading.  But if there is no animation,\nyou need a way to render the scene once the image has loaded.  In fact, the\n", " function has several optional parameters:", "The ", " parameter tells how the texture is applied. For\nbasic image textures, pass the default value ", "\u2014or pass\n", " as the parameter, to use the default.  The ", "\nand ", " parameters are callback functions.  The ", "\nfunction, if defined, will be called once the image has been successfully loaded.\nThe ", " function will be called if the attempt to load the image\nfails.  For example, if there is a function ", "() that renders the\nscene, then ", " itself could be used as the ", " function:", "Another possible use of ", " would be to delay assigning\nthe texture to a material until the image has finished loading.  If you do\nchange the value of ", ", be sure to set", "to make sure that the change will take effect when the object is redrawn.", "A ", " has a number of properties that can be\nset, including properties to set the ", "\nand ", " filters for the\ntexture and a property to control the generation of ", ", which is done automatically\nby default.  The properties that you are most likely to want to change are the wrap mode\nfor texture coordinates outside the range 0 to\u00a01 and the ", ".\n(See ", " for more information about these properties.)", "For a ", " object ", ", the properties ", " and ", "\ncontrol how ", " and ", " texture coordinates outside the range 0 to 1 are\ntreated.  The default is \"clamp to edge.\"  You will most likely want\nto make the texture repeat in both directions by setting the property values to\n", ":\n", "RepeatWrapping works best with \"seamless\" textures, where the top edge of the image\nmatches up with the bottom edge and the left edge with the right.\n", " also offers an interesting variation called \"mirrored\nrepeat\" in which every other copy of the repeated image is flipped.  This eliminates\nthe seam between copies of the image.  For mirrored repetition, use the property value\n", ":", "The texture properties ", " and ", " control the scaling and the translation\n", " that are applied to the texture.  (There is no\ntexture rotation.)  The values of these properties are of type ", ",\nso that each property has an ", " and a ", " component.  For a ", ",\n", ", the two components of ", " give the texture translation in the horizontal\nand vertical directions.  To offset the texture by 0.5 horizontally, you can say\neither", "or", "Remember that a positive horizontal offset will move the\ntexture to the ", " on the objects, because the offset is applied to the\ntexture coordinates not to the texture image itself.", "The components of the property ", "\ngive the texture scaling in the horizontal and vertical directions.  For example,", "will scale the texture coordinates by a factor of 2 horizontally and\n3 vertically.  Again, the effect on the image is the inverse, so that the image\nis shrunk by a factor of 2 horizontally and 3 vertically.  The result is that you\nget two copies of the image in the horizontal direction where you would have had\none, and three vertically.  This explains the name \"repeat\", but note that the values\nare not limited to be integers.", "This demo lets you\nview some textured ", " objects.  The \"Pill\" object in the demo, by the way,\nis a compound object consisting of a cylinder and two hemispheres.\n", "\n", "\n", "Suppose that we want to use an image texture on the pyramid that was created at the\nbeginning of this section. In order to apply a texture image to an object, WebGL needs texture coordinates\nfor that object.  When we build a mesh from scratch, we have to supply the texture\ncoordinates as part of the mesh's geometry object.", "A geometry object such as ", " in the example has a property\nnamed ", " to hold texture coordinates.  (\"UV\"\u00a0refers to the\ncoordinates on an object that are mapped to the ", " and ", " coordinates in a texture.)\nThe value of ", " is an array, where each element of the array\nis itself an array of arrays; in practice only the element ", "[0]\nis used.  (There is no reason for the extra array dimension, except to allow for more\noptions in future versions of ", ".)  The value of ", "[0] is itself an array,\nwith one element for each face in the geometry.\nThe data stored for each face is, again, an array: ", "[0][", "] is\nan array containing one pair of coordinates for each of the three vertices of face number\u00a0", ".  \nFinally, each pair of texture coordinates\nin that array is represented as an object of type ", ".", "The pyramid has six triangular faces.  We need an array of three objects of\ntype ", " for each face.  The coordinates have to be chosen\nto map the image in a reasonable way onto the faces.  My choice of coordinates\nmaps the entire texture image onto the square base of the pyramid, and it cuts\na triangle out of the image to apply to each of the sides. It takes some\ncare to come up with the correct coordinates.  I define the texture\ncoordinates for the pyramid geometry as follows:", "Note that this is a three-dimensional array.", "The sample program ", " shows\nthe pyramid with a brick texture.  Here is an image from the program:", "\n", "In order to understand how to work with objects effectively in ", ", it can be useful to\nknow more about how it implements ", ".\nI have explained that an ", ", ", ", has properties\n", ", ", ", and ", " that specify its modeling\ntransformation in its own local coordinate system.  But these properties are not used\ndirectly when the object is rendered.  Instead, they are combined to compute another\nproperty, ", ", that represents the transformation as a matrix.  By default, this\nmatrix is recomputed automatically every time the scene is rendered.  This can be inefficient\nif the transformation never changes, so ", " has another property,\n", ", that controls whether ", " is computed automatically.\nIf you set ", " to ", ", the update is not done.  In that case,\nif you do want to change the modeling transformation, you can call ", "()\nto compute the matrix from the current values of ", ", ", ",\nand ", ".", "We have seen how to modify ", " modeling transformation by directly changing the\nvalues of the properties ", ", ", ", and ", ".\nHowever, you can also change the position by calling the function ", "(", "),\n", "(", "), or ", "(", ") to move\nthe object by a specified amount in the direction of a coordinate axis.\nThere is also a function ", "(", "),\nwhere ", " is a ", " and ", " is\na number giving the distance to translate the object.  The object is moved\nin the direction of the vector, ", ".  The vector must be normalized;\nthat is, it must have length\u00a01.  For example, to translate ", "\nby 5 units in the direction of the vector (1,1,1), you could say", "There are no functions for changing the scaling transform.  But\nyou can change the object's rotation with the functions ", "(", "), \n", "(", "), and ", "(", ") to rotate the object \nabout the coordinate axes. (Remember that angles are measured in radians.)\nCalling ", "(", ") is not the same as adding ", "\nonto the value of ", ", since it can change the ", " and\n", " rotations as well as the ", " rotation.", "There is also a function ", "(", "), where ", " is a\n", ".  This function rotates the object through the\nangle ", " about the vector (that is, about the line between the origin\nand the point given by ", ").  The ", " must be a normalized vector.", "I should emphasize that the translation and rotation functions modify the\n", " and ", " properties of the object.  That is, they apply\nin object coordinates, not world coordinates, and they are applied when the object is\nrendered, in the order scale, then rotate, then translate.  For example, a rotation is\nworld coordinates can change the position of an object, if it is not positioned\nat the origin.  However, changing the value of the ", " property of\nan object will never change its position.", "(Rotation is actually even more complicated.  The rotation of an object, ", ",\nis actually represented by the property ", ", not by the property\n", ".  ", " are mathematical objects\nthat are often used in computer graphics as an alternative to ", ",\nto represent rotations.  However, when you change one of the properties\n", " or ", ", the other is automatically updated\nto make sure that both properties represent the same rotation.  So, we don't need\nto work directly with the quaternions.)", "There is one more useful method for setting the rotation:  ", "(", "),\nwhich rotates the object so that it is facing towards a given point.  \nThe parameter, ", ", is a ", ", which must be expressed\nin the object's own local coordinate system.  (For an object that has no parent, or whose\nancestors have no modeling transformations, that will be the same as ", ".)\nThe object is also rotated so that its \"up\" direction is equal to the value of the property\n", ", which by default is (0,1,0).  This function can be used with any object, but it\nis most useful for a camera.   \n", "Although it is possible to create mesh objects by listing their vertices and faces,\nit would be difficult to do it by hand for all but very simple objects.  It's much\neasier, for example, to design an object in an interactive modeling program\nsuch as ", " (", ").  ", " has utility functions for loading \nmodels from files, and it comes with scripts that can be used to\nexport objects from Blender and other 3D modeling programs to a\nfile format that ", " can read.  (In the ", " download,\nlook in the ", " folder inside the ", " folder.)", "\n", " has its own file format, in which models are specified\nusing ", ", a common format for representing JavaScript objects.\nThis is the file format that is produced\nby the export scripts.  The class ", "\ncan be used to read model descriptions from such files.  There are some\nother loaders, which work with other file formats, but I will only discuss \n", " here.", "If ", " is an object of type ", ",\nyou can use its ", "() method to start the process of loading a model:", "The first parameter is a URL for the file that contains the model.\nJSON models are stored as actual JavaScript\ncode, so the file will usually have a name ending in\u00a0\".js\".  The second parameter, ", ",\nis  a function that will be called when the loading is done.  The loading\nis asynchronous; ", "() starts the process and returns immediately.  It is\nthe responsibility of the callback function to use the data from the file to create a\n", " ", " and to add it to the scene.  \nThe callback function takes two parameters,\n", " and ", ", which contain the information needed to create\nthe object; the parameters represent the data that has been read from the file.\nThe ", " parameter is an array of materials meant to be used\nwith the material type ", ".  This allows\ndifferent parts of the model to have different materials. If the length of the\n", " array is one, then you can simply use ", "[0] as the\nmaterial for the object.  (Of course, you could also just use your own material\ninstead of the material from the file.)", "Here, then, is a pair of functions that can be used to load a JSON model\nfrom a specified url and add it to the scene (although in general, you probably\nwant to do something more complicated with the object):", "The sample program ", "\nuses ", " to read model descriptions.  It can display seven\ndifferent objects.  I created one of the objects using Blender. The other objects\ncame from the the ", " download.  The program uses a simple white material on all of the\nmodels.", "A JSON model can include texture coordinates, and its materials can use image textures.\nIf you use a JSON model with textures, they should be in the same folder\nas the file that contains the model. ", "I'll also mention that JSON models can define \nsimple keyframe animations. To do this,\nthe file includes alternative sets of vertices for each keyframe (", " calls them\n\"morph targets\").  There is a class named ", " that\ncan be used to run the animation.  I won't discuss it further here, but this demo is an\nexample:", "\n", "\n"], "chapter_title": "Three.js: A 3D Scene Graph API", "id": 5.2}, {"section_title": "Transformations in 3D", "chapter_id": "Chapter 7", "section_id": "Section 7.1", "content": ["We have already seen in ", " how to draw ", "\nusing WebGL, and how to implement 2D transformations.  Drawing primitives is the same in 3D,\nexcept that there are three coordinates per vertex instead of two.  Transformations in 3D are also\n similar\nto 2D, but for transformations the increase in complexity that comes with the third dimension is\nsubstantial.  This section covers the geometric side of 3D graphics with WebGL.  In the\n", ", we will move on to the question of \nlighting and materials.", "But before we begin working more seriously with WebGL, it will be nice to have a better\nway to include shader source code on a web page.  Up until now, I have stored the\ncode in JavaScript string literals.  That format is hard to read and very hard to edit.\nThere is another technique that is often used:  Put the GLSL shader source code inside\n", " elements.  Here is an example for a vertex shader:", "This relies on the fact that a web browser will not recognize the ", " listed in\nthe ", " element, so it will not try to execute the script.  However,\nit does store the content of the ", " element in the ", " data structure that\nrepresents the web page.  The content can be retrieved as a string using the standard \nDOM API.  I won't explain the API functions that are used, but\nhere is a function that takes the ", " of the script element as its parameter and returns a\nstring containing the text from inside the element:\n", "For the above example, I would call ", "(\"vshader\") and use the\nreturn value as the source code for the vertex shader.  The sample programs for this\nchapter use this technique.  For fragment shaders, I will use the\n", " \"x-shader/x-fragment\", but in fact the value of ", " doesn't\nmatter as long as it is something that web browsers will not recognize as\na legitimate scripting language.  (By the way, JavaScript ", " elements\ncan use \"text/javascript\" as the ", ", but that is the default value and\nis usually omitted.)", "Transformations are essential to computer graphics.  The WebGL API does not \nprovide any functions for working with transformations.  In ", ",\nwe used a simple JavaScript library to represent modeling transformations in 2D.\nThings get more complex in three dimensions.  For 3D graphics with WebGL,\nthe JavaScript side will usually have to create both a ", "\nand a ", ", and it will have to apply rotation, scaling, and\ntranslation to the modelview matrix, all without help from WebGL.  Doing so is much\neasier if you have a JavaScript library to do the work.  One commonly used library\nis ", ", a free JavaScript library for vector and\nmatrix math written by Brandon Jones and Colin MacKenzie IV.  It\nis available from ", ".\nThe download from that web site includes a JavaScript file\n", " that defines the library.  According to its license,\nthis file can be freely used and distributed.  You can find a copy in the\n", " folder in the web site download of this book.  My examples \nuse version 2.3 of the library.  Note that ", "\nis a \"minified\" JavaScript file, which is not meant to be human-readable.\nTo learn about the library, see the documentation on the web site\n(currently at ", " as I write this),\nor read the source files in the download.  (You can also read the full source for version 2.2, in\nhuman-readable form including comments, in the file ", ".", "The ", " API can be made can be made available for use\non a web page with a script element such as", "This assumes that ", " is in the same directory as\nthe web page.", "The ", " library defines what it calls \"classes\" named ", ", ", ", and ", "\nfor working with vectors of 2, 3, and 4 numbers.  It defines ", " for working with\n3-by-3 matrices and ", " for 4-by-4 matrices.  The names should not be confused\nwith the ", " types of the same names; ", " in entirely on the JavaScript side.\nHowever, a ", " ", " can be passed to a ", " program\nto specify the value of a GLSL ", ", and similarly for the other vector and matrix types.\n", "Each ", " class defines a set of functions for working with vectors and matrices.\nIn fact, however, although the documentation uses the term \"class,\"\n", " is not object-oriented.  Its classes are really just JavaScript objects, \nand the functions in its classes are what would be called static methods in Java.  Vectors and matrices\nare represented in ", " as arrays, and the functions in classes like ", "\nand ", " simply operate on those arrays.  There are no objects of type\n", " or ", " as such, just arrays of length 4 or 16 respectively.\nThe arrays can be either ordinary JavaScript arrays or ", "\nof type ", ".  If you let ", " create the\narrays for you, they will be ", ", but all ", "\nfunctions will work with either kind of array.  For example, if the ", " documentation\nsays that a parameter should be of type ", ", it is OK to pass either a ", "\nor a regular JavaScript array of three numbers as the value of that parameter.", "Note that it is also the case\nthat either kind of array can be used in WebGL functions such as ", "()\nand ", "().  ", " is designed to work with those functions.\nFor example, a ", " in ", " is an array of length 16 that holds the\nelements of a 4-by-4 array in ", ", the same format that\nis used by ", ".", "Each ", " class has a ", "() function which creates an array of\nthe appropriate length and fills it with default values.  For example,", "sets ", " to be a new ", " of length 16,\ninitialized to represent the identity matrix.  Similarly,", "creates a ", "  of length 3, filled with\nzeros.  Each class also has a function ", "(", ") that creates a copy of its\nparameter\u00a0", ".  For example:", "Most other functions do ", " create new arrays.  Instead, they modify the contents of their\nfirst parameter.  For example, ", "(", ") will modify ", " so that\nit holds the matrix product of ", " and ", ".  Each parameter must be a ", "\n(that is, an array of length 16) that already exists.  It is OK for some of the arrays\nto be the same.  For example, ", "(", ") has the effect of\nmultiplying ", " times ", " and modifying ", " so that it contains the answer.", "There are functions for multiplying a matrix by standard transformations such as\nscaling and rotation.  For example if ", " and ", " are ", " and ", " is a \n", ",\nthen ", "(", ") makes ", " equal to the product of ", " and\nthe matrix that represents translation by the vector\u00a0", ".  In practice, we will\nuse such operations mostly on a matrix that represents the modelview transformation.\nSo, suppose that we have a ", " named ", " that holds the current modelview\ntransform.  To apply a translation by a vector ", ", we can say", "This is equivalent to calling ", "(", ") in\nOpenGL.  That is, if we draw some geometry after this statement, using ", "\nas the modelview transformation, then the geometry will first be translated by\n", " and then will be transformed by whatever was the previous value\nof ", ".  Note the use of a vector to specify the translation in this\ncommand, rather than three separate parameters; this is typical of ", ".\nTo apply a scaling transformation with scale factors ", ", ", ", and ", ",\nuse", "For rotation, ", " has four functions, including three for the common\ncases of rotation about the ", ", ", ", or ", " axis.  The fourth rotation function\nspecifies the axis of rotation as the line from (0,0,0) to a point (", ").\nThis is equivalent to ", "\nUnfortunately, the angle of rotation in these functions\nis specified in radians rather than in degrees:", "These function allow us to do all the basic modeling and viewing transformations that we\nneed for 3D graphics.  To do hierarchical graphics, we also need to save and restore the\ntransformation as we traverse the scene graph.  For that, we need a stack. We\ncan use a regular JavaScript array, which already has ", " and ", " operations.  \nSo, we can create the stack as an empty array:", "We can then push a copy of the current modelview matrix onto the stack by saying", "and we can remove a matrix from the stack and set it to be the current modelview matrix with", "These operations are equivalent to ", "() and ", "()\nin OpenGL.", "The starting point for the modelview transform is usually a ", ".\nIn OpenGL, the function ", " is often used to set up the viewing transformation  \n(", ").  The ", " library has a \"lookAt\" function to do the same thing:", "Note that this function uses three ", " in place of the nine separate parameters\nin ", ", and it places the result in its first parameter instead of in a global variable.\nThis function call is actually equivalent to the two OpenGL commands", "So, you don't have to set ", " equal to the identity matrix before calling\n", ", as you would usually do in OpenGL.  However, you do have to create\nthe ", " matrix at some point before using ", ", such as by calling", "If you do want to set an existing ", " to the identity matrix,\nyou can do so with the ", " function.  For example,", "You could use this as a starting point if you wanted to compose the view transformation out of\nbasic scale, rotate, and translate transformations.", "Similarly, ", " has functions for setting up projection transformations.\nIt has functions equivalent to ", ", ", ", and ", "\n(", "),\nexcept that the field-of-view angle in ", " is given in radians rather\nthan degrees:", "As with the modelview transformation, you do not need to load ", " with the identity before calling one\nof these functions, but you must create ", " as a ", " (or an array of length 16).", "Of course, the point of making a projection and a modelview transformation is to use\nthem to transform coordinates while drawing primitives.  In WebGL, the transformation\nis usually done in the vertex shader.  The coordinates for a primitive are specified\nin ", ".  They are multiplied by the modelview transformation\nto covert them into ", " and then by the projection matrix\nto covert them to the final ", " that are actually used for\ndrawing the primitive.  Alternatively, the modelview and projection matrices can\nbe multiplied together to get a matrix that represents the combined transformation;\nobject coordinates can then be multiplied by that matrix to transform them\ndirectly into clip coordinates.", "In the shader program, coordinate transforms are ususally represented as\nGLSL ", " of type ", ".\nThe shader program can use either separate projection and modelview matrices or\na combined matrix (or both).  Sometimes, a separate modelview transform matrix is\nrequired, because certain lighting calculations are done in eye coordinates,\nbut here is a minimal vertex shader that uses a combined matrix:", "This shader is from the sample program ", ".\nThat program lets the user view a colored cube, using just basic color with no lighting applied.\nThe user can select either an orthographic or a perspective projection and can rotate the\ncube using the keyboard.  The rotation is applied as a modeling transformation consisting\nof separate rotations about the ", "-, ", "-, and ", "-axes. \nFor transformation matrices on \nthe JavaScript side, the program uses the ", " class from the ", " library\nto represent the projection, modelview, and combined transformation matrices:", "Only ", " corresponds to a shader variable.  The location of\nthat variable in the shader program is obtained during initialization using", "The transformation matrices are computed in the ", "() function, using\nfunctions from the ", " ", " class.  The\nvalue for ", " is sent to the shader program using\n", " before the primitives that make up the cube are drawn.\nHere is the code that does it:", "If separate modelview and projection matrices are used in the shader program,\nthen the modelview matrix can be applied to transform object coordinates to\neye coordinates, and the projection can then be applied to the eye coordinates\nto compute ", ".  Here is a minimal vertex shader that does that:", "\n", " are essential for \nlighting calculations (", ").  When a surface\nis transformed in some way, it seems that the normal vectors to that\nsurface will also change.  However, that is not true if the transformation is a translation.\nA normal vector tells what direction a surface is facing.  Translating\nthe surface does not change the direction in which the surface is facing, so\nthe nomal vector remains the same.\nRemember that a vector doesn't have a position, just a length and\na direction.  So it doesn't even make sense to talk about moving or\ntranslating a vector.", "Your first guess might be that the normal vector should be transformed\nby just the rotation/scaling part of the transformation.  The guess is that \nthe correct transformation is represented by the 3-by-3 matrix that is\nobtained by dropping the right column and the bottom row\nfrom the 4-by-4 coordinate transformation matrix. \n(The right column represents the translation part of the transformation,\nand the bottom row is only there because implementing translation in a matrix\nrequires the use of ", " to represent vectors.\nNormal vectors, where translation is not an issue, do not use homogeneous coordinates.)\nBut that can't be correct in all cases.  Consider, for example, a ", ".\nAs this illustration shows, if the normal vectors to an object are subjected\nto the same shear transformation as the object, the resulting vectors\nwill not be perpendicular to the object:", "\n", "Nevertheless, it is possible to get the correct transformation matrix for normal\nvectors from the coordinate transformation matrix.  It turns out that you need to\ndrop the fourth row and the fourth column and then take something called the\n\"inverse transpose\" of the resulting 3-by-3 matrix.  You don't need to know\nwhat that means or why it works. The ", " library will compute it\nfor you.  The function that you need is ", ", and it is\ndefined in the ", " class:", "In this function call, ", " is the ", " that\nrepresents the transformation that is applied to coordinates, and ", " is\na ", " that already exists.  This function computes the inverse transpose\nof the rotation/scale part of ", " and places the answer in\n", ".  Since we need normal vectors for lighting calculations, and\nlighting calculations are done in eye coordinates, the coordinate transformation that\nwe are interested in is usually the modelview transform.", "The normal matrix should be sent to the shader program, where it is needed to\ntransform normal vectors for use in lighting calculations.  Lighting requires unit\nnormal vectors, that is, normal vectors of length one.  The normal matrix does not\nin general preserve the length of a vector to which it is applied, so it will\nbe necessary to normalize the transformed vector.  GLSL has a built-in function for\nnormalizing vectors.  A vertex shader that implements lighting might take the form:", "We will look at several specific examples in the ", ".", "Computer graphics is a lot more interesting when there is user interaction.  The 3D\nexperience is enhanced considerably just by letting the user rotate the scene, to view\nit from various directions.  The ", "\nexample lets the user rotate the scene using the keyboard.  But using the mouse for rotation\ngives the user much better control.  I have written two JavaScript classes,\n", " and ", ",\nto implement two different styles of rotation-by-mouse.", "The ", " class is defined in the file \n", ".  To use it on a web page, you need to include that file\nin a ", " tag, and you need to create an object of type ", ":", "The first parameter must be a ", " ", " element.  It should be the\ncanvas where WebGL renders the scene. The ", " constructor\nadds a listener\nfor mouseDown events to the canvas.  The second parameter to the constructor is optional.  \nIf it is defined, it must be a function.  The function is called, with no parameters, each time the rotation\nchanges.  Typically, the callback function is the function that renders the image in the\ncanvas.  The third parameter is also optional.  If defined, it must be a non-negative number.\nIt gives the distance of the viewer from the center of rotation.  The default value is zero,\nwhich can be OK for an orthographic projection but is usually not correct.", "A ", " keeps track of a viewing transformation that changes as\nthe user rotates the scene.  The most important function is ", "().\nThis function returns an array of 16 numbers representing the matrix for the viewing transformation \nin column-major order.  The matrix can be sent directly to the shader program using ", ",\nor it can be used with functions from the ", " library as\nthe initial value of the modelview matrix.", "The sample program ", " is an example\nof using a ", ".  The program uses a perspective projection defined by the\n", " function", "The ", " for the rotator has to be between the ", " and ", "\ndistances in the projection.  Here, ", " is 8 and ", " is 12, and the\n", " can be set to 10.  The rotator is created during initialization using the statement", "In the ", "() function, the viewing transformation is obtained from the rotator\nbefore drawing the scene.  There is no modeling transformation in this program.  The modelview matrix\nis multiplied by the projection matrix using a ", " function, and the\ncombined transformation matrix is sent to the shader program:", "That's really all that you need to know if you just want to use ", "\nin your own programs.  I have also written an alternative rotator class,\n", ", which is defined in the JavaScript file\n", ".  A ", "\ncan be used in the same way as a ", ".\nThe main difference is that a ", " allows completely\nfree rotation while a ", " has the constraint that\nthe ", "-axis will always remain vertical in the image.", "The sample program ", " uses a\n", ", but is otherwise identical to the ", "\nexample. The following demo lets you try out both types of rotator.  A ", "\nis used for the cube on the left, and a ", " \nis used on the right:", "\n", "\n", "By default, the center of rotation for either type of rotator is the origin, \neven if the origin is not at the center of the image.  However, you can change the \ncenter of rotation to be the point (", ") by calling ", "([a,b,c]).  \nThe parameter must be an array of three numbers.  Typically, (", ") would be the\npoint displayed at the center of the image (the point that would be the view reference\npoint in ", ").", "You don't need to understand the mathematics that is used to implement a\nrotator.  In fact, ", " uses some advanced\ntechniques that I don't want to explain here.  However, ", "\nis, well, more simple, and it's nice to know how it works. So, I will explain how the\nview transformation is computed for a ", ".\nActually, it will be easier to think in terms of the corresponding modeling\ntransformation on the scene as a whole.  (Recall the equivalence between\nmodeling and viewing (", ").)", "The modeling transformation includes a rotation about the ", "-axis followed by a rotation about\nthe ", "-axis.  The sizes of the rotations change as the user drags the mouse.  \nLeft/right motion controls the rotation about the ", "-axis, while up/down motion\ncontrols the rotation about the ", "-axis.  The rotation\nabout the ", "-axis is restricted to lie in the range \u221285 to 85 degrees.  Note that\na rotation about the ", "-axis followed by a rotation about the ", "-axis always leaves the\n", "-axis pointing in a vertical direction when projected onto the screen.", "Suppose the center of rotation is (", ") instead of (0,0,0).  To implement that,\nbefore doing the rotations, we need to translate the scene to move the point (", ") \nto the origin.  We can do that with a translation by (", ").\nThen, after doing the rotation, we need to translate the origin back to the\npoint (", ").", "Finally, if the ", " is not zero, we need to push the scene ", "\nunits away from the viewer.  We can do that with a translation by (0,0,", ").\nIf ", " is the view distance, ", " is the rotation about the ", "-axis,\nand ", " is the rotation about the ", "-axis, then the sequence of modeling transformations\nthat we need to apply to the scene is as follows:", "Keeping in mind that modeling transformations are applied to objects in the opposite\nof the order in which they occur in the code, the view matrix could be created by\nthe following ", " commands:", "In fact, in my code, I create the view matrix directly, based on the matrices for the individual\ntransformations.  The 4-by-4 matrices for rotation and translation are given in \n", ".  The view matrix for a ", "\nis the matrix product of five translation and rotation matrices:", "\n", "It's actually not too difficult to implement the multiplication. See the JavaScript file,\n", ", if you are curious."], "chapter_title": "3D Graphics with WebGL", "id": 7.1}, {"section_title": "Three.js Basics", "chapter_id": "Chapter 5", "section_id": "Section 5.1", "content": ["\n", " is an object-oriented JavaScript library for 3D graphics.  It is an\nopen-source project created by Ricardo Cabello (who goes by the handle \"mr.doob\", \n", "), with contributions from other programmers.\nIt seems to be the most popular open-source JavaScript library for 3D web applications.  ", " uses concepts\nthat you are already familiar with, such as geometric objects, transformations, lights,\nmaterials, textures, and cameras.  But it also has additional features that build on \nthe power and flexibility of WegGL.", "You can download ", " and read the documentation at its main web site,\n", ".  The download is quite large,\nsince it includes many examples and support files.  In this book, I use Release 71 of the\nsoftware, from March 16, 2015.  The API has been fairly stable, so programs from this book \nare likely to continue to work with later versions.", "The core features of ", " are defined in a single large JavaScript file named\n\"three.js\", which can be found in a ", " directory in the download.\nThere is also a smaller \"minified\" version, ", ", that contains the same definitions\nin a format that is not meant to be human-readable.  (You can also find copies of these\nfiles as part of the web site download of this book, in the ", " folder\ninside the source folder.)  To use ", " on a web page,\nyou need to include one of the two scripts in a <script> element on the page.\nFor example, assuming that ", " is in the same folder as the web page,\nthen the script element would be:", "In addition to this core, the ", " download has a directory containing many\nexamples and a variety of support files that are used in the examples.  Although\nI will stick mainly to the core, I will also use a few of the extras and will\nnote their source when I do.", "\n", " works with the ", " \nelement, the same thing that we\nused for 2D graphics in ", ".  In many web browsers, in\naddition to its 2D Graphics API, a canvas also supports drawing in 3D\nusing WebGL, which is about as different as it can be from the 2D API.\nWebGL is not available in some browsers that support <canvas>.\nFor example, this is true in Internet Explorer 9 and 10. But WebGL is implemented in\nInternet Explorer\u00a011, as well as recent versions of Chrome, Safari, and Firefox.\nIt also works in browsers on some mobile devices.", "\n", " is an object-oriented ", " API.\n(See ", ".) The basic procedure is to build a scene graph\nout of ", " objects, and then to ", " an image\nof the scene it represents.  Animation can be implemented by modifying \nproperties of the scene graph between frames.", "The ", " library is made up of a large number of classes.\nThree of the most basic are\n", ", ", ",\nand ", ".  A\u00a0", " program\nwill need at least one object of each type.  Those objects are often stored in global variables", "Note that almost all of the ", " classes and constants that we will use \nare properties  of an object named ", ",\nand their names begin with \"", "\".  I will sometimes\nrefer to classes without using this prefix, and it is not usually used in the\n", " documentation, but the prefix must always be included in actual \nprogram code.", "A ", " object is a holder for all the objects that make up a 3D world, including\nlights, graphical objects, and possibly cameras.  It acts as a root node for the scene\ngraph.  A ", " is a special kind of object that represents a viewpoint from\nwhich an image of a 3D world can be made.   It represents a combination of a\n", " and a ", ".   \nA renderer is an object that can create an image from a scene graph.", "The scene is the simplest of the three objects.  A scene can be created as an object\nof type ", " using a constructor with no parameters:", "The function ", "(", ") can be used to add cameras, lights,\nand graphical objects to the scene. It is probably the only ", " function that you\nwill need to call.  The function ", "(", "), which removes an item\nfrom the scene, is also occasionally useful.", "There are two kinds of camera, one using ", " and one using\n", ".  They are represented by classes\n", " and\n", ", which are subclasses\nof ", ".  The constructors specify\nthe projection, using parameters that are familiar from OpenGL (see ", "):", "or", "The parameters for the orthographic camera specify the x, y, and z limits\nof the view volume, in ", "\u2014that is, in a coordinate system in which the\ncamera is at (0,0,0) looking in the direction of the negative ", "-axis, with the ", "-axis\npointing up in the view.  The  ", " and ", " parameters give the z-limits in terms\nof distance from the camera.  For an orthographic projection, ", " can be negative,\nputting the \"near\" clipping plane in back of the camera.\nThe parameters are the same as for the OpenGL function ", "(), except for reversing\nthe order of the two parameters that specify the top and bottom clipping planes.", "Perspective cameras are more common.  The parameters for the \nperspective camera come from the function ", "()\nin OpenGL's ", " library.  The first parameter determines the vertical extent of the\nview volume, given as an angle measured in degrees.  The ", " is the ratio\nbetween the horizontal and vertical extents; it should usually be set to the width of the\ncanvas divided by its height.  And ", " and ", " give the z-limits on the view\nvolume as distances from the camera.  For a perspective projection, \nboth must be positive, with ", " less\nthan ", ".  Typical code for creating a perspective camera would be:", "where ", " holds a reference to the <canvas> element\nwhere the image will be rendered.\nThe near and far values mean that only things between 1 and 100 units in front of the\ncamera are included in the image.  Remember that using an unnecessarily large value for\n", " or an unnecessarily small value for ", " can interfere \nwith the accuracy of the ", ".", "A camera, like other objects, can be added to a scene, but it does not have\nto be part of the scene graph to be used.  You might add it to the scene graph if\nyou want it to be a parent or child of another object in the graph.\nIn any case, you will generally want to apply a ", "\nto the camera to set its position and orientation in 3D space.\nI will cover that later when I talk about transformations more generally.", "\n", " actually has several renderer classes, which can render to different targets.\nThe core library includes a WebGL renderer and a canvas renderer.  The canvas renderer\ntranslates 3D graphics into the 2D canvas API that was covered in ", ".\nIt can be used as a fallback when WebGL is not available, but it doesn't implement\nmany of the more interesting 3D features, and it is generally much slower than the WebGL renderer.\nAmong the extras included in the ", "\ndownload is a renderer for ", " and even one that creates 3D graphics using\n", ".  However, this book will only use the WebGL renderer.", "A renderer that renders using WebGL is an instance of the class\n", ".  Its constructor has\none parameter, which is a JavaScript object containing settings that\naffect the renderer.  The settings you are most likely to specify are\n", ", which tells the renderer where to draw, and\n", ", which asks the renderer to use ", " if possible:", "Here, ", " would be a reference to the <canvas>\nelement where the renderer will display the images that it produces.\n(Note that the technique of having a JavaScript object as a parameter\nis used in many ", " functions.  It makes it possible to\nsupport a large number of options without requiring a long list of parameters that\nmust all be specified in some particular order.  Instead, you only need to\nspecify the options for which you want to provide non-default values,\nand you can specify those options by name, in any order.)", "The main thing that you want to do with a renderer is render an image.\nFor that, you also need a scene and a camera.\nTo render an image of a given ", " from the point of view of a given ", ",\ncall", "This is really the central command in a ", " application.", "(I should note that most of the examples that I have seen do not provide\na canvas to the renderer; instead, they allow the renderer to create it.\nThe canvas can then be obtained from the renderer and added to\nthe page.  Furthermore, the canvas typically fills the entire browser window.\nThe sample program ", " shows how to do that.\nHowever, all of my other examples use an existing canvas, with the renderer constructor shown above.)", "A ", " scene graph is made up of objects of type\n", " (including objects that belong to\nsubclasses of that class).  Cameras, lights, and visible objects are all\nrepresented by subclasses of ", ".\nIn fact, ", " itself is also a subclass of ", ".", "Any ", " contains\na list of child objects, which are also of type ", ".\nThe child lists define the structure of the scene graph.  If ", " and ", " are of type\n", ", then the method ", "(", ") adds\n", " to the list of children of ", ".  The method ", "(", ")\ncan be used to remove an object from the list.", "A ", " scene graph must, in fact, be a tree.  That is, every node\nin the graph has a unique parent node, except for the root node, which has no\nparent.  An ", ", ", ", has a property\n", " that points to the parent of ", " in the scene graph, \nif any.  You should never set this property directly.  It is set automatically when the node\nis added to the child list of another node.  If ", " already has a parent when it\nis added as a child of ", ", then ", " is first removed from the child list of\nits current parent before it is added to the child list of ", ".", "To make it easy to duplicate parts of the structure of a scene graph,\n", " defines a ", "() method.  This method\ncopies the node, including the recursive copying of the children of that node.\nThis makes it easy to include multiple copies of the same structure in a scene\ngraph:", "An ", ", ", ", has an associated transformation,\nwhich is given by properties ", ", ", ", and ", ".\nThese properties represent a modeling transformation to be applied to the object and its\nchildren when the object is rendered.  The object is first scaled, then rotated, then\ntranslated according to the values of these properties.  (Transformations are actually \nmore complicated than this, but we will keep things simple for now and will return\nto the topic later.)", "The values of ", " and ", " are objects of type\n", ".  A\u00a0", " represents a ", " or point in \nthree dimensions.  (There are similar classes ", "\nand ", " for vectors in 2 and 4 dimensions.)\nA ", " object can be constructed from three numbers\ngiving the coordinates of the vector:", "This object has properties ", ", ", ", and ", " representing\nthe coordinates.  The properties can be set individually; for example: ", ".\nThey can also be set together, with the method ", "(", ").\nThe ", " class also has many methods implementing vector operations such as\naddition, dot product, and cross product.", "For an ", ", the properties ", ", \n", ", and ", " give the amount of scaling of the object in\nthe x, y, and z directions.  The default values, of course, are\u00a01.  Calling", "means that the object will be subjected to a uniform scaling factor of 2 \nwhen it is rendered.  Setting", "will shrink it to half-size in the y-direction only (assuming that\n", " and ", " still have their default values).", "Similarly, the properties ", ", ", ", and\n", " give the translation amounts that will be applied to the object\nin the x, y, and z directions when it is rendered.  For example, since a camera is \nan ", ", setting", "means that the camera will be moved from its default position at the origin to the\npoint (0,0,20) on the positive ", "-axis.  This modeling transformation on the camera\nbecomes a viewing transformation when the camera is used to render a scene.", "The object ", " has properties ", ", \n", ", and ", " that represent rotations about\nthe x-, y-, and z-axes.   The angles are measured in radians.\nThe object is rotated first about the x-axis, then  about the y-axis, then about the z-axis. \n(It is possible to change this order.)  The value of ", " is not\na vector.  Instead, it belongs to a similar type, ", ",\nand the angles of rotation are called ", ".", "A visible object in ", " is made up of either\npoints, lines, or triangles.  An individual object corresponds to\nan OpenGL ", " such as\n", ", ", ", or ", "\n(see ", ").  There are three classes to\nrepresent these possibilities:  ", ",\n", ", and ", ".", "A visible object is made up of some geometry plus a ", " that\ndetermines the appearance of that geometry.  In ", ", the\ngeometry and material of a visible object are themselves represented by\nJavaScript classes ", " and\n", ".", "An object of type ", " has a property named\n", " that is an array of ", ".  When creating\na geometry by hand, we can simply push vectors onto that array.  For example,\nsuppose that we want to represent a cloud of 1000 random points inside the\nsphere of radius one centered at the origin:", "To make this point cloud into a visible object, we also need a material.\nFor a point cloud, we can use an object of type ", ",\nwhich is a subclass of ", ".  The material can specify\nthe color and the size of the points, among other properties:", "The parameter to the constructor is a JavaScript object whose properties are used\nto initialize the material.  With the ", " property set to ", ",\nthe size is given in pixels; if it is ", ", then the size is\nscaled to reflect distance from the viewer.  \nIf the ", " is omitted, a default value of white is used.\nThe default for ", " is\u00a01 and for ", " is ", ". \nThe parameter can be omitted entirely, to use all the defaults.\nA ", "\nis not affected by lighting; it simply shows the color specified by its ", " property.", "It is also possible to assign values to properties of the material after the object has\nbeen created.  For example,", "Once we have the geometry and the material, \nwe can use them to create the visible object, of type ", ",\nand add it to a scene:", "This\ndemo shows a point cloud.  You can animate the cloud, and you can control the size\nand number of points.  Note that points are rendered as squares.\n", "\n", "\n", "The color parameter in the above material was specified by the string \"yellow\".  Colors\nin ", " are stored as values of type ", ".\nThe class ", " represents an ", ".\nA ", " object ", " has properties ", ", ", ",\nand ", " giving the red, blue, and green color components as floating point numbers in the\nrange from 0.0 to 1.0. Note that there is no ", "\ncomponent; ", " handles transparency separately from color.", "There are several ways to construct a ", " object.  The constructor\ncan take three parameters giving the RGB components.  It can take a single string parameter\ngiving the color as a CSS color string, like those used in the 2D canvas graphics API.\nExamples include \"white\", \"red\", \"rgb(255,0,0)\", and \"#FF0000\".  Or the color constructor can take\na single integer parameter in which each color component is given as an eight-bit field\nin the integer.  Usually, an integer that is used to represent a color in this way is\nwritten as a hexadecimal literal, beginning with \"0x\".  Examples include 0xff0000 for\nred, 0x00ff00 for green, 0x0000ff for blue, and 0x007050 for a dark blue-green.  Here are some\nexamples of using color constructors:", "In many contexts, such as the ", " constructor,\n", " will accept a string or integer where a color\nis required; the string or integer will be fed through the ", " \nconstructor.  As another example, a ", " object has a\n\"clear color\" property that is used as the background color when the renderer renders a\nscene.  This property could be set using any of the following commands:", "An object of type ", " can represent\neither a line strip or a set of disconnected line segments\u2014what would be\ncalled ", " or ", " in OpenGL.  Unfortunately,\nthere is no option corresponding to ", ".  For a triangle, we\nneed a list of four vertices to be connected into a line strip, where the last\nvertex is equal to the first. These must be added to the vertex array of the line's geometry object.  For\nexample:", "Alternatively, we could create a\nnew array containing the vertices and assign it to the property ", ":", "We will also need a material.  For lines, the material can be represented\nby an object of type ", ".\nAs usual, the parameter for the constructor is a JavaScript\nobject, whose properties can include ", " and ", ".  For example:", "With the geometry and material in hand, we can create a ", "\nobject.  The constructor takes the geometry and material as parameters, along with\none of the constants ", " or ", "  to indicate\nwhether a line strip or disconnected line segments should be drawn:", "If the third parameter is omitted, it defaults to ", ".\nIf the second parameter is omitted, it defaults to a ", "\nwith a random color.", "Let's look at one more option:  using a different color for each vertex.\nTo do this, you need to add vertex colors to the ", "\nobject.  The array of vertex colors is stored in the ", " property of the geometry.\nFurthermore, to tell ", " to use the colors from the geometry color array,\nyou need to set the ", " property of the\n", " to ", ".  Let's\nmake a triangle with a red, a blue, and a green vertex\u2014and add the triangle\nto a scene so that we can see it on the screen:", "This produces the image:", "\n", "The \"Basic\" in ", "\nindicates that this material uses basic colors that do not require lighting\nto be visible and are not affected by lighting.  This is generally what\nyou want for lines.", "A mesh object in ", " corresponds to the OpenGL primitive\n", ".  \nThe geometry object for a mesh must specify the triangles,\nin addition to the vertices.  We will see later how to do that. However,\n", " comes with classes to represent common mesh geometries, such as a sphere,\na cylinder, and a torus.  For these built-in classes, you just need to\ncall a constructor to create the geometry.  For example, the class \n", "\nrepresents the geometry for a cylinder, and its constructor takes the form", "The geometry created by this constructor represents an approximation for a cylinder\nthat has its axis lying along the ", "-axis.  It extends from\nfrom \u2212", "/2 to ", "/2 along that axis.\nThe radius of its circular top is ", " and of its\nbottom is ", ".  The two radii\ndon't have to be the same; when the are different, you get a truncated cone rather\nthan a cylinder as such.  Use a value of zero for ", " to get an actual cone.\nThe parameters ", " and ", "\ngive the number of subdivisions around the circumference of the cylinder and\nalong its length respectively\u2014what are called slices and stacks in the \n", " library for OpenGL.\nThe parameter ", " is a boolean value that indicates\nwhether the top and bottom of the cylinder are to be drawn; use the value ", "\nto get an open-ended tube.  Finally, the last two parameters allow you to make\na partial cylinder.  Their values are given as angles, measured in radians, about \nthe ", "-axis.  Only the part of the cylinder beginning at ", "\nand ending at ", " plus ", " is rendered.  For example,\nif ", " is ", ", you will get a half-cylinder.", "The large number of parameters to the constructor gives a lot of flexibility.\nThe parameters are all optional but it's probably best to\ninclude at least the first three since the default size is rather large.\nAnd the default for ", " is 8, which gives a poor approximation\nfor a smooth cylinder.", "Other standard mesh geometries are similar.  Here are some constructors, listing\nall parameters (but keep in mind that most of the parameters are optional):", "The class ", "\nrepresents the geometry of a rectangular box centered at the origin.  Its constructor\nhas three parameters to give the size of the box in each direction; these are ", "\noptional.  The last three parameters are optional. They give the number of subdivisions\nin each direction, with a default of\u00a01; values greater than one will cause the faces\nof the box to be subdivided into smaller triangles.", "The class ", " represents the\ngeometry of a rectangle lying in the ", "-plane, centered at the origin.\nIts parameters are similar to those for a cube.  A ", "\nrepresents an annulus, that is, a disk with a smaller disk removed from its center.\nThe ring lies in the ", "-plane, with its center at the origin.\nYou should always specify the inner and outer radii of the ring.", "For ", ", all parameters\nare optional.  The constructor creates a sphere centered at the origin,\nwith axis along the ", "-axis.  The first parameter, which gives the radius of the sphere,\nhas a default of 50 (which is usually too big).  The next two parameters\ngive the numbers of slices and stacks (with default values that are usually too small).\nThe last four parameters allow you to make a piece of a sphere; the default values\ngive a complete sphere.  The four parameters are angles measured in radians.  ", "\nand ", " are measured in angles around the equator and give the extent in\nlongitude of the spherical shell that is generated.  For example,\n", "creates the geometry for the \"western hemisphere\" of a sphere.  The\nlast two parameters are angles measured along a line of latitude from the north pole\nof the sphere to the south pole.  For example, to get the sphere's \"northern hemisphere\":", "For ", ", the constructor creates a torus\nlying in the ", "-plane, centered at the origin, with the ", "-axis passing through\nits hole.  The parameter ", " is the distance from\nthe center of the torus to the center of the torus's tube, while ", "\nis the radius of the tube.  The next two parameters give the number of subdivisions in\neach direction.  The last parameter, ", ", allows you to make just part of a torus.\nIt is an angle between 0 and ", ", measured along the circle at the center of the\ntube.\n", "There are also geometry classes representing the \n", ":\n", ",\n", ",\n", ", and\n", ".  (For a cube use a\n", ".)  The constructors for these four classes\ntake two parameters.  The first specifies the size of the polyedron, with a default\nof 1.  The size is given as the radius of the sphere that contains the polyhedron.\nThe second parameter is an integer called ", ".  The default value, 0, gives\nthe actual regular polyhedron.  Larger values add detail by adding additional faces.\nAs the detail increases, the polyhedron becomes a better approximation for a sphere.\nThis is easier to understand with an illustration:\n", "\n", "The image shows four mesh objects that use icosahedral geometries with\ndetail parameter equal to 0, 1, 2, and 3.", "To create a mesh object, you need a material as well as a geometry.  There are\nthree kinds of material suitable for mesh objects:  ", ",\n", ", and\n", ".  The first represents a color that\nis not affected by lighting; it looks the same whether or not there are lights in the\nscene. The other two represent materials that need to be lit to be seen.  They implement\nmodels of lighting known as ", " and\n", ".  The major difference\nis that ", " has a specular color but\n", " does not.  Both can have diffuse,\nambient, and emissive colors.  For all three material classes, the constructor has one\nparameter, a JavaScript object that specifies values for properties of the\nmaterial.  For example:\n", "This example shows the five color parameters for a Phong material.  The parameters\nhave the same meaning as the five ", " properties in OpenGL (", ").\nA Lambert material lacks ", " and ", ", and a basic mesh\nmaterial has only the ", " parameter.", "There are a few other material properties that you might need to set in the constructor.\nExcept for ", ", these apply to all three kinds of mesh material:", "As an example, let's make a shiny, blue-green, open, five-sided tube with flat sides:", "You can use the following\ndemo to view several ", " mesh objects, using a variety of geometries\nand materials.  Drag your mouse on the object to rotate it.\nYou can also explore the level of detail for the regular polyhedron\ngeometries.", "\n", "\n", "The demo can show a wireframe version of an object overlaid on a solid version.\nIn ", ", the wireframe and solid versions are actually two objects that\nuse the same geometry but different materials.  Drawing two objects at exactly the\nsame depth can be a problem for the ", ".  You might remember from \n", " that OpenGL uses polygon offset to solve the problem.\nIn ", ", you can apply polygon offset to a material.  In the demos, this\nis done for the solid materials that are shown at the same time as wireframe materials.  \nFor example,", "The settings shown here for ", ", ", ", and\n", " will increase the depth of the object that uses this material\nslightly so that it doesn't interfere with the wireframe version of the same object.", "One final note: You don't always need to make new materials and geometries\nto make new objects.  You can reuse the same materials and geometries in multiple\nobjects.", "Compared to geometries and materials, lights are easy!  ", "\nhas several classes to represent lights.  Light classes are subclasses of ", ".\nA light object can be added to a scene and will then illuminate objects in the scene.\nWe'll look at ", ",\n", ",\n", ", and ", ".", "The class ", " represents\nlight that shines in parallel rays from a given direction, like the light from\nthe sun.  The ", " property of a directional light gives the direction\nfrom which the light shines.  (This is the same ", " property, of type\n", ", that all scene graph objects have, but the meaning\nis different for directional lights.)  Note that the light shines from the given position \ntowards the origin.  The default position is\nthe vector (0,1,0), which gives a light shining down the ", "-axis.  The\nconstructor for this class has two parameters:", "where ", " specifies the color of the light, given as a ", " object,\nor as a hexadecimal integer, or as a CSS color string.   Lights do not have separate\ndiffuse and specular colors, as they do in OpenGL.  The ", " is a non-negative number\nthat controls the brightness of the light, with larger values making the light\nbrighter.  A light with intensity zero gives no light at all.  The parameters\nare optional.  The default for ", " is white (0xffffff) and for ", " \nis\u00a01.  The intensity can be greater than 1, but values less than 1 are usually preferable,\nto avoid having too much illumination in the scene.", "Suppose that we have a camera on the positive ", "-axis,\nlooking towards the origin, and we would like a light that shines in the same\ndirection that the camera is looking.  We can use a directional light whose\nposition is on the positive ", "-axis:", "The class ", "  represents a light\nthat shines in all directions from a point.  The location of the point is given\nby the light's ", " property.  The constructor has three optional\nparameters:", "The first two parameters are the same as for a directional light,\nwith the same defaults.  The ", " is a non-negative number.\nIf the value is zero\u2014which is the default\u2014then the illumination from the\nlight extends to infinity, and intensity does not decrease with distance.  While\nthis is not physically realistic, it generally works well in practice.  If ", "\nis greater than zero, then the intensity falls from a maximum value at the light's position\ndown to an intensity of zero at a distance of ", " from the light;\nthe light has no effect on objects that are at a distance greater than ", ".  \nThis falloff of light intensity with distance is referred to as\n", " of the light source.", "A third type of light is ", ".\nThis class exists to add ambient light to a scene.  An ambient light has\nonly a color:", "Adding an ambient light object to a scene adds ambient light of the specified color\nto the scene.  The color components of an ambient light should be rather\nsmall to avoid washing out colors of objects.", "For example, suppose that we would like a yellowish point light at (10,30,15) whose illumination\nfalls off with distance from that point, out to a distance of 100 units.  We also want to\nadd a bit of yellow ambient light to the scene:", "The fourth type of light, ", ", is something new\nfor us.  An object of that type represents a ", ", \nwhich is similar to a point light, except that instead of shining in all directions, a spotlight\nonly produces a cone of light.  The vertex of the cone is located at the position of the light.\nBy default, the axis of the cone points from that location towards the origin (so unless you\nchange the direction of the axis, you should move the position of the light away from the origin).\nThe constructor adds two parameters to those for a point light:", "The ", " is a number between 0 and ", "\nthat determines the size of the cone of light.  It is the angle between the\naxis of the cone and the side of the cone.  The default value is ", ".\nThe ", " is a non-negative number that determines how fast the intensity\nof the light decreases as you move from the axis of the cone toward the side.  The default\nvalue, 10, gives a reasonable result. An ", " of zero gives no\nfalloff at all, so that objects at all distances from the axis are evenly illuminated.", "The technique for setting the direction of a ", " spotlight is a little\nodd, but it does make it easy to control the direction.\nAn object ", " of type ", " has a property\nnamed ", ".  The target is a scene graph node.  The cone of light from\nthe spotlight is pointed in the direction from spotlight's position towards the\ntarget's position.  When a spotlight is first created, its target is a new, empty \n", ", with position at (0,0,0).  However, you can set the target to be\nany object in the scene graph, which will make the spotlight shine towards that object.\nFor ", " to calculate the spotlight direction, a target whose position is\nanything other than the origin must actually be a node in the scene graph.\nFor example, suppose we want a spotlight located at the point (0,0,5) and pointed\ntowards the point (2,2,0):", "The interaction of spotlights with material illustrates an important different between\nPhong and Lambert shading.  With a ", ", the \n", " is applied at the vertices of a primitive, and the vertex colors computed by that\nequation are then interpolated to calculate colors for the pixels in the primitive.\nWith ", ", on the other hand, the lighting equation is\napplied at each individual pixel.  The following illustration shows what can happen when we shine\na spotlight onto a square that was created using ", ":\n", "\n", "For the two squares on the left, the square was not subdivided; it is made up of two\ntriangular faces.  The square at the left, which uses Phong shading, shows the expected spotlight\neffect. The spotlight is pointed at the center of the square.  Note how the illumination\nfalls off with distance from the center.  When I used the same square and spotlight\nwith Lambert shading in the second picture, I got no illumination at all!\nThe vertices of the the square lie outside the cone of light from the spotlight.\nWhen the lighting equation is applied, the vertices are black, and the black color\nof the vertices is then applied to all the pixels in the square.", "For the third and fourth squares in the illustration, plane geometries with horizontal\nand vertical subdivisions were used with Lambert shading.  In the third picture, the\nsquare is divided into 18 triangles, and the lighting equation is applied only at\nthe vertices of those triangles.  The result is still a very poor approximation for\nthe correct illumination.  In the fourth square, with 10 subdivisions, the approximation\nis better but still not perfect.", "The upshot is, if you want an object to be properly illuminated by a spotlight, use a\n", " on the object, even if it has no specular reflection.\nA ", " will only give acceptable results if the\nfaces of the object are very small.", "In the rest of this chapter, we will go much deeper into ", ", but you\nalready know enough to build 3D models from basic geometric objects.  An example\nis in the sample program ", ", which shows\na very simple model of a car driving around the edge of a cylindrical base.  The car\nhas rotating tires.  The diskworld is shown in the picture on the left below.\nThe picture on the right shows one of the axles from the car, with a tire on\neach end.", "\n", "I will discuss some of the code that is used to build this models.  If you want\nto experiment with your own models, you can use the program ", "\nas a starting point.", "To start with something simple, let's look at how to make a tree from a\nbrown cylinder and a green cone.  I use an ", "\nto represent the tree as a whole, so that I can treat it as a unit.  The\ntwo geometric objects are added as children of the ", ".", "The trunk is a cylinder with height equal to 1.  Its axis lies along the ", "-axis,\nand it is centered at the origin.  The plane of the diskworld lies in the ", "-plane,\nso I want to move the bottom of the trunk onto that plane.  This is done by setting the\nvalue of ", ", which represents a translation to be applied to\nthe trunk. Remember that objects have their own modeling coordinate system.\nThe properties of objects that specify transformations, such as ", ", \ntransform the object in that coordinate system.  In this case, the trunk is part of\na larger, compound object that represents the whole tree.  When the scene is\nrendered, the trunk is first transformed by its own modeling transformation.\nIt is then further transformed by any modeling transformation that is applied to\nthe tree as a whole.  (This type of ", " was first \ncovered in ", ".)", "Once we have a tree object, it can be added to the model that represents the\ndiskworld.  In the program, the model is an object of type ", "\nnamed ", ".  The model will contain several trees, but the trees\ndon't have to be constructed individually.  I can\nmake additional trees by cloning the one that I have already created.  For example:", "This adds two trees to the model, with different sizes and positions.  When the\ntree is cloned, the clone gets its own copies of the modeling transformation properties,\n", " and ", ".  Changing the values of those properties in the\noriginal tree object does not affect the clone.", "Lets turn to a more complicated object, the axle and wheels.  I start by creating\na wheel, using a torus for the tire and using three copies of a cylinder for the spokes.  \nIn this case, instead of making a new ", " to hold all \nthe components of the wheel, I add the cylinders as children of the torus.  Remember that any screen\ngraph node in ", " can have child nodes.", "Once I have the wheel model, I can use it along with one more cylinder\nto make the axle. For the axle, I use a cylinder lying along the ", "-axis.\nThe wheel lies in the ", "-plane.  It is facing in the correct direction,\nbut it lies in the center of the axle.  To get it into its correct position at\nthe end of the axle, it just has to be translated along the ", ".", "Note that for the second wheel, I add the original wheel model rather than a\nclone.  There is no need to make an extra copy.  With the ", " in hand,\nI can build the car from two copies of the axle plus some other components.", "The diskworld can be animated.  To implement the animation, properties of\nthe appropriate scene graph nodes are modified before each frame of the animation\nis rendered.  For example, to make the wheels on the car rotate, the rotation\nof each axle about its ", "-axis is increased in each frame:", "This changes the modeling transformation that will be applied to the axles\nwhen they are rendered.  In its own coordinate system, the central axis of an axle lies along\nthe ", "-axis.  The rotation about the ", "-axis rotates the axle,\nwith its attached tires, about its axis.", "For the full details of the sample program, see the ", "."], "chapter_title": "Three.js: A 3D Scene Graph API", "id": 5.1}, {"section_title": "Hardware and Software", "chapter_id": "Chapter 1", "section_id": "Section 1.3", "content": ["We will be using OpenGL as the primary basis for 3D graphics programming.\nThe original version of OpenGL was released in 1992 by a company named\nSilicon Graphics, which was known for its graphics workstations\u2014powerful,\nexpensive computers designed for intensive graphical applications.  (Today,\nyou probably have more graphics computing power on your smart phone.)  OpenGL\nis supported by the graphics hardware in most modern computing devices, including\ndesktop computers, laptops, and many mobile devices.  This section will give\nyou a bit of background about the history of OpenGL and about the graphics \nhardware that supports it.", "In the first desktop computers, the contents of the screen were managed\ndirectly by the ", ".  For example, to draw a line segment on the screen, the CPU\nwould run a loop to set the color of each pixel that lies along the line.\nNeedless to say, graphics could take up a lot of the CPU's time.  And graphics\nperformance was very slow, compared to what we expect today.  So what has changed?\nComputers are much faster in general, of course, but the big change is that\nin modern computers, graphics processing is done by a specialized component\ncalled a ", ", or Graphics Processing Unit.  A GPU includes processors\nfor doing graphics computations; in fact, it can include a large number of such\nprocessors that work in parallel to greatly speed up graphical operations.  \nIt also includes its own dedicated memory for storing things like images and \nlists of coordinates.  GPU processors have very fast\naccess to data that is stored in GPU memory\u2014much faster than their access to data\nstored in the computer's main memory.", "To draw a line or perform some other graphical operation, the CPU simply has to\nsend commands, along with any necessary data, to the GPU, which is responsible\nfor actually carrying out those commands.  The CPU offloads most of the graphical\nwork to the GPU, which is optimized to carry out that work very quickly.\nThe set of commands that the GPU understands make up the ", "\nof the GPU.  OpenGL is an example of a graphics API, and most GPUs support\nOpenGL in the sense that they can understand OpenGL commands, or at least\nthat OpenGL commands can efficiently be translated into commands that the\nGPU can understand.", "OpenGL is not the only graphics API.  The best-known alternative is probably \nDirect3D, a 3D graphics API used for Microsoft Windows.  OpenGL is more widely\navailable, since it is not limited to Microsoft, but Direct3D is supported by\nmost graphics cards, and it has often introduced new features earlier than OpenGL. ", "I have said that OpenGL is an API, but in fact it is a series of APIs that have\nbeen subject to repeated extension and revision.  The current version, in early\n2015, is 4.5, and it is very different from the 1.0 version from 1992.  Furthermore,\nthere is a specialized version called OpengGL\u00a0ES for \"embedded systems\" such\nas mobile phones and tablets.  And there is also WebGL, for use in Web browsers,\nwhich is basically a port of OpenGL ES\u00a02.0.  It's useful to know something\nabout how and why OpenGL has changed.", "First of all, you should know that OpenGL was designed as a \"client/server\"\nsystem.  The server, which is responsible for controlling the computer's\ndisplay and performing graphics computations, carries out commands issued by the\nclient.  Typically, the server is a GPU, including its graphics processors and memory.\nThe server executes OpenGL commands.  The client is the CPU in the same computer, along \nwith the application program that it is running. OpenGL commands come from the\nprogram that is running on the CPU.  However,\nit is actually possible to run OpenGL programs remotely over a network.  That\nis, you can execute an application program on a remote computer (the OpenGL client), while\nthe graphics computations and display are done on the computer that you are\nactually using (the OpenGL server).", "The key idea is that the client and the server are separate components, and there\nis a communication channel between those components.  OpenGL commands and the\ndata that they need are communicated from the client (the CPU) to the server (the GPU)\nover that channel.  The capacity of the channel can be a limiting factor in graphics\nperformance.  Think of drawing an image onto the screen.  If the GPU can draw the\nimage in microseconds, but it takes milliseconds to send the data for the image\nfrom the CPU to the GPU, then the great speed of the GPU is irrelevant\u2014most of\nthe time that it takes to draw the image is communication time.", "For this reason, one of the driving factors in the evolution of OpenGL has been\nthe desire to limit the amount of communication that is needed between the CPU and\nthe GPU.  One approach is to store information in the GPU's memory.  If some data\nis going to be used several times, it can be transmitted to the GPU once and\nstored in memory there, where it will be immediately accessible to the GPU.\nAnother approach is to try to decrease the number of OpenGL commands that must\nbe transmitted to the GPU to draw a given image.", "OpenGL draws ", " such as triangles.\nSpecifying a primitive means specifying ", "\nand ", " for each of its ", ".  In the\noriginal OpenGL\u00a01.0, a separate command was used to specify the coordinates of each vertex,\nand a command was needed each time the value of an attribute changed.  To draw a single \ntriangle would require three or more commands.  Drawing a complex object made up of\nthousands of triangles would take many thousands of commands.  Even in OpenGL\u00a01.1,\nit became possible to draw such an object with a single command instead of thousands.  All the data\nfor the object would be loaded into arrays, which could then be sent in a single\nstep to the GPU.  Unfortunately, if the object was going to be drawn more than\nonce, then the data would have to be retransmitted each time the object was drawn.\nThis was fixed in OpenGL\u00a01.5 with ", ".\nA VBO is a block of memory in the GPU that can store the coordinates or attribute values for\na set of vertices.  This makes it possible to reuse the data without having to retransmit it\nfrom the CPU to the GPU every time it is used.", "Similarly, OpenGL 1.1 introduced ", "\nto make it possible to store several images on the GPU for use as ", ".\nThis means that texture images that are going to be reused several times can be loaded once\ninto the GPU, so that the GPU can easily switch between images without having to reload them.", "As new capabilities were added to OpenGL, the API grew in size.  But the growth was still\noutpaced by the invention of new, more sophisticated techniques for doing graphics.  Some\nof these new techniques were added to OpenGL, but\nthe problem is that no matter how many features you add, there will always be demands for \nnew features\u2014as well as complaints that all the new features are making things too \ncomplicated! OpenGL was a giant machine, with new pieces always being tacked onto it, \nbut still not pleasing everyone. The real solution was to make the machine ", ".\nWith OpenGL 2.0, it became possible to write programs to be executed as part of the\ngraphical computation in the GPU.  The programs are run on the GPU at GPU speed.\nA programmer who wants to use a new graphics technique can write a program to \nimplement the feature and just hand it to the GPU.  The OpenGL API doesn't have to\nbe changed.  The only thing that the API has to support is the ability to send programs\nto the GPU for execution.", "The programs are called ", " (although the term does't\nreally describe what most of them actually do).  The first shaders to be introduced were\n", " and ", ".\nWhen a ", " is drawn, some work has to be done at each vertex of the primitive,\nsuch as applying a ", " to the vertex coodinates or\nusing the ", " and global ", " environment\nto compute the color of that vertex.  A vertex shader is a program that can take over the\njob of doing such \"per-vertex\" computations.  Similarly, some work has to be done for each\npixel inside the primitive.  A fragment shader can take over the job of performing such\n\"per-pixel\" computations.  (Fragment shaders are also called pixel shaders.)", "The idea of programmable graphics hardware was very successful\u2014so successful that\nin OpenGL\u00a03.0, the usual per-vertex and per-fragment processing\nwas deprecated (meaning that its use was discouraged). \nAnd in OpenGL\u00a03.1, it was removed from\nthe OpenGL standard, although it is still present as an optional extension.  In practice,\nall the original features of OpenGL are still supported in desktop versions of OpenGL and will\nprobably continue to be available in the future.  On the embedded system side, however,\nwith OpenGL\u00a0ES\u00a02.0 and later, the use of shaders is mandatory, and a large part\nof the OpenGL\u00a01.1 API has been completely removed.\nWebGL, the version of OpenGL for use in web browsers, \nis based on OpenGL\u00a0ES\u00a02.0, and it also requires shaders to get anything at all done.\nNevertheless, we will begin our study of OpenGL with version 1.1.  Most of the concepts and\nmany of the details from that version are still relevant, and it offers an easier entry point\nfor someone new to 3D graphics programming.", "OpenGL shaders are written in ", " (OpenGL Shading Language).  Like\nOpenGL itself, GLSL has gone through several versions. We will spend some time later in the\ncourse studying GLSL\u00a0ES\u00a01.0, the version used with WebGL\u00a01.0 and\nOpenGL\u00a0ES\u00a02.0.  GLSL uses a syntax similar to the C programming language.", "As a final remark on GPU hardware, I should note that the computations that are done for\ndifferent vertices are pretty much independent, and so can potentially be done in parallel.\nThe same is true of the computations for different fragments.  In fact, GPUs can\nhave hundreds or thousands of processors that can operate in parallel.  Admittedly, the\nindividual processors are much less powerful than a CPU, but then typical per-vertex\nand per-fragment computations are not very complicated.  The large number of processors,\nand the large amount of parallelism that is possible in graphics computations, makes\nfor impressive graphics performance even on fairly inexpensive GPUs."], "chapter_title": "Introduction", "id": 1.3}, {"section_title": "Elements of 3D Graphics", "chapter_id": "Chapter 1", "section_id": "Section 1.2", "content": ["When we turn to 3D graphics, we find that the most common approaches have more\nin common with ", " than with ", ".  \nThat is, the content of an image is specified as a list of geometric objects. \nThe technique is referred to as ", ". The starting point\nis to construct an \"artificial 3D world\" as a collection of simple geometric shapes, arranged in\nthree-dimensional space.  The objects can have ", " that, combined with\nglobal properties of the world, determine the appearance of the objects.\nOften, the range of basic shapes is very limited, perhaps including only points, line segments,\nand triangles.  A more complex shape such as a polygon or sphere can be built or\napproximated as a collection of more basic shapes, if it is not itself considered\nto be basic.  To make a two-dimensional image of the scene,\nthe scene is ", " from three dimensions \ndown to two dimensions.  Projection is the equivalent of\ntaking a photograph of the scene.  Let's look at how it all works in a\nlittle more detail.", "\n", "\nWe start with an empty 3D space or \"world.\" Of course, this space exists only conceptually, but it's \nuseful to think of it as real and to be able to visualize it in your mind.  The space needs\na ", " that associates each point in the space with three numbers, \nusually referred to as the ", ", ", ", and ", " coordinates of the point.\nThis coordinate system is referred to as \"world coordinates.\"", "We want to build a scene inside the world, made up of geometric objects.  For example, \nwe can specify a line segment in the scene by giving the coordinates of its two endpoints, \nand we can specify a triangle by giving the coordinates of its three vertices.  The smallest building\nblocks that we have to work with, such as line segments and triangles, are called\n", ".  Different graphics\nsystems make different sets of primitive available, but in many cases only very basic\nshapes such as lines and triangles are considered primitive.  A complex scene can contain\na large number of primitives, and it would be very difficult to create the scene by\ngiving explicit coordinates for each individual primitive.  The solution, as any programmer\nshould immediately guess, is to chunk together primitives into reusable components.\nFor example, for a scene that contains several automobiles, we might create a geometric\nmodel of a wheel.  An automobile can be modeled as four wheels together with models of\nother components.  And we could then use several copies of the automobile model in the\nscene.  Note that once a geometric model has been designed, it can be used as a\ncomponent in more complex models.  This is referred to as ", ".", "Suppose that we have constructed a model of a wheel out of geometric primitives.\nWhen that wheel is moved into position in the model of an automobile, the coordinates of\nall of its primitives will have to be adjusted.  So what exactly have we gained by building\nthe wheel?  The point is that all of the coordinates in the wheel are adjusted\n", ".  That is, to place the wheel in the automobile, we just have to\nspecify a single adjustment that is applied to the wheel as a whole.  The type of \"adjustment\"\nthat is used is called a ", " (or geometric\ntransformation).  A geometric transform\nis used to adjust the size, orientation, and position of a geometric object.  When making\na model of an automobile, we build ", " wheel.  We then apply four different\ntransforms to the wheel model to add four copies of the wheel to the automobile.\nSimilarly, we can add several automobiles to a scene by applying different transforms\nto the same automobile model.", "The three most basic kinds of geometric transform are called ", ",\n", ", and ", ".\nA scaling transform is used to set the size of an object, that is, to make it bigger or smaller\nby some specified factor.\nA rotation transform is used to set an object's orientation, by rotating it by some  angle\nabout some specific axis.  A translation transform is used to set the position of an\nobject, by displacing it by a given amount from its original position.\nIn this book, we will meet these transformations first in two dimensions, where they\nare easier to understand. But it is in 3D graphics that they become truly essential.", "\n", "\nGeometric shapes by themselves are not very interesting.  You have to be able\nto set their appearance.  This is done by assigning ", "\nto the geometric objects.  An obvious attribute is color, but getting a realistic \nappearance turns out to be a lot more complicated than simply specifying a color\nfor each primitive.  In 3D graphics, instead of color, we usually talk about\n", ".   The term material here refers to the properties that determine the\nintrinsic visual appearance of a surface.  Essentially, this means how the surface\ninteracts with light that hits the surface.  Material properties can include a basic\ncolor as well as other properties such as shininess, roughness, and transparency.\n", "One of the most useful kinds of material property is a ", ".\nIn most general terms, a texture is a way of varying material properties from point-to-point\non a surface.  The most common use of texture is to allow different colors for different\npoints.  This is done by using a 2D image as a texture, which can be applied to a surface\nso that the image looks like it is \"painted\" onto the surface.\nHowever, texture can also refer to changing values for things like transparency or\n\"bumpiness.\"  Textures allow us to add detail to a scene without using a huge number of\ngeometric primitives; instead, you can use a smaller number of textured primitives.", "A material is an intrinsic property of an object, but the actual appearance of the\nobject also depends on the environment in which the object is viewed.\nIn the real world, you don't see anything unless there is some light in the environment.\nThe same is true in 3D graphics:  you have to add simulated ", "\nto a scene.  There can be several sources of light in a scene.  Each light source can have \nits own color, intensity, and direction or position.  The light from those sources will \nthen interact with the material properties of the objects in the scene.  Support for\nlighting in a graphics system can range from fairly simple to very complex and computationally\nintensive.", "\n", "\nIn general, the ultimate goal of 3D graphics is to produce 2D images of the 3D world.  \nThe transformation from 3D to 2D involves ", " and\n", ".  The world looks different when seen from different\npoints of view.  To set up a point of view, we need to specify the position of the viewer \nand the direction that the viewer is looking.  It is also necessary to specify\nan \"up\" direction, a direction that will be pointing upwards in the final image.\nThis can be thought of as placing a \"virtual camera\" into the scene.  Once the\nview is set up, the world as seen from that point of view can be projected into\n2D.  Projection is analogous to taking a picture with the camera.", "The final step in 3D graphics is to assign colors to individual pixels in \nthe 2D image. This process is called ", ",\nand the whole process of producing an image is referred to as ", "\nthe scene.", "In many cases the ultimate goal is not to create a single image, but to create an\n", ", consisting a sequence of images that show the world\nat different times.  In an animation, there are small changes from one image in the\nsequence to the next.  Almost any aspect of a scene can change during an animation,\nincluding coordinates of primitives, transformations, material properties, and the view.\nFor example, an object can be made to grow over the course of an animation by\ngradually increasing the scale factor in a scaling transformation that is applied to\nthe object.  And changing the view during an animation can give the effect of\nmoving or flying through the scene.  Of course, it can be difficult to compute\nthe necessary changes.  There are many techniques to help with the computation.  One of the most \nimportant is to use a \"physics engine,\" which computes the motion \nand interaction of objects based on the laws of physics.  (However, you won't learn\nabout physics engines in this book.)"], "chapter_title": "Introduction", "id": 1.2}, {"section_title": "Lighting and Material", "chapter_id": "Chapter 7", "section_id": "Section 7.2", "content": ["In this section, we turn to the question of ", " and ", " in\nWebGL. We will continue to use the basic OpenGL model that was covered in ", "\nand ", ", but now we are responsible for implementing the\n", " in our own GLSL ", " programs.  That means being more\naware of the implementation details.  It also means that we can pick and choose which parts\nof the lighting equation we will implement for a given application.", "The goal of the lighting equation is to compute a color for a point on a surface.  The inputs\nto the equation include the ", " properties of the surface and the properties of light\nsources that illuminate the surface.  The angle at which the light hits the surface\nplays an important role. The angle can be computed from the direction to the light source and the\n", " to the surface.  Computation of ", " also\nuses the direction to the viewer and the direction of the reflected ray.  The four vectors\nthat are used in the computation are shown in this lighting diagram from ", ":", "\n", "The vectors ", ", ", ", ", ", and ", " should be ", ".\nRecall that unit vectors\nhave the property that the cosine of the angle between two unit vectors is given by the\n", " of the two vectors.", "The lighting equation also involves ", " and\n", " color, which do not depend the direction\nvectors shown in the diagram.", "You should keep this big picture in mind as we work through some examples that\nuse various aspects of the lighting model.", "Even very simple lighting can make 3D graphics look more realistic.  For minimal lighting,\nI sometimes use what I call a \"viewpoint light,\" a white light that shines from the direction \nof the viewer into the scene.  In the simplest case, a ", " \ncan be used.  In ", ", a directional viewpoint light shines in the \ndirection of the negative ", "-axis.  The light direction vector (", " in\nthe above diagram), which points towards the light source, is (0,0,1).", "To keep things minimal, let's consider ", " only.  In that case,\nthe color of the light reflected from a surface is the product of the diffuse material color of\nthe surface, the color  of the light, and the cosine of the angle at which the light\nhits the surface.   The product is computed separately for the red, green, and blue components\nof the color.  We are assuming that the light is white, so the light color is 1 in the formula.\nThe material color will probably come from the JavaScript side as a uniform or attribute variable.", "The cosine of the angle at which the light hits the surface is given by the dot product of the\nnormal vector ", " with the light direction vector ", ".  In eye coordinates,\n", " is (0,0,1).  The dot product, ", " or ", "(0,0,1), \nis therefore simply ", ", the ", "-component of ", ".  However, this assumes\nthat ", " is also given in eye coordinates.  The normal vector will ordinarily come\nfrom the JavaScript side and will be expressed in object coordinates. Before it is used in\nlighting calculations, it must be transformed to the eye coordinate system.  As discussed\nin ", ", to do that we need a normal transformation matrix that\nis derived from the modelview matrix.  Since the normal vector must be of length one,\nthe GLSL code for computing ", " would be something like", "where ", " is the original normal vector in object coordinates, ", "\nis the normal transformation matrix, and ", " is a built-in GLSL function that\nreturns a vector of length one pointing in the same direction as its parameter.", "There is one more complication:  The dot product ", " can be negative, which would mean that the\nnormal vector points away from the light source (into the screen in this case).  Ordinarily, that would\nmean that the light does not illuminate the surface.  In the case of a viewpoint light, where we know\nthat every visible surface is illuminated, it means that we are looking at\nthe \"back side\" of the surface (or that incorrect normals were specified).\nLet's assume that we want to treat the two sides of the surface the \nsame.  The correct normal vector for the back side is the negative of the normal vector for the front side,\nand the correct dot product is (", ")\u00b7", ".  We can handle both cases if we simply use\n", "(", ").  For ", " = (0,0,1), that would be ", "(", ").  If\n", " is a ", " giving the diffuse color of the surface, the visible color can be\ncomputed as", "If ", " is instead a ", " giving an RGBA color, only the RGB\ncomponents should be multiplied by the dot product:", "The sample program ", " implements this minimal\nlighting model.  The lighting calculations are done in the vertex shader.  Part of the scene\nis drawn without lighting, and the vertex shader has a uniform ", " variable to\nspecify whether lighting should be applied.  Here is the vertex shader source code from\nthat program:", "It would be easy to add ambient light to this model, using a uniform variable to specify\nthe ambient light level.  Emission color is also easy.", "The directional light used in this example is technically only correct for an\n", ", although it will also generally give acceptable results\nfor a ", ".  But the correct viewpoint light for a\nperspective projection is a point light at (0,0,0)\u2014the position of the \"eye\" in\neye coordinates.  A point light is a little more difficult than a directional light.", "Remember that lighting calculations are done in eye coordinates.\nThe vector ", " that points from the surface to the light can be computed as", "where ", " is a ", " that gives the position of the light in\neye coordinates, and ", " is a ", " giving the position of the\nsurface point in eye coordinates.  For a viewpoint light, the ", "\nis (0,0,0), and ", " can be computed simply as ", "(\u2212", ").\nThe eye coordinates for the surface point must be computed by applying the modelview \nmatrix to the object coordinates for that point.\nThis means that the shader program needs to know the modelview matrix; it's\nnot sufficient to know the combined modelview and projection matrix.  The vertex shader\nshown above can modified to use a point light at (0,0,0) as follows:", "(Note, however, that in some situations, it can be better to move the \nlighting calculations to the fragment shader, as we will soon see.)", "To add specular lighting to our basic lighting model, we need to work with the\nvectors ", " and ", " in the lighting diagram.  In perfect specular reflection,\nthe viewer sees a specular highlight only if ", " is equal to\u00a0", ",\nwhich is very unlikely.  But in the lighting equation that we are using, the amount\nof specular reflection depends on the dot product ", ", which represents\nthe cosine of the angle between ", " and\u00a0", ".  The formula for\nthe contribution of specular reflection to the visible color is", "where ", " is the ", " (the material property called\n\"shininess\" in OpenGL).  The formula is only valid if ", " is greater than\nzero; otherwise, the specular contribution is zero.", "The unit vector ", " can be computed from ", " and ", ".  (Some trigonometry shows that\n", " is given by 2*(", ")", ".) \nGLSL has a built-in function\n", "(", ") that computes the reflection of a vector ", " through a unit\nnormal vector\u00a0", "; however, the value of ", "(", ") is\n", " rather than\u00a0", ".  (GLSL assumes a light direction vector that\npoints from the light toward the surface, while my ", " vector does the reverse.)", "The unit vector ", " points from the surface towards the position of the viewer.\nRemember that we are doing the calculations in eye coordinates.\nFor an orthographic projection, the viewer is essentially at infinite distance, and\n", " can be taken to be (0,0,1).  For a perspective projection, the viewer is at\nthe point (0,0,0) in eye coordinates, and ", " is given by \n", "(", ") where ", " contains the xyz coordinates\nof the surface point in the eye coordinate system.  Putting all this together, and\nassuming that we already have ", " and ", ", the GLSL code for computing\nthe color takes the form:", "The sample program ", " implements lighting\nwith diffuse and specular reflection.  For this program, which draws curved surfaces, \nnormal vectors are given as a vertex attribute rather rather than a uniform variable.\nTo add some flexibility to the lighting, the light position is specified as a uniform variable rather than a constant.\nFollowing the OpenGL convention, ", " is a ", ".  For a directional light,\nthe ", "-coordinate is 0, and the eye coordinates of the light are ", ".\nIf the ", "-coordinate is non-zero, the light is a point light, and its eye coordinates\nare ", ".  (The division by ", " is the\nconvention for ", ", but in practice, ", "\nwill usually be either zero or one.)  The program allows for different diffuse and specular\nmaterial colors, but the light is always white, with diffuse intensity 0.8 and\nspecular intensity 0.4.  You should be able to understand all of the code in\nthe vertex shader:", "The fragment shader just assigns the value of ", " to ", ".", "This approach imitates OpenGL 1.1 in that it does lighting calculations in the\nvertex shader.  This is sometimes called ", ".\nIt is similar to ", " in ", ", \nexcept that Lambert shading only uses diffuse reflection.\nBut there are many cases where per-vertex lighting does not give good results.\nWe saw in ", " that it can give very bad results for ", ".\nIt also tends to produce bad specular highlights, unless the primitives are very small.", "If a light source is close to a primitive, compared to the size of the primitive,\nthe the angles that the light makes with the primitive at the vertices can have very\nlittle relationship to the angle of the light at an interior point of the primitive:", "\n", "Since lighting depends heavily on the angles, per-vertex lighting will not give a good \nresult in this case.  To get better results, we can do ", ".\nThat is, we can move the lighting calculations from the vertex shader into the fragment shader.", "To do per-pixel lighting, certain data that is available in the vertex shader must\nbe passed to the fragment shader in varying variables.  This includes, for example, either\nobject coordinates or eye coordinates for the surface point.\nThe same might apply to properties such as diffuse color, if they\nare attributes rather then uniform variables.  Of course, uniform variables are directly\naccessible to the fragment shader.  Light properties will generally be uniforms, and\nmaterial properties might well be.", "And then, of course, there are the normal vectors, which are so essential for lighting.\nAlthough normal vectors can sometimes be uniform variables, they are usually attributes.\nPer-pixel lighting generally uses interpolated normal vectors, passed to the fragment\nshader in a varying variable.  (", " is just per-pixel lighting\nusing interpolated normals.) An interpolated normal vector is in general only an approximation\nfor the geometrically correct normal, but it's usually good enough to give good results.\nAnother issue is that interpolated normals are not necessarily unit vectors, even if\nthe normals in the vertex shader are unit vectors.  So, it's important to normalize the\ninterpolated normal vectors in the fragment shader.  The original normal vectors in the\nvertex shader should also be normalized, for the interpolation to work properly.", "The sample program ", " uses\nper-pixel lighting.  I urge you to read the shader source code in that program.\nAside from the fact that lighting calculations have been moved\nto the fragment shader, it is identical to the previous sample program.", "This demo\nlets you view objects drawn using per-vertex lighting side-by-side with identical\nobjects drawn using per-pixel lighting.   It uses the\nsame shader programs as the two sample programs. \nSee the help text in the demo for more information:", "\n", "\n", "Our shader programs are getting more complex.  As we add support for multiple lights,\nadditional light properties, two-sided lighting, textures, and other features, it will be\nuseful to use data structures and functions to help manage the complexity.  GLSL data\nstructures were introduced in ", ", and functions in\n", ". Let's look briefly at how they can be used to work with\nlight and material.", "It makes sense to define a ", " to hold the properties of a light.  The properties\nwill usually include, at a minimum, the position and color of the light.  Other properties\ncan be added, depending on the application and the details of the lighting model that are \nused.  For example, to make it possible to turn lights on and off, a ", " variable\nmight be added to say whether the light is enabled:", "A light can then be represented as a variable of type ", ".  It will likely\nbe a ", " variable so that its value can be specified on the JavaScript side.\nOften, there will be multiple lights, represented by an array; for example, to allow for\nup to four lights:", "Material properties can also be represented as a ", ".  Again, the details\nwill vary from one application to another.  For example, to allow for diffuse and specular color:", "With these data types in hand, we can write a function to help with the lighting\ncalculation.  The following function computes the contribution of one light to the color of\na point on a surface. (Some of the parameters could be global variables in the shader\nprogram instead.)", "Then, assuming that there are four lights, the full calculation of the\nlighting equation might look like this:", "The sample program ", " uses GLSL data structures\nsimilar to the ones we have just been looking at.  It also introduces a few new features.\nThe program draws the graph of a parametric surface.  The (", ") coordinates of points on the\nsurface are given by functions of two variables ", " and\u00a0", ".  The definitions of\nthe functions can be input by the user.  There is a viewpoint light, but two extra lights have\nbeen added in an attempt to provide more even illumination.  The graph is considered to have two sides, \nwhich are colored yellow and blue.  The program can, optionally, show grid lines on the surface.  \nHere's what the default surface looks like, with grid lines:", "\n", "This is an example of ", " (", ").  \nWe need two materials, a front material for drawing front-facing polygons and a back material \nfor drawing back-facing polygons.  Furthermore, when drawing a back face, we have to reverse the\ndirection of the normal vector, since normal vectors are assumed to point out of the front\nface.", "But when the shader program does lighting calculations, how can\ndoes it know whether it's drawing a front face or a back face?  That information comes\nfrom outside the shader program: The fragment shader has a built-in boolean variable\nnamed ", " whose value is set to ", " before the fragment shader is\ncalled, if the shader is working on the front face of a polygon.  When doing per-pixel lighting,\nthe fragment shader can check the value of this variable to decide whether to use the front material or\nthe back material in the lighting equation.  The sample program has two uniform variables\nto represent the two materials.  It has three lights.\nThe normal vectors and eye coordinates of the point are varying variables.\nAnd the normal transformation matrix is also applied in the fragment shader:", "A color for the fragment is computed using these variables and the\n", " function shown above:", "Note that in the second call to ", ", the normal vector\nis given as \u2212", ".  The negative side reverses the direction\nof the normal vector for use on a back face.", "If you want to use two-sided lighting when doing per-vertex lighting, you have\nto deal with the fact that ", " is not available in the\nvertex shader.  One option is to compute both a front color and a back color\nin the vertex shader and pass both values to the fragment shader as\nvarying variables.  The fragment shader can then decide which color to use,\nbased on the value of ", ".", "There are a few WebGL settings related to two-sided lighting.  Ordinarily, WebGL\ndetermines the front face of a triangle according to the rule that when the front face is viewed,\nvertices are listed in counterclockwise order around the triangle.  The JavaScript command\n", " reverses the rule, so that vertices are listed in clockwise\norder when the front face is viewed.  The command ", " restores\nthe default rule.", "In some cases, you can be sure that no back faces are visible.  This will happen when\nthe objects are closed surfaces seen from the outside, and all the polygons face towards\nthe outside.  In such cases, it is wasted effort to draw back faces, since you can be sure\nthat they will be hidden by front faces.  The JavaScript command\n", "(", ") tells WebGL to discard polygons without drawing\nthem, based on whether they are front-facing or back-facing.  The commands\n", "(", ") and ", "(", ") determine\nwhether it is back-facing or front-facing polygons that are discarded when ", "\nis enabled; the default is back-facing.", "The sample program can display a set of grid lines on the surface. As always, drawing two\nobjects at exactly the same depth can cause a problem with the ", ".\nAs we have already seen at the end of ", " and in\n", ", OpenGL uses ", " to\nsolve the problem.  The same solution is available in WebGL.  Polygon offset\ncan be turned on with the commands", "and turned off with", "In the sample program, polygon offset is turned on while drawing the graph\nand is turned off while drawing the grid lines.", "In our examples so far, lights have been fixed with respect to the viewer.  But some\nlights, such as the headlights on a car, should move along with an object.  And some,\nsuch as a street light, should stay in the same position in the world, but changing position\nin the rendered scene as the point of view changes.", "Lighting calculations are done in eye coordinates.  When the position of a light\nis given in object coordinates or in world coordinates, the position must be\ntransformed to eye coordinates, by applying the appropriate modelview transformation.\nThe transformation can't be done in the shader program, because the modleview matrix\nin the shader program represents the transformation for the object that\nis being rendered, and that is almost never the same as the transformation for the\nlight.  The solution is to store the light position in eye coordinates.  That is,\nthe shader's uniform variable that represents the position of the light must be \nset to the position in eye coordinates.", "For a light that is fixed with respect to the viewer, the position of the light\nis already expressed in eye coordinates.  For example, the position of a point light\nthat is used as a viewpoint light is (0,0,0), which is the location of the viewer\nin eye coordinates.  For such a light, the appropriate modelview transformation is\nthe ", ".", "For a light that is at a fixed position in world coordinates, the appropriate modelview\ntransformation is the viewing transformation.  The viewing transformation\nmust be applied to the world-coordinate light position to transform it to eye coordinates.  In WebGL,\nthe transformation should be applied on the JavaScript side, and the output of the\ntransformation should be sent to a uniform variable in the shader program.  Similarly,\nfor a light that moves around in the world, the combined modeling and viewing transform should be\napplied to the light position on the JavaScript side.  The ", " library\n(", ") defines the function", "which can be used to do the transformation.  The ", " in the function call\nwill be the modelview transformation matrix.  Recall, by the way, that light position is\ngiven as a ", ", using homogeneous coordinates.\n(See ", ".)  Multiplication by the modelview\nmatrix will work for any light, whether directional or point, whose position is\nrepresented in this way.  Here is a JavaScript\nfunction that can be used to\nset the position:", "Remember that the light position, like other light properties, must be set before\nrendering any geometry that is to be illuminated by the light.", "We encountered spotlights in ", " in ", ".\nIn fact, although I didn't mention it, spotlights already existed in OpenGL\u00a01.1.\nInstead of emitting light in all directions, a spotlight emits only a cone of light.\nA spotlight is a kind of point light. The vertex of the cone is located at the\nposition of the light.  The cone points in some direction, called the ", ".  The spot\ndirection is specified as a vector.  The size of the cone is specified by a ", ";\nlight is only emitted from the light position in directions whose angle with the spot\ndirection is less than the cutoff angle. Furthermore, for angles less than the cutoff\nangle, the intensity of the light ray can decrease\nas the angle between the ray and spot direction increases.  The rate at which the\nintensity decreases is determined by a non-negative number called the ", ".\nThe intensity of the ray is given by ", " where\n", " is the basic intensity of the light, ", " is the cosine of the angle\nbetween the ray and the spot direction, and ", " is the spot exponent.\n", "This illustration shows three spotlights shining on a surface; the images\nare taken from the sample program ", ":", "\n", "The cutoff angle for the three spotlights is 30 degrees.  In the image on the\nleft, the spot exponent is zero, which means there is no falloff in intensity with increasing\nangle from the spot direction.  For the middle image, the spot exponent is 10, and\nfor the image on the right, it is 20.", "Suppose that we want to apply the lighting equation to a spotlight. Consider a point\n", " on a surface.  The lighting equation uses a unit vector, ", ", that points\nfrom ", " towards the light source.  For a spotlight, we need a vector that points\nfrom the light source towards ", "; for that we can use\u00a0", ".\nConsider the angle between ", " and the spot direction.  If that angle is\ngreater than the cutoff angle, then ", " gets no illumination from the spotlight.\nOtherwise, we can compute the cosine of the angle between ", " and the\nspot direction as the dot product ", ", where ", " is a unit vector that points\nin the spot direction.", "\n", "To implement spotlights in GLSL, we can add uniform variables to represent the\nspot direction, cutoff angle, and spot exponent.  My implementation actually uses\nthe cosine of the cutoff angle instead of the angle itself, since I can then compare the cutoff value\nusing the dot product, ", ", that represents the cosine of the\nangle between the light ray and the spot direction.  The ", "\nstruct becomes:", "With this definition, we can compute the factor ", " that is multiplied\nby the basic light color to give the effective light intensity of the spotlight at a point\non a surface.  The following code for the computation is from the fragment shader in the sample program.\nThe value of ", " is assigned to ", ":", "You should try the\n", ", and read the source code.\nOr try this demo, which\nis similar to the sample program, but with an added option to animate the \nspotlights:", "\n", "\n", "The ", " uniform variable gives the direction of the spotlight in eye coordinates.\nFor a moving spotlight, in addition to transforming the position, we also have to worry\nabout transforming the direction in which the spotlight is facing.  The spot direction is\na vector, and it transforms in the same way as normal vectors.  That is, the same\nnormal transformation matrix that is used to transform normal vectors is also used\nto transform the spot direction.  Here is a JavaScript function that can be used to\napply a modelview transformation to a spot direction vector and send the output\nto the shader program:", "Of course, the position of the spotlight also has to be transformed, as for\nany moving light.", "There is one more general property of light to consider: ", ".\nThis refers to the fact that the amount of illumination from a light source should\ndecrease with increasing distance from the light.  Attenuation applies only to point\nlights, since directional lights are effectively at infinite distance.\nThe correct behavior, according\nto physics, is that the illumination is proportional to one over the square of the\ndistance.  However, that doesn't usually give good results in computer graphics.  In fact,\nfor all of my light sources so far, there has been ", " attenuation with distance.", "OpenGL 1.1 supports attenuation.  The light intensity can be multiplied\nby 1.0\u00a0", "\u00a0(", "+", "+", "),\nwhere ", " is the distance to the light source, and ", ", ", ", and ", "  \nare properties of the light.  The numbers ", ", ", ", and ", " are called\nthe \"constant attenuation,\" \"linear attenuation,\" and \"quadratic attenuation\" of the light source.\nBy default, ", " is one, and ", " and ", " are zero, which means that there\nis no attenuation.", "Of course, there is no need to implement exactly the same model in your own applications.\nFor example, quadratic attenuation is rarely used.  In the next sample program, I use\nthe formula  1\u00a0", "\u00a0(1+", ")\nfor the attenuation factor. The attenuation constant ", " is added as another property\nof light sources.  A value of zero means no attenuation.  In the lighting computation,\nthe contribution of a light source to the lighting equation is multiplied by the attenuation\nfactor for the light.", "The sample program ", " is our final, more complex,\nexample of lighting in WebGL. The basic scene is the same as the ", " example\n", " from ", ", but\nI have added several lighting effects.", "The scene shows a red \"car\" traveling around the edge of a disk-shaped \"world.\"\nIn the new version, there is a sun that rotates around the world.  The sun is turned off\nat night, when the sun is below the disk.  (Since there are no shadows, if the sun were left\non at night, it would shine up through the disk and illuminate objects from below.)\nAt night, the headlights of the car turn on.  They are implemented as spotlights that travel\nalong with the car; that is, they are subject to the same modelview transformation that\nis used on the car.  Also at night, a lamp in the center of the world is turned on.\nLight attenuation is used for the lamp, so that its illumination is weak except for\nobjects that are close to the lamp.  Finally, there is dim viewpoint light that is always\non, to make sure that nothing is ever in absolute darkness.  Here is a night scene from the program, in which you\ncan see how the headlights illuminate the road and the trees, and you can probably see that\nthe illumination from the lamp is stronger closer to the lamp:", "\n", "But you should run the program to see it in action!  And read the source code to\nsee how it's done.", "My diskworld example uses per-pixel lighting, which gives much better results\nthan per-vertex lighting, especially for spotlights.  However, with multiple lights,\nspotlights, and attenuation, per-pixel lighting requires a lot of uniform variables in the\nfragment shader \u2014 possibly more than are supported in some implementations.\n(See ", " for information about limitations in shader programs.)\nThat's not really serious for a sample program in a textbook; it just means that the\nexample won't work in some browsers on some devices.  But for more serious applications,\nan alternative approach would be desirable, hopefully better than simply moving the\ncalculation to the vertex shader.  One option is to use a ", "\nin which the scene is rendered several times, with each pass doing the lighting calculation\nfor a smaller number of lights.", "(In fact, even on my computer, which advertises a limit of 4096 uniform vectors in\nthe fragment shader, I had a problem trying to use five lights in the diskworld example.\nWith five lights, I got weird color errors.  For example, the red or green color component might\nbe completely missing from the image.  I solved the problem by using the same light for\nthe sun and the lamp, which are never both on at the same time.  The program worked fine\nwith four lights.  Admittedly, I have a six-year old computer with a somewhat buggy GPU,\nbut this shows some of the issues that can come up for certain WebGL programs on\ncertain devices.)"], "chapter_title": "3D Graphics with WebGL", "id": 7.2}, {"section_title": "Textures", "chapter_id": "Chapter 7", "section_id": "Section 7.3", "content": ["Most of the WebGL API for working with textures was already\ncovered in ", ".  In this section, we look\nat several examples and techniques for using textures.", "In ", ", we saw how to apply a\n", " in OpenGL.  OpenGL maintains\na texture transform matrix that can be manipulated to apply\nscaling, rotation, and translation to texture coordinates\nbefore they are used to sample a texture.  It is easy to program\nthe same operations in WebGL.  We need to compute the texture\ntransform matrix on the JavaScript side.  The transform matrix\nis then sent to a uniform matrix variable in the the shader program, \nwhere it can be applied to the texture coordinates.  Note that as\nlong as the texture transformation is ", ",\nit can be applied in the vertex shader, even though the texture is\nsampled in the fragment shader.  That is, doing the transformation\nin the vertex shader and interpolating the transformed texture\ncoordinates will give the same result as interpolating the original\ntexture coordinates and applying the transformation to the interpolated\ncoordinates in the fragment shader.", "Since we are using ", " for coordinate transformation\nin 3D, it makes sense to use it for texture transforms as well.  If we use\n2D texture coordinates, we can implement scaling, rotation, and translation\non the JavaScript side using the ", " class from ", ".  \nThe functions that we need are", "For implementing texture transformations, the parameters ", " and ", " in these\nfunctions will be the texture transform matrix.  For example, to apply a scaling by a factor\nof 2 to the texture coordinates,  we might use the code:", "The last line assumes that ", " is the location of a uniform variable\nof type ", " in the shader program.  (And remember that scaling the texture coordinates\nby a factor of 2 will ", " the texture on the surfaces to which it is applied.)", "The sample WebGL program ", " uses texture transformations\nto animate textures.  In the program, texture coordinates are input into the vertex shader\nas an attribute named ", " of type ", ", and the texture transformation is\na uniform variable named ", " of type ", ".  The transformed\ntexture coordinates are computed in the vertex shader with the GLSL commands", "Read the source code to see how all this is used in the context of a complete program.", "Texture coordinates are typically provided to the shader program as\nan attribute variable.  However, when texture coordinates are not available,\nit is possible to generate them in the shader program.  While the results\nwill not usually look as good as using texture coordinates that are\ncustomized for the object that is being rendered, they can be acceptable\nin some cases.", "Generated texture coordinates should usually be computed from the\n", " of the object that is being rendered.  That is,\nthey are computed from the original vertex coordinates, before any transformation\nhas been applied.  Then, when the object is transformed, the texture\nwill be transformed along with the object so that it will look like the texture\nis attached to the object.  The texture coordinates could\nbe almost any function of the object coordinates.  If an affine function\nis used, as is usually the case, then the texture coordinates can be computed in the vertex shader.\nOtherwise, you need to send the object coordinates to the fragment shader\nin a varying variable and do the computation there.", "The simplest idea for generated texture coordinates is simply to use the\n", " and ", " coordinates from the object coordinate system as the\ntexture coordinates.  If the vertex coordinates are given as the value of\nthe attribute variable ", ", that would mean using ", "\nas texture coordinates.  This has the effect of projecting the texture onto\nthe surface from the direction of the positive ", "-axis, perpendicular to the\n", "-plane.  The mapping works OK for a polygon that is facing, more-or-less,\nin the direction of positive\u00a0", ", but it doesn't give good results for\npolygons that are edge-on to the ", "-plane.  Here's what the mapping\nlooks like on a cube:", "\n", "The texture projects nicely onto the front face of the cube.  It also works OK\non the back face of the cube (not visible in the image), except that it is mirror-reversed.\nOn the other four faces, which are exactly edge-on to the ", "-plane, you just\nget lines of color that come from pixels along the border of the texture image.\n(In this example, one copy of the texture image exactly fills the front face of\nthe cube.  That doesn't happen automatically; you might need a texture transform\nto adapt the texture image to the surface.)", "Of course, we could project in other directions to map the texture to other\nfaces of the cube.  But how to decide which direction to use?  Let's say that we\nwant to project along the direction of one of the coordinate axes.\nWe want to project, approximately at least, from the direction that the surface is facing.  \nThe normal vector to the surface tells us that direction.  We should project in the\ndirection where the normal vector has its greatest magnitude.  For example, if\nthe normal vector is (0.12,\u00a00.85,\u00a00.51), then we should project from\nthe direction of the positive ", "-axis.  And a normal vector equal to\n(\u22120.4,\u00a00.56,\u00a0\u22120.72) would tell us to project from the\ndirection of the negative ", "-axis.  This resulting \"cubical\" generated\ntexture coordinates are perfect for a cube, and it looks pretty good on most\nobjects, except that there can be a seam where the direction of projection changes.\nHere, for example, the technique is applied to a teapot:\n", "\n", "When using ", ", so that all of the normals to a polygon point\nin the same direction, the computation can be done in the vertex shader.  With\n", ", normals at different vertices of a polygon can point\nin different directions.  If you project texture coordinates from different directions\nat different vertices and interpolate the results, the result is likely to be\na mess.  So, doing the computation in the fragment shader is safer.  Suppose\nthat the interpolated normal vectors and object coordinates are provided to\nthe fragment shader in  varying variables named ", " and ", ".\nThen the following code can be used to generate \"cubical\" texture coordinates:", "When projecting along the ", "-axis, for example, the ", " and ", " coordinates\nfrom ", " are used as texture coordinates.  The coordinates are computed\nas either ", " or ", ", depending on whether the\nprojection is from the positive or the negative direction of ", ".  The order of\nthe two coordinates is chosen so that a texture image will be projected directly onto\nthe surface, rather than mirror-reversed.", "You can experiment\nwith generated textures using the following demo.\nThe demo shows a variety of textures and objects using cubical generated texture coordinates,\nas discussed above.  You can also try texture coordinates projected just onto the ", "\nor ", " plane, as well as a cylindrical projection that wraps a texture image once around\na cylinder.  A final option is to use the ", " and ", " coordinates from the \neye coordinate system as texture coordinates.  That option fixes the texture on the\nscreen rather than on the object, so the texture doesn't rotate with the object.  The effect\nis interesting, but probably not very useful.", "\n", "\n", "Up until now, all of our textures have been image textures.  With an image\ntexture, a color is computed by ", " the image, based on a\npair of texture coordinates.  The image essentially defines a function that\ntakes texture coordinates as input and returns a color as output.\nHowever, there are other ways to define such functions besides looking up\nvalues in an image.  A\u00a0", " is\ndefined by a function whose value is computed rather than looked up.\nThat is, the texture coordinates are used as input to a code segment\nwhose output is a color value.", "In WebGL, procedural textures can be defined in the fragment shader.\nThe idea is simple:  Take a ", " representing a set of texture coordinates.\nThen, instead of using a ", " to look up a color, use the\n", " as input to some mathematical computation that computes a \n", " representing a color.  In theory any computation could be used,\nas long as the components of the ", " are in the range 0.0 to\u00a01.0.", "We can even extend the idea to 3D textures.  2D\u00a0textures use a ", " as\ntexture coordinates.  For 3D texture coordinates, we use a ", ".\nInstead of mapping points on a plane to color, a 3D texture maps points in\nspace to colors.  It's possible to have 3D textures that are similar to image textures.\nThat is, a color value is stored for each point in a 3D grid, and the texture is\nsampled by looking up colors in the grid.  However, a 3D grid of colors takes up\na lot of memory.  On the other hand, 3D procedural textures use no memory resources and\nuse very little more computational resources than 2D procedural textures.", "So, what can be done with procedural textures?  In fact, quite a lot.  There\nis a large body of theory and practice related to procedural textures. We will\nlook at a few of the possibilities.  Here's a torus, textured using four\ndifferent procedural textures.  The images are from the demo shown\nat the end of this subsection:", "\n", "The torus on the left uses a 2D procedure texture representing a checkerboard pattern.\nThe 2D texture coordinates were provided, as usual, as values of a vertex attribute\nvariable in the shader program.  The checkerboard pattern is regular grid of equal-sized\ncolored squares, but, as with any 2D texture, the pattern is stretched and distorted when\nit is mapped to the curved surface of the torus.  Given texture coordinates in the\nvarying variable ", ", the color value for the checkerboard texture\ncan be computed as follows in the fragment shader:", "The ", " in the second and third lines\nrepresents a texture transformation that is used to adapt the size of\nthe texture to the object that is being textured.  (The texture coordinates for the torus\nrange from 0 to 1; without the scaling, only one square in the checkerboard pattern would\nbe mapped to the torus.  For the torus in the picture, ", " is\u00a08.)\nThe ", " function computes the largest integer less than or equal to its\nparameter, so ", " and ", " are integers.  The value of ", "(", ",2.0)\nis either 0.0 or 1.0, so the test in the fourth line tests whether ", " is even or odd.\nThe idea here is that when either ", " or ", " increases or decreases by 1,\n", " will change from even to odd or from odd to even; that ensures that neighboring\nsquares in the pattern will be differently colored.", "The second torus in the illustration uses a 3D checkerboard pattern.  The 3D pattern\nis made up of a grid of cubes that alternate in color in all three directions.  For the\n3D texture coordinates on the cube, I use object coordinates.  That is, the 3D texture coordinates\nfor a point are the same as its position in space, in the object coordinate system in which\nthe torus is defined.  The effect is like carving the torus out of a solid block that\nis colored, inside and out, with a 3D checkerboard pattern.  Note that you don't see\ncolored squares or rectangles on the surface of the torus; you see the intersections of\nthat surface with colored cubes.  The intersections have a wide variety of shapes.\nThat might be a disadvantage for this particular 3D texture,\nbut the advantage is that there is no stretching and distortion of the texture.\nThe code for computing the 3D checkerboard is the same as for the 2D case, but using three\nobject coordinates instead of two texture coordinates.", "Natural-looking textures often have some element of randomness.  We can't use\nactual randomness, since then the texture would look different every time it is drawn.\nHowever, some sort of pseudo-randomness can be incorporated into the algorithm that computes\na texture.  But we don't want the colors in the texture to look completely random\u2014there\nhas to be some sort of pattern in the pattern!  Many natural-looking procedural textures\nare based on a type of pseudo-randomness called ", ",\nnamed after Ken Perlin who invented the algorithm in 1983.  The third torus in the above\nillustration uses a 3D texture based directly on Perlin noise.  The \"marble\" texture\non the fourth torus uses Perlin noise as a component in the computation.  Both textures\nare 3D, but similar 2D versions are also possible.  (I\u00a0don't know the algorithm for\nPerlin noise.  I copied the GLSL code from ", ".\nThe code is published under an MIT-style open source license, so that it can be used freely in\nany project.)", "In the sample program, 3D Perlin noise is computed by a function ", "(", "),\nwhere ", " is a ", " and the output of the function is a ", " in \nthe range \u22121.0 to 1.0.  Here is the computation:", "Here, ", " is a varying variable containing the 3D object coordinates of\nthe point that is being textured, and ", " is a texture transformation that\nadapts the size of the texture to the torus.\nSince the output of ", "() varies between \u22121.0 and 1.0, ", " varies\nfrom 0.5 to 1.0, and the ", " for the texture\nranges from pale magenta to white.  The color variation that you see on the third torus\nis characteristic of Perlin noise.  The pattern is somewhat random, but it has regular,\nsimilarly sized features.  With the right scaling and coloration, basic Perlin noise\ncan make a decent cloud texture.", "The marble texture on the fourth torus in the illustration is made by adding some\nnoise to a regular, periodic pattern.  The basic technique can produce a wide variety\nof useful textures.  The starting point is a periodic function of one variable,\nwith values between 0.0 and 1.0.  To get a periodic pattern in 2D or 3D, the\ninput to the function can be computed from the texture coordinates.\nDifferent functions can produce very different effects.  The three patterns shown\nhere use the functions (1.0+", "(", "))/2.0, ", "(", "(", "))\nand (", "\u2212", "(", ")), respectively:", "\n", "In the second image, taking the absolute value of ", "(", ") produces\nnarrower, sharper dark bands than the plain ", " function in the first image.  \nThis is the function that is used for the marble texture in the illustration.\nThe sharp discontinuity in the third image can be an interesting visual effect.", "To get the 2D pattern from a function ", "(", ") of one variable, we can use\na function of a ", ", ", ", defined as ", "(", "),\nwhere ", " and ", " are constants.  The values of ", " and ", " determine the\norientation and spacing of the color bands in the pattern.  For a 3D pattern, we would use\n", "(", ").", "To add noise to the pattern, add a Perlin noise function to the input of the\nfunction.  For a 3D pattern, the function would become", "The new constants ", " and ", " determine the size and intensity of the\nperturbations to the pattern.  As an example, the code that creates the marble texture\nfor the torus is:", "(The ", " at the end was added to make the color bands even sharper than they\nwould be without it.)", "The following demo\nlets you apply a variety of 3D textures to different objects.  The procedural textures used in the\ndemo are just a small sample of the possibilities.", "\n", "\n", "So far, the only textures that we have encountered have affected color.  Whether they were image\ntextures, environment maps, or procedural textures, their effect has been to vary the color\non the surfaces to which they were applied.  But, more generally, texture can refer to variation\nin any property.  One example is ", ", where the property that is\nmodified by the texture is the normal vector to the surface.  A\u00a0normal vector determines \nhow light is reflected by the surface,  which is a major visual clue to the direction that the\nsurface faces.  Modifying the normal vectors has the effect of modifying the apparent\norientation of the surface, as least with respect to the way it reflects light.\nIt can add the appearance of roughness or \"bumps\" to the surface.  The effect can be visually\nsimilar to changing the positions of points on the surface, but with bumpmapping the change in\nappearance is achieved without actually changing the surface geometry.  The alternative\napproach of modifying the actual geometry, which is called \"displacement mapping,\" can give\nbetter results but requires a lot more computational and memory resources.", "The typical way to do bumpmapping is with a height map.  A height map, is a\n", " image in which the variation in color is used to specify the amount\nby which points on the surface are (or appear to be) displaced.  A height map is \nmapped to a surface in the same way as an image texture, using texture coordinates\nthat are supplied as an attribute variable or generated computationally.  But instead\nof being used to modify the color of a pixel, the color value from the height map is used\nto modify the normal vector that goes into the lighting equation that computes the\ncolor of the pixel.  A height map that is used in this way is also called a bump map.\nI'm not sure that my implementation of this idea is optimal, but it can produce\npretty good results.", "Here are two examples.  For each example, a bumpmapped torus is shown next to the\nheight map that was applied to the torus:", "\n", "In the first example, the black dots in the height map produce the appearance of bumps on the torus.\nThe darker the color from the map, the greater apparent displacement of the point on the surface.  The\nblack centers of the dots map to the tops of the bumps.\nFor the second example, the dark curves in the height map seem to produce deep grooves in the surface.\nAs is usual for textures, the height maps have been stretched to cover the torus, \nwhich distorts the shape of the features from the map.", "To see how bumpmapping can be implemented, let's first imagine that we want to apply\nit to a one-dimensional \"surface.\"  Consider a normal vector to a point on the surface,\nand suppose that a height map texture is applied to the surface.  Take a vector, shown\nin black in the following illustration, that points in the direction in which the\nheight map grayscale value is increasing.", "\n", "We want the surface to appear as if it is tilted, as shown in the middle of the illustration.\n(I'm assuming here that darker colors in the height map corresponding to smaller heights.)\nLiterally tilting the surface would change the direction of the normal vector.  We can get\nthe same change in the normal vector by adding some multiple of the vector from the height map\nto the original normal vector, as shown on the right above.  Changing the number that is multiplied\nby the height map vector changes the degree of tilting of the surface.  Increasing the mulitplier\ngives a stronger bump effect.  Using a negative multiple will tilt the surface in the opposite\ndirection, which will transform \"bumps\" into \"dimples,\" and vice versa.  I will refer to the\nmultiplier as the ", ".", "Things get a lot more complicated for two-dimensional surfaces in 3D space.  A 1D \"surface\"\ncan only be tilted left or right.  On a 2D surface, there are infinitely\nmany directions to tilt the surface.  Note that the vector that points in the direction of tilt\npoints along the surface,\nnot perpendicular to the surface.  A vector that points along a surface is called a tangent\nvector to the surface.  To do bump mapping, we need a tangent vector for each point on the surface.\nTangent vectors will have to be provided, along with normal vectors, as part of the data for\nthe surface.  For my version of bumpmapping, the tangent vector that we need should be coordinated \nwith the texture coordinates for the surface: The tangent vector should point in the direction in which the ", " coordinate\nin the texture coordinates is increasing.", "In fact, to properly account for variation in the height map, we need a second tangent vector.\nThe second tangent vector is perpendicular both to the normal and to the first tangent vector.\nIt is commonly called the \"binormal\" vector, and it can be computed from the normal and the tangent.\n(The binormal should point in the direction in which the ", " texture coordinate is increasing,\nbut whether that can be exactly true will depend on the texture mapping.  As long as it's not too far\noff, the result should be OK.)", "Now, to modify the normal vector, proceed as follows:  Sample the height maps at two points, separated by\na small difference in the ", " coordinate.  Let ", " be the difference between the two values;\n", "\u00a0represents the rate at which the height value is changing in the direction of\nthe tangent vector (which, remember, points in the ", " direction along the surface).\nThen sample the height map at two points separated by a small difference in the ", " coordinate,\nand let ", " be the difference between the two values, so that ", " represents the rate at which \nthe height value is changing in the direction of the binormal vector.  Let ", " be the\nvector ", "T\u00a0+\u00a0", "B, where ", " is\nthe tangent vector and ", " is the binormal.  Then add ", ", or a multiple of ", ",\nto the original normal vector to produce the modified normal that will be used in the lighting \nequation.  (If you know multivariable calculus, what we are doing here amounts to using approximations for\ndirectional derivatives and the gradient vector of a height function on the surface.)", "I have tried to explain the procedure in the following illustration.  You need to\nvisualize the situation in 3D, noting that the normal, tangent, and binormal vectors are\nperpendicular to each other.  The white arrows on the left are actually multiples of the\nbinormal and tangent vectors, with lengths given by the change in color between two pixels.", "\n", "The sample program ", " demonstrates bumpmapping.\nThe two bumpmapped toruses in the above illustration are from that program.\nWhen you run the program, pay attention to the specular highlights!  They will help you to\nsee how a bumpmap texture differs from an image texture.  The effect might\nbe more obvious if you change the \"Diffuse Color\" from white to some other color.  The\nspecular color is always white.", "(For this program, I had to add tangent vectors to my objects.  I chose three objects\u2014a cube,\na cylinder, and a torus\u2014for which tangent vectors were relatively easy to compute.\nBut, honestly, it took me a while to get all the tangent vectors pointing in the correct\ndirections.)", "The bumpmapping is implemented in the fragment shader in the sample program.  The essential\nproblem is how to modify the normal vector.  Let's examine the GLSL code that does the\nwork:", "The first three lines compute the normal, tangent, and binormal unit vectors.\nThe normal and tangent come from varying variables whose values are interpolated from\nattribute variables, which were in turn input to the shader program from the JavaScript side.\nThe binormal, which is perpendicular to both the normal and the tangent, is\ncomputed as the ", " of the normal and tangent (", ").", "The next four lines get the values of the height map at the pixel that corresponds to the\nsurface point that is being processed and at two neighboring pixels.  ", " is\nthe height map value at the current pixel, whose coordinates in the texture are given by\nthe texture coordinates, ", ".  The value for ", " is the red\ncolor component from the bumpmap texture; since the texture is grayscale, all of its\ncolor components have the same value.  ", " is the value from the pixel above\nthe current pixel in the texture; the coordinates are computed by adding\n1.0/", " to the ", "-coordinate of the current pixel, where\n", " is a uniform variable that gives the size of the texture image, in pixels.\nSince texture coordinates in the image run from 0.0 to 1.0, the difference in the\n", "-coordinates of the two pixels is 1.0/", ".  Similarly,\n", " is the height map value for the pixel to the right of the current pixel\nin the bumpmap texture.  I should note that the ", " for\nthe bumpmap texture was set to ", ", because we need to read the actual value from\nthe texture, not a value averaged from several pixels, as would be returned by the\ndefault minification filter.", "The two vectors (", ")", " and\n(", ")", " are the two white vectors in the\nabove illustration.  Their sum is ", ".  A\u00a0multiple of that\nsum is added to the normal vector to give the modified normal vector.  The\nmultiplier, ", ", is a uniform ", " variable.", "All of the calculations so far have been done in the object coordinate system.\nThe resulting normal depends only on the original object coordinates, not on\nany transformation that has been applied.  The normal vector still has to be\ntransformed into eye coordinates before it can be used in the lighting equation.\nThat transformation is done in the last line of code shown above.", "\n", " showed how to use ", "\nin ", " to make it look like the surface of an object reflects an \nenvironment.  Environment mapping uses a ", ", and it is really\njust a way of mapping a cubemap texture to the surface.  It doesn't make the object \nreflect other objects in its environment.  We can make it look as if the object is reflecting\nits environment by adding a ", "\u2014a large cube surrounding the scene, with the cubemap\nmapped onto its interior. However, the object will only seem to be reflecting the\nskybox. And if there are other objects in the environment, they won't be part of\nthe reflection.", "The sample program ", " does environment mapping\nin WebGL.  The program shows a single fully reflective object inside a skybox.  No lighting\nis used in the scene; the colors for both the skybox and the object are taken directly from\nthe cubemap texture.  The object looks like a perfect mirror.  This is not the only way of \nusing an environment map. For example, a basic object color could be computed using the\nlighting equation\u2014perhaps even with an image texture\u2014and the environment map\ncould be blended with the basic color to give the appearance of a shiny but not fully\nreflective surface.  However, the point of the sample program is just to show how to use\na skybox and environment map in WebGL. The shader programs that are used to do that\nare actually quite short.", "As for the cubemap texture itself, ", " showed how to load\na cubemap texture as six separate images and how to access that texture in GLSL using\na variable of type ", ". Remember that a cubemap texture is sampled\nusing a 3D vector that points from the origin towards the point on the cube where the texture is\nto be sampled.", "It's easy to render the skybox:  Draw a large cube, centered at the origin, enclosing\nthe scene and the camera position.  To color a pixel in the fragment shader, sample the\ncubemap texture using a vector that points from the origin through the point on the\ncube that is being rendered, so that the color of a point on the cube is the same as\nthe color of the corresponding point in the cubemap.  Note that it is the cube's object\ncoordinates that are used to sample the texture, since the texture should be attached\nto the cube when we rotate the view.", "In the shader program for rendering a skybox, the vertex shader just needs to\ncompute ", " as usual and pass the object coordinates on to the\nfragment shader in a varying variable.  Here is the vertex shader source code for\nthe skybox:", "And the fragment shader simply uses the object coordinates to get the \nfragment color by sampling the cubemap texture:", "Note that the vector that is used to sample a cubemap texture does not have\nto be a unit vector; it just has to point in the correct direction.", "To understand how a cube map texture can be applied to an object as a reflection map,\nwe have to ask what point from the texture should be visible at a point on the object?\nIf we think of the texture as an actual environment, then a ray of light would come\nfrom the environment, hit the object, and be reflected towards the viewer.  We just have\nto trace that light ray back from the viewer to the object and then to the environment.\nThe direction in which the light ray is reflected is determined, as always, by the\nnormal vector.  Consider a 2D version of the geometry.  You can think of this\nas a cross-section of the 3D geometry:", "\n", "In this illustration, the dotted box\nrepresents the cubemap texture.  (You really should think of it as being at infinite distance.)\n", "\u00a0is a vector that points from the object\ntowards the viewer.  ", "\u00a0is the normal vector to the surface.  And ", " is the reflection of\n", " through\u00a0", ".  ", "\u00a0points to the point in the cubemap texture\nthat is visible to the viewer at the point on the surface; it is the vector that is needed to sample\nthe cubemap texture.  The picture shows the three vectors at two different points on the surface.\nIn GLSL, ", " can be computed as ", "(", ",\u00a0", ").", "If the same cubemap texture is also applied to a skybox, it will look as if the object\nis reflecting the skybox\u2014but ", " if no transformation has been applied to\nthe skybox cube.  The reason is that transforming the skybox does not automatically\ntransform the cubemap texture. \nSince we want to be able to rotate the view, we need to be able to transform the skybox.\nAnd we want the reflected object to look like it is reflecting the skybox in its transformed\nposition, not in its original position.  \nThat viewing transformation can be thought of as\na modeling transformation on the skybox, as well as on\nother objects in the scene.  We have to figure out how to make it apply to the cubemap texture.\nLet's think about what happens in the 2D case when we rotate the view\nby \u221230 degrees.  That's the same as rotating the skybox and object by 30 degrees.  In the \nillustration, I've drawn the viewer at the same position as before, and I have rotated the scene.\nThe square with the fainter dotted outline is the skybox. The cubemap texture\nhasn't moved:", "\n", "If we compute ", " as before and use it to sample the cubemap texture, we get the\nwrong point in the texture.  The viewer should see the point where ", " intersects\nthe skybox, not the point where ", " intersects the texture.  The correct point in the\ntexture is picked out by the vector\u00a0", ".  ", "\u00a0is computed by applying\nthe inverse of the viewing transformation to\u00a0", ".  ", "\u00a0was rotated\nby the viewing transformation; the inverse viewing transformation undoes that transformation,\nputting ", " into the same coordinate system as the cube map.  In this case, since ", "\nwas rotated by 30 degrees, a rotation of \u221230 degrees is applied to compute\u00a0", ".\n(This is just one way to understand the geometry.  If you prefer to think of the cubemap as\nrotating along with the skybox, then we need to apply a texture transformation to the\ntexture\u2014which is another way of saying that we need to transform ", " before using\nit to sample the texture.)", "In the sample program, the shader program that is used to represent the object is\ndifferent from the one used to render the skybox.  The vertex shader is very \ntypical.  Note that the modelview transformation can include modeling transforms\nthat are applied to the object in addition to the viewing transform that is applied\nto the entire scene.  Here is the source code:", "The vertex shader passes eye coordinates to the fragment shader in a varying\nvariable.  In eye coordinates, the viewer is at the point (0,0,0),\nand the vector ", " that points from the surface to the viewer is\nsimply ", ".", "The source code for the fragment shader implements the algorithm\ndiscussed above for sampling the cubemap texture.  Since we are doing\nperfect reflection, the color for the fragment comes directly from the\ntexture:", "The ", " is computed on the JavaScript side from\nthe modelview matrix, after the viewing transform has been applied but\nbefore any addition modeling transformation is applied, using the commands", "We need a ", " to transform a vector.  The first line discards \nthe translation part of the modelview matrix, putting the result in\n", ".  Translation doesn't affect vectors, but the\ntranslation part is zero in any case since the viewing transformation in this program\nis just a rotation.  The second line converts ", " into its inverse."], "chapter_title": "3D Graphics with WebGL", "id": 7.3}, {"section_title": "Painting and Drawing", "chapter_id": "Chapter 1", "section_id": "Section 1.1", "content": ["The main focus of this book is three-dimensional (3D) graphics,\nwhere most of the work goes into producing a 3D model of a scene.\nBut ultimately, in almost all cases, the end result of a computer\ngraphics project is a two-dimensional image.   And of course,\nthe direct production and manipulation of 2D images is an important\ntopic in its own right.  Furthermore, a lot of ideas carry over\nfrom two dimensions to three.  So, it makes sense to start\nwith graphics in 2D.", "An image that is presented on the computer screen is made up of ", ".  The screen\nconsists of a rectangular grid of pixels, arranged in rows and columns.  The pixels are small enough that\nthey are not easy to see individually.  In fact, for many very high-resolution displays, they become\nessentially invisible.  At a given time, each pixel can\nshow only one color.  Most screens these days use 24-bit color, where\na color can be specified by three 8-bit numbers, giving the levels of red, green, and blue in the color.\nAny color that can be shown on the screen is made up of some combination of these three \"primary\" colors.\nOther formats are possible, such as ", ", where each pixel is some shade of gray\nand the pixel color is given by one number that specifies the level of gray on a black-to-white scale.\nTypically, 256 shades of gray are used.\nEarly computer screens used ", ", where only a small set of colors, usually\n16 or 256, could be displayed.  For an indexed color display, there is a numbered list of possible colors,\nand the color of a pixel is specified by an integer giving the position of the color in the list.", "In any case, the color values for all the pixels on the screen are stored in a large block of memory\nknown as a ", ".  Changing the image on the screen requires changing\ncolor values that are stored in the frame buffer.  The screen is redrawn many times per second, so that almost immediately\nafter the color values are changed in the frame buffer, the colors of the pixels on the screen will\nbe changed to match, and the displayed image will change.", "A computer screen used in this way is the basic model of ", ".  The term \"raster\" technically\nrefers to the mechanism used on older vacuum tube computer monitors:  An electron beam would move along\nthe rows of pixels, making them glow.  The beam was moved\nacross the screen by powerful magnets that would deflect the path of the electrons.\nThe stronger the beam, the brighter the glow of the pixel, so the brightness of\nthe pixels could be controlled by modulating the intensity of the electron beam.  The color values\nstored in the frame buffer were used to determine the intensity of the electron beam.  (For a color\nscreen, each pixel had a red dot, a green dot, and a blue dot, which were separately illuminated by\nthe beam.)\n", "A modern flat-screen computer monitor is not a raster in the same sense.  There is no moving\nelectron beam.  The mechanism that controls the colors of the pixels is different for different\ntypes of screen.  But the screen is still made up of pixels, and the color values for all the\npixels are still stored in a frame buffer.  The idea of an image consisting of a grid of\npixels, with numerical color values for each pixel, defines raster graphics.\n", "Although images on the computer screen are represented using pixels, specifying individual\npixel colors is not always the best way to create an image.  Another way\nis to specify the basic geometric objects that it contains, shapes such as lines, circles,\ntriangles, and rectangles.  This is the idea that defines ", ":  Represent an\nimage as a list of the geometric shapes that it contains.  To make things more interesting,\nthe shapes can have ", ", such as the thickness of a line or the\ncolor that fills a rectangle.  Of course, not every image can be composed from simple\ngeometric shapes. This approach certainly wouldn't work for a picture of a beautiful sunset\n(or for most any other photographic image).  However, it works well for many types of\nimages, such as architectural blueprints and scientific illustrations.\n", "In fact, early in the history of computing, vector graphics was even used directly on\ncomputer screens.  When the first graphical computer displays were developed,\nraster displays were too slow and expensive to be practical.  Fortunately, it was\npossible to use vacuum tube technology in another way:  The electron beam could be made\nto directly draw a line on the screen, simply by sweeping the beam along that line.\nA vector graphics display would store a ", " of lines\nthat should appear on the screen.  Since a point on the screen would glow only very briefly\nafter being illuminated by the electron beam, the graphics display would go through\nthe display list over and over, continually redrawing all the lines on the list.\nTo change the image, it would only be necessary to change the contents of the display list.\nOf course, if the display list became too long, the image would start to flicker\nbecause a line would have a chance to visibly fade before its next turn to be redrawn.\n", "But here is the point: For an image that can be specified as a reasonably small\nnumber of geometric shapes, the amount of information needed to represent the image\nis much smaller using a vector representation than using a raster representation.\nConsider an image made up of one thousand line segments.  For a vector representation\nof the image, you only need to store the coordinates of two thousand points, the\nendpoints of the lines.  This would take up only a few kilobytes of memory.  To store\nthe image in a frame buffer for a raster display would require much more memory.  \nSimilarly, a vector display could\ndraw the lines on the screen more quickly than a raster display could copy the the same image from\nthe frame buffer to the screen.  (As soon as raster displays became fast and \ninexpensive, however, they quickly displaced vector displays because of their\nability to display all types of images reasonably well.)\n", "The divide between raster graphics and vector graphics persists in several areas\nof computer graphics.  For example, it can be seen in a division between two\ncategories of programs that can be used to create images:  ", "\nand ", ".  In a painting program, the image is represented\nas a grid of pixels, and the user creates an image by assigning colors to pixels.\nThis might be done by using a \"drawing tool\" that acts like a painter's brush,\nor even by tools that draw geometric shapes such as lines or rectangles. But the point in a\npainting program is to color the individual pixels, and it is only the pixel colors that are saved.\nTo make this clearer, suppose that you use a painting program to draw a house, then\ndraw a tree in front of the house.  If you then erase the tree, you'll only reveal a blank\nbackground, not a house.  In fact, the image never really contained a \"house\" at all\u2014only\nindividually colored pixels that the viewer might perceive as making up a picture of a house.\n", "In a drawing program, the user creates an image by adding geometric shapes, and the\nimage is represented as a list of those shapes.  If you place a house shape (or collection of shapes\nmaking up a house) in the image, and you then place a tree shape on\ntop of the house, the house is still there, since it\nis stored in the list of shapes that the image contains.  If you delete the tree, the house will\nstill be in the image, just as it was before you added the tree.  Furthermore, you should be able\nto select one of the shapes in the image and move it or change its size, so drawing programs\noffer a rich set of editing operations that are not possible in painting programs.  (The reverse,\nhowever, is also true.)\n", "A practical program for image creation and editing might combine elements of painting and\ndrawing, although one or the other is usually dominant.  For example, a drawing program might\nallow the user to include a raster-type image, treating it as one shape.  A painting program\nmight let the user create \"layers,\" which are separate images that can be layered one on top\nof another to create the final image.  The layers can then be manipulated much like the shapes\nin a drawing program (so that you could keep both your house and your tree in separate layers,\neven if in the image of the house is in back of the tree).\n", "Two well-known graphics programs are ", " and ", ".\n", " is in the category of painting programs, while ", " is more of a drawing program.\nIn the world of free software, the GNU image-processing program, ", ", is a good alternative\nto ", ", while ", " is a reasonably capable free drawing program.\nShort introductions to Gimp and Inkscape can be found in ", ".", "The divide between raster and vector graphics also appears in the field of graphics file formats.\nThere are many ways to represent an image as data stored in a file.  If the original image\nis to be recovered from the bits stored in the file, the representation must follow some exact, known\nspecification.  Such a specification is called a graphics file format.  Some popular graphics file\nformats include GIF, PNG, JPEG, and SVG.  Most images used on the Web are GIF, PNG, or JPEG.\nModern web browsers also have support for SVG images.\n", "GIF, PNG, and JPEG are basically raster graphics formats; an image is specified by storing a color \nvalue for each pixel.  GIF is an older file format,\nwhich has largely been superseded by PNG, but you can still find GIF images on the web.  (The GIF\nformat supports animated images, so GIFs are often used for simple animations on Web pages.) GIF uses\nan indexed color model with a maximum of 256 colors.  PNG can use either indexed or full 24-bit color,\nwhile JPEG is meant for full color images. \n", "The amount of data necessary to represent a raster image can be quite\nlarge.  However, the data usually contains a lot of redundancy, and the data can be \"compressed\"\nto reduce its size.  GIF and PNG use ", ", which means that the\noriginal image can be recovered perfectly from the compressed data.\nJPEG uses a ", " algorithm,\nwhich means that the image that is recovered from\na JPEG file is not exactly the same as the original image; some information has been lost.\nThis might not sound like a good idea, but in fact the difference is often not very noticeable, and\nusing lossy compression usually permits a greater reduction in the size of the compressed data.\nJPEG generally works well for photographic images, but not as well for images that have sharp edges\nbetween different colors.  It is especially bad for line drawings and images that contain text; PNG is \nthe preferred format for such images.  \n", "SVG, on the other hand, is fundamentally a vector graphics format (although SVG images can include\nraster images).  SVG is actually an XML-based language for describing two-dimensional vector graphics\nimages.  \"SVG\" stands for \"Scalable Vector Graphics,\" and the term \"scalable\" indicates one of the\nadvantages of vector graphics: There is no loss of quality  when the size of the image is increased.\nA line between two points can be represented at any scale, and it is still the same perfect geometric line.\nIf you try to greatly increase the size of a raster image, on the other hand, you will find that you don't\nhave enough color values for all the pixels in the new image; each pixel from the original image\nwill be expanded to cover a rectangle of pixels in the scaled image, and you will get multi-pixel blocks of\nuniform color.  The scalable nature of SVG images make them a good choice for web browsers and for\ngraphical elements on your computer's desktop.  And indeed, some desktop environments are now using\nSVG images for their desktop icons.", "A digital image, no matter what its format, is specified using a ", ".\nA coordinate system sets up a correspondence between numbers and geometric points.  In two dimensions,\neach point is assigned a pair of numbers, which are called the coordinates of the point.  The two coordinates\nof a point are often called its ", "-coordinate and ", "-coordinate, although the names\n\"x\" and \"y\" are arbitrary.", "A raster image is a two-dimensional grid of pixels arranged into\nrows and columns.  As such, it has a natural coordinate system in which each pixel corresponds \nto a pair of integers giving the number of the row and the number of the column that contain the\npixel.  (Even in this simple case, there is some disagreement as to whether the rows should be numbered from top-to-bottom\nor from bottom-to-top.)", "For a vector image, it is natural to use real-number coordinates.  The coordinate system for an\nimage is arbitrary to some degree; that is, the same image can be specified using different coordinate\nsystems.  I do not want to say a lot about coordinate systems here, but they will be a major\nfocus of a large part of the book, and they are even more important in three-dimensional graphics\nthan in two dimensions."], "chapter_title": "Introduction", "id": 1.1}, {"section_title": "Implementing 2D Transforms", "chapter_id": "Chapter 6", "section_id": "Section 6.5", "content": ["This chapter uses WebGL for 2D drawing.  Of course, the real motivation\nfor using WebGL is to have high-performance 3D graphics on the web.\nWe will turn to that in the ", ".\nWith WebGL, implementing ", " is the\nresponsibility of the programmer, which adds a level of complexity \ncompared to OpenGL\u00a01.1.  But before we attempt to deal with that complexity\nin three dimensions, this short section shows how to implement\ntransforms and ", " in a 2D context.", "Transforms in 2D were covered in ", ".  To review: The basic\ntransforms are scaling, rotation, and translation.  A sequence of such transformations\ncan be combined into a single ", ".  A\u00a02D affine transform maps\na point (", ") to the point (", ") given by formulas of the form", "where ", ", ", ", ", ", ", ", ", ", and ", " are constants.\nAs explained in ", ", this transform can be represented as\nthe 3-by-3 matrix", "\n", "With this representation, a point (", ") becomes the three-dimensional vector\n(", "), and the transformation can be implemented as multiplication of the vector\nby the matrix.", "To apply a transformation to a primitive, each vertex of the primitive has to be\nmultiplied by the transformation matrix.  In GLSL, the natural place to do that\nis in the vertex shader.  Technically, it would be possible to do the multiplication\non the JavaScript side, but GLSL can do it more efficiently, since it can work on multiple\nvertices in parallel, and it is likely that the GPU has efficient hardware support for\nmatrix math.  (It is, by the way, a property of affine transformations that it suffices\nto apply them at the vertices of a primitive. Interpolation of the transformed vertex coordinates\nto the interior pixels of the primitive will given the correct result; that is, it gives the same\nanswer as interpolating the original vertex coordinates and then applying the transformation\nin the fragment shader.)", "In GLSL, the type ", " represents 3-by-3 matrices, and ", " represents three-dimensional\nvectors.  When applied to a ", " and a ", ", the multiplication operator ", "\ncomputes the product.  So, a transform can applied using a simple GLSL assignment statement such as", "For 2D drawing, the original coordinates are likely to come into the vertex shader\nas an ", " of type ", ".  \nWe need to make the attribute value into a \n", " by adding 1.0 as the ", "-coordinate.  The transformation matrix\nis likely to be a ", ", to allow the JavaScript side\nto specify the transformation.  This leads to the following minimal vertex shader\nfor working with 2D transforms:", "In the last line, the value for ", " must be a ", ".  For a 2D point,  the\n", "-coordinate should be 0.0, not 1.0, so we use only the ", "- and ", "-coordinates\nfrom ", ".", "On the JavaScript side, the function ", " is used to specify a\nvalue for a uniform of type ", " (", ").\nTo use it, the nine elements of the matrix must be\nstored in an array in ", ".  For loading an affine transformation matrix into\na ", ", the command would be something like this:", "To work with transforms on the JavaScript side, we need a way to represent\nthe transforms.  We also need to keep track of a\n\"current transform\" that is the product all the individual\n", " that are in effect.\nThe current transformation changes whenever a transformation such as rotation or\ntranslation is applied.  We need a way to save a copy of the current transform\nbefore drawing a complex object and to restore it after drawing.  Typically, a\nstack of transforms is used for that purpose.  You should be well familiar \nwith this pattern from both 2D and 3D graphics.  The difference here is that\nthe data structures and operations that we need are not built into the\nstandard API, so we need some extra JavaScript code to implement them.", "As an example, I have written a JavaScript class, ", ",\nto represent affine transforms in 2D.  This is a very minimal implementation.\nThe data for an object of type ", "\nconsists of the numbers ", ", ", ", ", ", ", ", ", ", and ", "\nin the transform matrix.  There are methods in the class for multiplying the transform\nby scaling, rotation, and translation transforms.  These methods modify the transform\nto which they are applied, by multiplying it on the right by the appropriate matrix.\nHere is a full description of the API, where ", " is an object of type \n", ":", "In fact, an ", " object does not represent an affine\ntransformation as a matrix.  Instead, it stores the coefficients ", ", ", ", ", ", \n", ", ", ", and ", " as properties of the object.  With this representation,\nthe ", " method in the ", " class can defined as\nfollows:", "This code multiplies the transform represented by \"this\" object by a scaling matrix,\non the right.  Other methods have similar definitions, but you don't need to understand\nthe code in order to use the API.", "Before a primitive is drawn, the current transform must sent as a ", " into the\nvertex shader, where the ", " is needed to transform \nthe vertices of the primitive.  The method ", "() returns the \ntransform as an array that can be passed to ", ", which sends\nit to the shader program.", "To implement the stack of transformations, we can use an array of objects of\ntype ", ".  In JavaScript, an array does not\nhave a fixed length, and it comes with ", "() and ", "() methods\nthat make it possible to use the array as a stack.  For convenience,\nwe can define functions ", "() and ", "()\nto manipulate the stack.  Here, the current transform is stored in a global\nvariable named ", ":", "This code is from the sample program ", ",\nwhich demonstrates using ", " and a stack of \ntransforms to implement hierarchical modeling. Here is a screenshot of one of the objects\ndrawn by that program:", "\n", "and here's the code that draws it:", "The function ", "() draws a square that has size 1 and is centered at (0,0) in its\nown object coordinate system.  The coordinates for the square have been stored in a\nbuffer, ", ", and ", " is the location of an attribute variable\nin the shader program.  The variable ", " holds the current modeling transform that\nmust be applied to the square.  It is sent to the shader program by calling", "The second function, ", "(), draws\n16 squares.  Between the squares, it modifies the modeling transform with", "The effect of these commands is cumulative, so that each square is a little smaller than \nthe previous one, and is rotated a bit more than the previous one.  The amount of rotation\ndepends on the frame number in an animation.", "The nested squares are one of three compound objects drawn by the program.  The function\ndraws the nested squares centered at (0,0).  In the main ", "() routine, I wanted to\nmove them and make them a little smaller.  So, they are drawn using the code:", "The ", "() and ", "() ensure that all of the changes\nmade to the modeling transform while drawing the squares will have no effect on \nother objects that are drawn later.  Transforms are, as always, applied to objects in the\nopposite of the order in which they appear in the code.", "I urge you to read the ", "\nand take a look at what it draws.  The essential ideas for working with transforms are\nall there.  It would be good to understand them before we move on to 3D."], "chapter_title": "Introduction to WebGL", "id": 6.5}, {"section_title": "Framebuffers", "chapter_id": "Chapter 7", "section_id": "Section 7.4", "content": ["The term \"frame buffer\" traditionally refers to the region of memory that holds\nthe color data for the image displayed on a computer screen.  In WebGL, a\n", " is a data structure that organizes the \nmemory resources that are needed to render an image.  A\u00a0WebGL graphics\ncontext has a default framebuffer, which is used for the image that \nappears on the screen.  The default framebuffer is created by the call to\n", "() that creates the graphics context. Its properties\ndepend on the options that are passed to that function and cannot be changed after it is created.\nHowever, additional framebuffers can be created, with properties controlled \nby the WebGL program.  They can be used for off-screen rendering, and they\nare required for certain advanced rendering algorithms.", "A framebuffer can use a ", " to hold the color data \nfor an image, a ", " to hold a depth value for each\npixel, and something called a stencil buffer (which is not covered in this\ntextbook).  The buffers are said to be \"attached\" to the framebuffer.\nFor a non-default framebuffer, buffers can be attached and detached by the\nWebGL program at any time.  A framebuffer doesn't need a full set of three buffers, \nbut you need a color buffer, a depth buffer, or both to be able to use the framebuffer for rendering.\nIf the depth test is not enabled when rendering to the framebuffer, then no depth buffer is needed.\nAnd some rendering algorithms, such as ", " (", ")\nuse a framebuffer with a depth buffer but no color buffer.", "The rendering functions ", "() and ", "() affect the\ncurrent framebuffer, which is initially the default framebuffer.  The current framebuffer\ncan be changed by calling", "The first parameter to this function is always ", ".  The second parameter\ncan be ", " to select the default framebuffer for drawing, or it can\nbe a non-default framebuffer created by the function ", "(),\nwhich will be discussed below.", "Before we get to examples of using non-default framebuffers, we look at some\nWebGL settings that affect rendering into whichever framebuffer is current.  Examples that\nwe have already seen include the clear color, which is used to fill the color buffer\nwhen ", "() is called, and the enabled state of the depth test.", "Another example that affects the use of the depth buffer is the ", ",\na boolean value that controls whether values are written to the depth buffer during rendering.\n(The enabled state of the depth test determines whether values from the depth buffer are ", "\nduring rendering; the depth mask determines whether new values are ", " to the depth\nbuffer.)  Writing to the depth buffer can be turned off with the command", "and can be turned back on by calling ", "(", ").  The default\nvalue is ", ".", "One example of using the depth mask is for rendering translucent geometry.  When some of the\nobjects in a scene are translucent, then all of the opaque objects should be rendered first,\nfollowed by the translucent objects.  (Suppose that you rendered a translucent object, and\nthen rendered an opaque object that lies behind the translucent object.  The depth test\nwould cause the opaque object to be hidden by the translucent object.  But \"translucent\" means\nthat the opaque object should be visible through the translucent object.  So it's important\nto render all the opaque objects first.)  Note that the depth test must still be enabled\nwhile the translucent objects are being rendered, since a translucent object can be hidden\nby an opaque object.  Also, ", " must be on while rendering the\ntranslucent objects.", "For fully correct rendering of translucent objects, the translucent\n", " should be\nsorted into back-to-front order before rendering, as in the ", "\n(", ").  However, that can be difficult to implement, and acceptable\nresults can sometimes be obtained by rendering the translucent primitives in arbitrary\norder (but still after the opaque primitives).\nIn fact that was done in the demos ", " from\n", " and ", " from\n", ".  However, it's important to turn off writing\nto the depth buffer, by calling ", "(", "),\nwhile rendering the translucent objects.  The reason is that a\ntranslucent object that is drawn behind another translucent object should be visible\nthrough the front object.", "It is also possible to control writing to the color buffer, using the ", ".\nThe color buffer has four \"channels\" corresponding to the red, green, blue, and alpha components\nof the color.  Each channel can be controlled separately.  You could, for example, allow writing\nto the red and alpha color channels, while blocking writing to the green and blue channels.\nThat would be done with the command", "The ", " function takes four parameters, one for each color channel.  A ", "\nvalue allows writing to the channel; a ", " value blocks writing.  When writing is blocked\nfor a channel during rendering, the value of the corresponding color component is simply ignored.", "One use of the color mask is for ", " rendering (", ").  \nAn anaglyph stereo image contains two images of the scene, one intended for the left eye and one\nfor the right eye.  One image is drawn using only shades of red, while the other uses\nonly combinations of green and blue.  The two images are drawn from slightly different\nviewpoints, corresponding to the views from the left and the right eye.  So the algorithm for\nanaglyph stereo has the form", "One way to set up the views from the left and right eyes is simply to rotate the\nview by a few degrees about the ", "-axis.  Note that the depth buffer, but not\nthe color buffer, must be cleared before drawing the second image, since otherwise\nthe depth test would prevent some parts of the second image from being written.", "Finally, I would like to look at blending in more detail.  Blending refers to\nhow the fragment color from the fragment shader is combined with the current color\nof the fragment in the color buffer.  The default, assuming that the fragment passes\nthe depth test, is to replace the current color with the fragment color.  When blending\nis enabled, the current color can be replaced with some combination of the current\ncolor and the fragment color.  Previously, I have only\ndiscussed turning on alpha blending for transparency with the commands", "The function ", "() determines how the new color is computed\nfrom the current color and the fragment color.  With the parameters shown here,\nthe formula for the new color, using GLSL syntax, is", "where ", " is the \"source\" color (that is, the color that is being written,\nthe fragment color) and ", " is the \"destination\" color (that is, the color\ncurrently in the color buffer, which is the destination of the rendering operation).\nAnd ", " is the alpha component of the source color.  The parameters to\n", "() determine the coefficients\u2014 ", " and (1\u2212", ")\u2014in\nthe formula.  The default coefficients for the blend function are given by", "which specifies the formula", "That is, the new color is equal to the source color; there is no blending.  Other coefficient\nvalues are possible, but I won't use them here.", "Note that blending applies to the alpha component as well as the RGB components of the\ncolor, which is probably not what you want.  When drawing with a translucent color, it means\nthat the color that is written to the color buffer will have an alpha component less than\u00a01.\nWhen rendering to a canvas on a web page, this will make the canvas itself translucent, allowing\nthe background of the canvas to show through.  (This assumes that the WebGL context was\ncreated with an alpha channel, which is the default.)  To avoid that, you can set the blend function\nwith the alternative command", "The two extra parameters specify separate coefficients to be used for the\nalpha component in the formula, while the first two parameters are used only for the RGB components.  \nThat is, the new color for the color buffer is computed using the formula", "With this formula, the alpha component in the destination (the color buffer) remains\nthe same as its original value.", "The blend function set by ", "(", ") can sometimes be\nused in multi-pass algorithms.  In a ", ", a scene is\nrendered several times, and the results are combined somehow to produce the final\nimage.  (Anaglyph stereo rendering is an example.)  If you simply want to add up the\nresults of the various passes, then you can fill the color buffer with zeros, enable\nblending, and set the blend function to (", ") during rendering.", "As a simple example, the sample program ", "\nuses a multi-pass algorithm to implement blurring.  The scene in the example is just a \ntexture image applied to a rectangle, so the effect is to blur the texture image.\nThe technique involves drawing the scene nine times.  In the fragment shader,\nthe color is divided by nine.  Blending is used to add the fragment colors from the\nnine passes, so that the final color in the color buffer is the average of the colors\nfrom the nine passes.  For eight of the nine passes, the scene is offset slightly\nfrom its original position, so that the color of a pixel in the final image is\nthe average of the colors of that pixel and the surrounding pixels from the original\nscene.", "The previous subsection applies to any framebuffer.  But we haven't\nyet used a non-default framebuffer.  We turn to that topic now.", "One use for a non-default framebuffer is to render directly into a texture.\nThat is, the memory occupied by a texture image can be attached to the framebuffer\nas its color buffer, so that rendering operations will send their output to\nthe texture image.  The technique, which is called ", ",\nis used in the sample program ", ".", "Texture memory is normally allocated when an image is loaded into the texture\nusing the function ", " or ", ".  (See ", ".)\nHowever, there is a version of ", " that can be used to allocate memory \nwithout loading an image into that memory.  Here is an example, from the sample program:", "It is the ", " parameter at the end of the last line that tells\n", " to allocate new memory without loading existing image data to fill that memory.\nInstead, the new memory is filled with zeros. The first parameter to ", " is\nthe texture target.  The target is ", " for normal textures,\nbut other values are used for working with cubemap textures.  The fourth and\nfifth parameters specify the height and width of the image; they should be powers of two. \nThe other parameters\nusually have the values shown here; their meanings are the same as for the\nversion of ", " discussed in ", ".\nNote that the texture object must first be created and bound; ", " applies\nto the texture that is currently bound to the active ", ".", "To attach the texture to a framebuffer, you need to create a framebuffer object\nand make that object the current framebuffer by binding it.  For example,", "Then the function ", " can be used to attach the\ntexture to the framebuffer:", "The first parameter is always ", ".  The second parameter\nsays a color buffer is being attached. (The last character in ", "\nis a zero, which allows the possibility of having more than one color\nbuffer attached to a framebuffer.  However, in standard WebGL\u00a01.0, only one color buffer\nis allowed.)  The third parameter is the same texture target that was used in\n", ", and the fourth is the texture object.  The last parameter\nis the ", " level; it will usually be zero, which means rendering\nto the texture image itself rather than to one of its mipmap images.", "With this setup, you are ready to bind the framebuffer and\ndraw to the texture.  After drawing the texture, call", "to start drawing again to the default framebuffer.  The texture is ready for use\nin subsequent rendering operations.  The texture object\ncan be bound to a texture unit, and a ", " variable can be used\nin the shader program to read from the texture.  You are very likely to use\ndifferent shader programs for drawing to the texture and drawing to the screen.\nRecall that the function ", "() is used to specify the shader program.", "In the sample program, the texture is animated, and a new image is drawn to the texture\nfor each frame of the animation.  The texture image is 2D, so the depth test is disabled\nwhile rendering it.  This means that the framebuffer doesn't need a depth buffer.\nIn outline form, the rendering function in the sample program has the form", "Note that the ", " has to be set by hand when drawing\nto a non-default frame buffer.  It then has to be reset when drawing the\non-screen image to match the size of the ", "\nwhere the on-screen image is rendered.  I should also note that only one texture\nobject is used in this program, so it can be bound once and for all\nduring initialization.  In this case, it is not necessary to call \n", "() in the ", "() function.", "This example could be implemented without using a framebuffer, as was done\nfor the example in ", ".  In that example, the\ntexture image was drawn to the default framebuffer, then copied to the\ntexture object.  However,\nthe version in this section is more efficient because it does not need to\ncopy the image after rendering it.", "It is often convenient to use memory from a texture object as the color buffer\nfor a framebuffer.  However, sometimes its more appropriate to create separate memory\nfor the buffer, not associated with any texture.  For the depth buffer, that is the\ntypical case.  For such cases, the memory can be created as a ", ".\nA renderbuffer represents memory that can be attached to a framebuffer for use as\na color buffer, depth buffer, or stencil buffer.  To use one, you need to create the \nrenderbuffer and allocate memory for it.  Memory is allocated using the function\n", "().  The renderbuffer must be bound by calling\n", "() before allocating the memory.  Here is an example\nthat creates a renderbuffer for use as a depth buffer:", "The first parameter to both ", " and ", "\nmust be ", ".  The second parameter to ", "\nspecifies how the renderbuffer will be used.  The value ", "\nis for a depth buffer with 16 bits for each pixel.  (Sixteen bits is the only option.)\nFor a color buffer holding RGBA colors with four eight-bit values per pixel, the second parameter would be ", ".\nOther values are possible, such as ", ", which uses 16 bits per pixel with\n5 bits for the red color channel, 6 bits for green, and 5 bits for blue.  For a\nstencil buffer, the value would be ", ".\nThe last two parameters to ", " are the width and height\nof the buffer.", "The function ", "() is used to attach a renderbufffer\nto be used as one of the buffers in a framebuffer.  It takes the form", "The framebuffer must be bound by calling ", " before\nthis function is called.  The first and third parameters to ", "\nmust be as shown.  The last parameter is the renderbuffer.  The second paramter\nspecifies how the renderbuffer will be used.  It can be\n", ", ", ", or\n", ".", "To render a 3D scene to a framebuffer, we need both a color buffer and\na depth buffer.  An example can be found in the sample\nprogram ", ".  This example uses\nrender-to-texture for a ", ".  The cubemap texture \nis then used as an ", "\non a reflective surface.  In addition to the environment map, the programs\nuses another cubemap texture for a ", ".  (See\n", ".)  Here's is an image from the program:", "\n", "The environment in this case includes the background skybox, but also includes\nseveral colored cubes that are not part of the skybox texture.  The reflective\nsphere in the center of the image reflects the cubes as well as the skybox, which\nmeans that the environment map texture can't be the same as the skybox texture\u2014it\nhas to include the cubes.  Furthermore, the scene can be animated and the cubes can move.  \nThe reflection in the sphere has to change as the cubes move.  This means that\nthe environment map texture has to be recreated in each frame.  For that, we can\nuse a framebuffer to render to the cubemap texture.", "A cubemap texture consists of six images, one each for the positive and negative\ndirection of the ", ", ", ", and ", " axes.  Each image is associated\nwith a different texture target (similar to ", ").  To render \na cubemap, we need to allocate storage for all six sides.  Here's the code from the\nsample program:", "We also need to create a framebuffer, as well as a renderbuffer for use as a depth buffer,\nand we need to attach the depth buffer to the framebuffer.  The same framebuffer\ncan be used to render all six images for the texture, changing the color buffer\nattachment of the framebuffer as needed.  To attach one of the six cubemap images\nas the color buffer, we just specify the corresponding cubemap texture target\nin the call to ", "().  For example, the command", "attaches the negative z image from ", " to be used\nas the color buffer in the currently bound framebuffer.", "After the six texture images have been rendered, the cubemap texture is\nready to be used.  Aside from the fact that six 3D images are rendered instead of\none 2D image, this is all very similar to the render-to-texture example from\nearlier in this section.", "The question remains of how to render the six images of the scene that are needed for the cubemap texture.\nTo make an environment map for a reflective object, we want images of the environment that\nsurrounds that object.  The images can be made with a camera placed at the center\nof the object.  The basic idea is to point the camera in the six directions of the\npositive and negative coordinate axes and snap a picture in each direction,\nbut it's tricky to get the details correct.  (And note that when we apply the result \nto a point on the surface, we will only have an approximation of the correct\nreflection.  For a geometrically correct reflection at the point, we would need the\nview from that very point, not the view from the center of the\nobject, but we can't realistically make a different environment map for each\npoint on the surface.  The approximation will look OK as long as other objects\nin the scene are not too close to the reflective surface.)", "A \"camera\" really means a ", " and a\n", ".  The projection needs a ninety-degree\nfield of view, to cover one side of the cube, and its ", " \nwill be\u00a01, since the sides of the cube are squares.  We can make the projection\nmatrix with a ", " command such as", "where the last two parameters, the near and far clipping distances, should be chosen to\ninclude all the objects in the scene.  If we apply no viewing transformation, the camera\nwill be at the origin, pointing in the direction of the negative ", "-axis.  If the\nreflective object is at the origin, as it is in the sample program, we can use the\ncamera with no viewing transformation to take the negative-z image for the cubemap texture.", "But, because of the\ndetails of how the images must be stored for cubemap textures, it turns out that we\nneed to apply one transformation.  Let's look at the layout of images for\na cubemap texture:", "\n", "The six sides of the cube are shown in black, as if the sides of the cube have\nbeen opened up and laid out flat.  Each side is marked with the corresponding coordinate\naxis direction.  Duplicate copies of the plus and minus y sides are shown in gray,\nto show how those sides attach to the negative z side.  The images that we make for\nthe cubemap must fit together in the same way as the sides in this layout.  However,\nthe sides in the layout are viewed from the ", " of the cube, while\nthe camera will be taking a picture from the ", " of the cube.  To get the\ncorrect view, we need to filp the picture from the camera horizontally.  After some\nexperimentation, I found that I also need to flip it vertically, perhaps because\nweb images are stored upside down with respect to the OpenGL convention.  We can\ndo both flips with a ", " transformation by (\u22121,\u22121,1).\nPutting this together, the code for making the cubemap's negative z image is", "The function in the last line renders the scene, except for the central reflective\nobject itself, and is responsible for sending the projection and modleview matrices\nto the shader programs.", "For the other five images, we need to aim the camera in a different direction\nbefore taking the picture.  That can be done by adding an appropriate rotation\nto the viewing transformation.  For example, for the positive x image, we need to\nrotate the camera by \u221290 degrees about the y-axis. As a viewing transform,\nwe need the command", "It might be easier to think of this as a modeling transformation that\nrotates the positive x side of the cube into view in front of the camera.", "In the ", ",\nthe six cubemap images are created in the function ", "().\nRead the source code of that function for the full details.", "This dynamic cubemap\nexample is a nice way to end the textbook, since it makes use of so many of the concepts and\ntechniques that we have covered.  Take a minute to think about everything that is\ngoing on in this demo version of the sample program, and how it was all implemented:", "\n", "\n"], "chapter_title": "3D Graphics with WebGL", "id": 7.4}, {"section_title": "Path Tracing", "chapter_id": "Chapter 8", "section_id": "Section 8.2", "content": ["We have seen how ray tracing can be extended to approximate a variety of\neffects that are not handled by the basic algorithm.  We look next at an\nalgorithm that accounts for all those effects and more in a fairly\nstraightforward and unified way: ", ".   Like ray tracing,\npath tracing computes colors for points in an image by tracing the\npaths of light rays backwards from the viewer through points on the\nimage and into the scene.  But in path tracing, the idea is to \naccount for ", " possible paths that the light could have\nfollowed.  Of course, that is not literally possible, but following a\nlarge number of paths can give a good approximation\u2014one that\ngets better as the number of paths is increased.", "In order to model a wide variety of physical phenomena, path tracing\nuses a generalization of the idea of ", " property.\nIn OpenGL, a material is a combination of ", ", \n", ", ", ", \nand ", " colors, plus ", ".\nThese properties, except for emission color, model how the surface interacts with\nlight.  Material properties can vary from point to point on a surface; that's an\nexample of a ", ".", "OpenGL material is only a rough approximation of reality.  In path tracing, a more general\nnotion is used that is capable of more accurately representing the properties of\nalmost any real physical surface or volume.  The replacement for materials\nis call a ", ", or Bidirectional Scattering Distribution\nFunction.", "Think about how light that arrives at some point can be affected by the\nphysical properties of whatever substance exists at that point.  Some of the\nlight might be absorbed.  Some might pass through the point without\nbeing affected at all.  And some might be \"scattered,\" that is,\nsent off in another direction.  In fact, we consider passing through the point\nas a special case of scattering.  A BSDF describes how light is scattered\nfrom each point on a surface or in a volume.", "Think of a single ray, or photon, of light that arrives at some point.  What happens to it can\ndepend on the direction from which it arrives.  In general, assuming that it is not absorbed, the light is more \nlikely to be scattered in some directions than in others.  (As in specular reflection,\nfor example.)  The BSDF at the \npoint gives the probability that the ray will leave the point heading in a \ngiven direction.  It is a \"bidirectional\" function because the answer is\na function of two directions, the direction from which the light arrives and\nthe outgoing direction that you are asking about.  (It is a \"distribution function\" in\nthe sense of the mathematical theory of continuous probability distributions,\nbut you don't need to understand that to get the general idea.  For us,\nit's enough to understand that the function says how light coming in from a\ngiven direction is distributed among possible outgoing directions.)\nNote that a BSDF is also a function of the point that you are talking about,\nand it can be a function of the wavelength of the light as well.", "Any point in space can be assigned a BSDF.  For empty space, the BSDF is trivial:\nIt simply says that light arriving at a point has a 100% probability of continuing\nin the same direction.  But light passing through fog or dusty air or dirty water has some\nprobability of being absorbed and some probability of being scattered to a random\ndirection.  Similar remarks apply to light passing through the interior of a\ntranslucent solid object.", "Traditionally, though, computer graphics has been mostly concerned with what\nhappens to light at the surface of an object.  Light can be absorbed or reflected or,\nif the object is translucent, transmitted through the surface.\nThe function that describes the reflection of light from a surface is sometimes\ncalled a BRDF (Bidirectional Reflectance Distribution Function), and the formula\nfor transmission of light is a BTDF (Bidirectional Transmission Distribution function).\nThe BSDF for a surface is a combination of the two.", "Let's consider OpenGL materials in terms of BSDFs.  In basic OpenGL, light can only\nbe reflected or absorbed.  For diffuse reflection, light has an\nequal probability of being reflected in every direction that makes an angle of less than\n90 degrees with the normal vector to the surface, and there is no dependence on\nthe direction from which the light arrives.  For specular reflection, the incoming\nlight direction matters.  In OpenGL, the possible outgoing directions for specularly reflected light form a cone,\nwhere the angle between the axis of the cone and the normal vector is equal to the angle between\nthe normal vector and the incoming light direction.  The axis of the cone is the most \nlikely direction for outgoing light, and the probability falls off as the angle between the outgoing\ndirection and\nthe direction of the axis increases.  The rate of falloff is specified by the shininess property of the material.\nThe BRFD for the surface combines the diffuse and specular reflection.  (The ambient material\nproperty doesn't fit well into the BSDF framework, since physically there is no such\nthing as an \"ambient light\" that is somehow different from regular light.)", "Ray tracing adds two new possibilities to the interaction of light with a surface:\nperfect, mirror-like reflection, where the outgoing light makes exactly the same angle\nwith the normal vector as the incoming light, and transmission of light into a translucent\nobject, where the outgoing angle is determined by the indices of refraction outside and\ninside the object.", "But BSDFs can provide even more realistic models of the interaction of light with\nsurfaces.  For example, the distinction between mirror-like reflection of an object \nand specular reflection of a light source is artificial.  A perfect mirror\nshould reflect both light sources and objects in a mirror-like way.  For a shiny but\nrough surface, all specular reflection would send the light in a cone of directions,\ngiving fuzzy images of objects and lights alike. A BSFD should handle both cases, and\nit shouldn't distinguish between light from light sources and light reflected off\nother objects.", "BSDFs can also correctly handle a phenomenon called ", ", which\ncan be an important visual effect for materials that are just a bit translucent, such as\nmilk, jade, and skin.  In sub-surface scattering, light that hits a surface can be \ntransmitted into the object, be scattered a few times internally inside the object, and\nthen emerge from the surface at another point.  How the light behaves inside the object is determined\nby the BSDF of the material in the interior of the object.  The BSDF in this case would\nbe similar to the one for fog, except that the probability of scattering would be larger.", "The point is that just about any physically realistic material can be modeled\nby a correctly chosen BSDF.", "Path tracing is based on a formula known as the \"rendering equation.\"  The formula\nsays that the amount of light energy leaving a given point in a given direction\nis is equal to the amount of light energy emitted by the point in that direction\nplus the amount of light energy arriving at the point from other sources that\nis then scattered in that direction.", "Here, emitted light means light that is created, as by a light source.  In the rendering\nequation, any object can be an emitter of light.  In OpenGL terms, it's as if an object\nwith an emission color actually emits light that can illuminate other objects.  An\narea light is just an extended object that emits light from every point, and it is common\nto illuminate scenes with large light-emitting objects.  (In fact, in\na typical path tracing setup, point lights and directional lights have to be assigned\nsome area to make them work correctly in the algorithm.)", "As for scattered light, the BSDF at a point determines how light arriving at a point is scattered.\nLight can, in general, arrive from any direction\nand can originate from any other point in the scene.  The rendering equation holds\nat ", " point.  It relates the light arriving at and departing from each point to\nthe light arriving at and departing from every other point.  It describes, in other words,\nan immensely complicated system, one for which you are unlikely to be able to find an\nexact solution.  A rendering algorithm can be thought of as an attempt to find\na good approximate solution to the rendering equation.", "Path tracing is a probabilistic rendering algorithm.  It looks at possible paths\nthat might have been followed by light arriving at the position of the viewer.\nEach possible path has a certain probability. Path tracing generates a random\nsample of possible paths, choosing paths in the sample according to their probabilities.\nIt uses those paths to create an image that approximates a \nsolution to the rendering equation.  It can be shown that as the size of the random\nsample increases, the image that is generated will approach the true solution.\nTo get a good quality image, the algorithm will have to trace thousands of paths\nfor each pixel in the image, but the result can be an almost shocking level of\nrealism.", "Let's think about how it should work.  First, consider the case where light\nis only emitted and reflected by surfaces.  As with ray tracing, we start at\nthe position of the viewer and cast a ray in the direction of a point on the\nimage, into the scene.  (See ", ".) \nWe find the first intersection of that ray with an object in the scene. \nOur goal to trace one possible path that the ray could have followed from \nits point of origin until it arrives at the viewer, and we want the\nprobability that we select a given path to be the probability that the\nlight actually followed that path.  This means that each time the light is\nscattered from a surface, we should choose the direction of the next segment\nof the path based on the BSDF for the surface.  That is, the direction is\nchosen at random, using the probability distribution that is encoded in the BSDF.\nWe construct the next segment of the path by casting a ray in the selected\ndirection.", "We continue to trace the path, backwards in time, possibly through multiple reflections, until it\nencounters an object that emits light.  That object serves as the original\nsource of the light.  The color that the path contributes to the image is\ndetermined by the color and intensity of the emitter, by the colors of \nsurfaces that the light hits along the way,\nand by the angles at which the light hits each surface.  If the path escapes\nfrom the scene before it hits a light emitting object, then it does not\ncontribute any color to the image. (It might be desirable to have a light-emitting\nbackground, like a sky, that emits light over a large area.)  Note that it\nis possible for an object to be both an emitter and a reflector of light.\nIn that case, a path can continue even after it gets to a light source.", "Of course, we have to trace many such paths.  The color for a pixel in\nthe image is computed as an average of the colors obtained for all the paths\nthat pass through that pixel.", "The algorithm can be extended to handle the case where light can be\nscattered at arbitrary points in space, and not just at surfaces.\nFor light traveling in a medium in 3D space, the question is, how far will the light travel before it is\nscattered?  The BSDF for the medium will determine a probability distribution on possible travel distances\nbetween scatterings.  When light enters a medium, that probability distribution is used to select a\nrandom distance that the light will travel before it is scattered (unless it hits a surface or enters a\nnew medium before it has traveled that distance).  When it scatters from a point in the medium, a\nnew direction and length are chosen at random for the next segment of the path, according to the BSDF\nof the medium.  For a light fog, the average distance between scatterings would be quite large;\nfor a dense medium like milk, it would be quite short.", "A great deal of computation is required to trace enough light paths to get\na high-quality image.  Although path tracing was invented in the 1980s, it\nis only recently that it has become practical for general use, and it can still\ntake many hours to get acceptable quality.  In fact, you can do path tracing\non your desktop computer using the 3D modeling program Blender, which is discussed\nin ", ".  Blender has an alternative rendering engine,\ncalled the Cycles renderer, that uses path tracing.  Cycles is not discussed in the\nappendix, but you can look up some tutorials on it, if you are interested in\nseeing what path tracing can do."], "chapter_title": "Beyond Realtime Graphics", "id": 8.2}, {"section_title": "Ray Tracing", "chapter_id": "Chapter 8", "section_id": "Section 8.1", "content": ["Ray tracing is probably the best known technique for higher quality\ngraphics.  The idea behind it is not complicated:  To find out what you see when you\nlook in a given direction, consider a ray of light that arrives at your location\nfrom that direction, and follow that light ray backwards to see where it came\nfrom.  Or, as it is usually phrased, cast a ray from your location in a given\ndirection, and see what it hits.  That's what you see when you look in that\ndirection.  The operation of determining what is hit by a ray is called\n", ".  It is fundamental to ray tracing and to \nother advanced graphics techniques.", "We have already seen ray casting used by objects of type ", " in the\n", " API (", ").  A ", " takes an initial\npoint and a direction, given as a ", ".  The point and vector determine\na ray, that is, a half-infinite line that extends from a starting point, in some direction,\nto infinity.  The ", " can find all the intersections of the ray\nwith a given set of objects in a ", " scene, sorted by order of distance from the rays's starting point.\nIn this chapter, we are interested in the first intersection, the one that is closest to the\nstarting point.", "To apply ray casting to rendering, let's say that we have a scene consisting of objects in\nthree-dimensional space, using ", " and\n", " for illumination.  We want to render\nan image of the scene from a given point of view.  It's convenient to imagine the image\nas a kind of rectangular window through which the scene is being viewed.  Given a point in\nthe image, we need to know how to color that point.  To compute a color for the point, we begin by casting a\nray from the position of the viewer through the point and into the scene.  We want to\nfind the first intersection of that ray with an object in the scene:", "\n", "In this illustration, the scene contains several red spheres and two point lights.  A ray\nis cast from the viewpoint (A) through a point (B) in the image.  The ray intersects two\nof the spheres, but we are only interested in the intersection point (C) that is closest\nto the viewpoint.  That's the point that is visible at B in the image.", "We need to compute the color that the viewer will see at point\u00a0B.  For that, just as in OpenGL,\nwe need a normal vector at point C, and we need the material properties of the surface at\u00a0C.  That\ndata has to be computable from the scene description.  The visible color also depends on the\nlight that is illuminating the surface.  Consider a light source, and let ", " be the\nvector that points from C in the direction of the light.  If the angle\nbetween ", " and the normal vector is greater than 90 degrees, then the light source lies\nbehind the surface and so does not add any illumination.  Otherwise, we can use ray casting\nagain:  Cast a ray from C in the direction ", ".  If that ray hits an object before\nit gets to the light, then that object will block light from that source from reaching\u00a0C.\nThat's the case for Light\u00a02 in the illustration: The ray from C in the direction of \nLight\u00a02 intersects an object at point\u00a0E before it gets to the light. On the other hand, the point C is\nilluminated by Light\u00a01.  A ray from a point on a surface in the direction of a \nlight source is called a ", ", because it can be used to\ndetermine whether the surface point lies in the shadow of another object.", "With all this information, we can color point B in the image\nusing the same lighting equation that is used in OpenGL (", ").\nAnd, as a bonus, we get shadows, which are hard to do in OpenGL!", "(If the ray from the\nviewpoint through B doesn't hit any objects in the scene, then B would be assigned\na background color or a \"sky\" color that varies with different directions.  Or maybe\nthe entire scene is surrounded by a ", ", and a ray that doesn't hit\nany other object will hit the skybox.)", "Ray casting is conceptually simple, but implementation details can be tricky.\nSpheres are actually fairly easy.  There is a formula for finding the intersections of\na line with a sphere, and a normal vector at a point on a sphere has the same\ndirection as the line from the center of the sphere to that point.  Assuming\nthat the sphere has a uniform material, we have all the data we need.", "However, surfaces are often given as triangular ", ",\nwith properties specified only at the vertices of the triangles.  Suppose that the\nintersection point found by our ray caster lies in one of those triangles.\nWe will have to compute the properties of that intersection point by \n", " the property values that were specified\nat the vertices of the triangle.", "The interpolation algorithm typically uses\nsomething called ", " on the triangle:\nIf A, B, and C are the vertices of a triangle, and P is a point in the triangle,\nthen P can be written uniquely in the form ", "*A\u00a0+\u00a0", "*B\u00a0+\u00a0", "*C,\nwhere ", ", ", ", and ", " are numbers in the range zero to one, and\n", "\u00a0+\u00a0", "\u00a0+\u00a0", "\u00a0=\u00a01.\nThe coefficients ", ", ", ", and ", " are called the barycentric coordinates\nof the point P in the triangle.   If some quantity has values V(A), V(B), and V(C) at the\nvertices of the triangle, then we can use the barycentric coordinates of P to get an interpolated\nvalue at\u00a0P:", "The quantity might be, for example, diffuse color, texture coordinates, or a normal vector.\n(Of course, I'm still leaving out a lot of the math here\u2014how to test whether a line\nintersects a triangle and how to find barycentric coordinates of the point of intersection.\nThere are formulas, but conceptually, they wouldn't add much to the discussion.)", "Basic ray casting can be used to compute OpenGL-style rendering and, with the addition of\nshadow rays, to implement shadows as well.  More features can be implemented by casting\na few more rays.  The improved algorithm is called ", ".", "Consider ", ".  In OpenGL, specular reflection \ncan make an object look shiny in the sense that specular highlights can be seen where\nthe object reflects light from a light source towards the viewer.  But in reality,\nan object that has a mirror-like surface doesn't just reflect light sources; it also reflects\nother objects.  If we are trying to compute a color for a point, ", ", on a mirror-like surface,\nwe need to consider the contribution to that color from mirror-like reflection of other\nobjects.  To do that, we can cast a \"reflected ray\" from\u00a0", ".  The direction of the\nreflected ray is determined by the normal vector to the surface at ", " and by the\ndirection from ", " to the viewer.  This illustration shows a 2D version.  Think of it\nas a cross-section of the situation in 3D:", "\n", "Here, the reflected ray from point ", " hits the purple square at point\u00a0", ", and\nthe viewer will see a reflection of point ", " at\u00a0", ".  (Remember that in ray tracing,\nwe follow the path of light rays ", " from the viewer, to find out where they came from.)", "To find out what the reflection of ", " looks like, we need to know the color of the the\nray that arrives at ", " from\u00a0", ".  But finding a color for ", " is the same sort\nof problem as finding a color for ", ", and we should solve it in the same way: by applying the ray-tracing\nalgorithm to\u00a0", "!  That is, we use the material properties of the surface at ", ", we\ncast shadow rays from ", " towards light sources to determine how ", " is illuminated, and\u2014if\nthe purple square has a mirror-like surface\u2014we cast a reflected ray from ", " to find out\nwhat it reflects.  In the illustration, the reflected ray from ", " hits a pentagon at point\u00a0", ",\nso the square reflects an image of the pentagon, and the disk reflects an image of the square, including\nits reflection of the pentagon.  Ray-tracing can handle multiple mirror-like reflections between objects\nin a scene!", "Because applying the ray-tracing algorithm at one point can involve applying the same algorithm\nat additional points, ray tracing is a recursive algorithm.  To distinguish this from simple\nray casting, ray tracing is often referred to as \"recursive ray tracing.\"", "Ray tracing can be extended in a similar way to handle transparency or, more properly, translucency.\nWhen computing a color for a point on a translucent object, we need to take into account light\nthat arrives at that point through the object.  To do that, we can cast yet another ray from that point,\nthis time ", " the object.  When a light ray passes from one medium, such as air, into \nanother medium, such as glass, the path of the light ray can bend.  This bending is called\n", ", and the ray that is cast through a translucent object is called\nthe \"refracted ray.\"  The above illustration shows the refracted ray from point ", " passing\nthrough the object and emerging from the object at\u00a0", ".  To find a color for that\nray, we would need to find out what, if anything, it hits, and we would need to apply ray tracing\nrecursively at the point of intersection.", "(The degree of bending of a light ray that passes from one medium to another\ndepends on a property of the two media called the \"index of refraction.\"\nMore exactly, it depends on the ratio between\nthe two indices of refraction.  In practice, the index of refraction outside\nobjects is often taken to be equal to one, so that the bending depends only on the index of\nrefraction of the objects.  The index of refraction is another material property for\ntranslucent objects.  It is often abbreviated ", ".)", "We should look at the computations in a little more detail.  The goal is to compute a color\nfor a point on an image.  We cast a ray from the viewpoint through the image and into the scene,\nand determine the first intersection point of the ray with an object.  The color of that point\nis computed by adding up the contributions from different sources.", "First, there are diffuse, specular, and possibly ambient reflection of light from various light sources.  These\ncontributions are based on the diffuse, specular, and ambient colors of the object, on the normal vector\nto the object, and on the properties of the light sources.  Note that some color properties of the object,\nusually the ambient and diffuse colors, might come from a texture.  The specular contribution can produce\nspecular highlights, which are essentially the reflections of light sources.\nShadow rays are used to determine which directional and point lights illuminate the object;\naside from that, this part of the calculation is similar to OpenGL.", "Next, if the surface has mirror-like reflection, then a reflected ray is cast and ray tracing is applied\nrecursively to find a color for that ray.  The contribution from that ray is combined with other\ncontributions to the color of the surface, depending on the strength of the mirror reflection.\nFor a perfect mirror, the reflected ray contributes 100% of the color, but in general the contribution\nis less.  Mirror reflectivity is a new material property.  It is responsible for reflections of\none object on the surface of another, while specular color is responsible for specular highlights.", "Finally, if the object is translucent, then a refracted ray is cast, and ray tracing is used to\nfind its color.  The contribution of that ray to the color of the object depends on the degree\nof transparency of the object, since some of the light can be absorbed rather than transmitted.\nThe degree of transparency can depend on the wavelength of the light\u2014as it does, for example,\nin colored glass.", "And, of course, all of these calculation need to be done three times, for the red, \nthe green, and the blue components of the color.", "The ray tracing algorithm is recursive, and, as every programmer knows, recursion needs\na base case.  That is, there has to come a time when, instead of calling itself, the algorithm\nsimply returns a value.  A base case occurs whenever a casted ray does not intersect any objects.\nAnother kind of base case can occur when it is determined that casting more rays cannot\ncontribute any significant amount to the color of the image.  For example, whenever a ray is\nreflected, some of that ray is absorbed rather than reflected, depending on the color of the\nobject from which it is reflected.  After reflecting many times, a ray would have very little\ncolor left to contribute to the final result.  A ray can also lose energy because of\n", " of light with distance, and a ray-tracing algorithm might take that\ninto account.  In addition, a ray tracing algorithm should always be run with a maximum\nrecursion depth, to put an absolute limit on the number of times the algorithm will\ncall itself.", "Although ray tracing can produce very realistic images, there are some things that\nit can't do.  For example, while ray  tracing works well for point lights and \ndirectional lights, it can't handle area lights.  An area light is one that has\narea.  That is, it is an object that emits light from its entire surface area\nrather than from a single point. Of course, real lights are area lights.  A fluorescent\nlight emits light from the surface of a cylinder.  A brightly lit window can be \nconsidered to be a kind of area light.  Even a light bulb does not really radiate light \nfrom a single point.  Ray tracing uses shadow rays to tell whether a light\nsource illuminates an object.  But a shadow ray is cast in only one direction,\nand can only hit one point on an area light.  To implement area lights exactly,\na different shadow ray would be needed for each point on the surface of the light.", "A ray tracer can handle area lights in an approximate way, by replacing the\narea light with a grid of point lights.  It can then cast a shadow ray towards\neach point in the grid.  Using more point lights in the grid will give a better\napproximation. Of course, casting all those shadow rays can add significantly\nto the computation time.", "Another problem with lighting in ray tracing is that it doesn't take into account \nillumination by reflected light.  For example,\nlight from a light source should reflect off a mirror, and the reflected light should\nilluminate other objects.  Ray tracing uses shadow rays to tell whether a light\nsource illuminates an object.  But that won't work for reflected light, since the\nalgorithm doesn't know where to aim the shadow ray\u2014reflected light could\ncome from any direction.", "This is not just a problem with mirrors.  Any reflected light, even from diffuse\nreflection, should illuminate nearby objects.  For example, light that is reflected\ndiffusely from a green object should add a green tint to nearby objects.  This effect\nis called \"color bleeding.\"  In reality, light can be reflected and re-reflected \nmany times, contributing a bit of color to each object that it hits.  As with \narea lights, approximate methods can be added to ray tracing to simulate this effect.\nFor example, \"photon mapping\" adds a pre-processing phase to ray tracing which simulates\nthe emission of a large number of light rays, or \"photons,\" from light sources, and tracks their paths\nthrough the scene to see how they add color to the objects that they hit.  The\ninformation from this \"photon map\" is then used during the ray tracing phase to\nproduce more realistic colors.", "OpenGL uses ", " as an approximation for light that has been\nreflected and re-reflected many times.  Ambient light is assumed to illuminate every object equally.\nHowever, that is a poor approximation.  A better approximation uses ", ",\nthe idea that ambient light heading towards a surface can be blocked, or \"occluded,\" by \nnearby objects.  Like ambient light, ambient occlusion is not physically realistic,\nbut in practice, it can make lighting look more realistic and objects look more three-dimensional.\nOne technique for estimating ambient occlusion uses ray casting.\nWe assume that the ambient light comes from the background of the scene.  To\nestimate ambient occlusion at a point, cast a number of rays from that point in random\ndirections, and count how many of those rays are blocked by geometry in the scene and how many\nreach the sky. The more rays that are blocked, the greater the degree of ambient occlusion\nat that point.", "Approximate methods such as these can be added on to ray tracing to increase its\nrealism, at the cost of significantly increased complexity and a potentially large \namount of extra computing.  We begin to see why very realistic images can take so\nlong to compute!"], "chapter_title": "Beyond Realtime Graphics", "id": 8.1}, {"section_title": "3D Coordinates and Transforms", "chapter_id": "Chapter 3", "section_id": "Section 3.2", "content": ["In ", ", we looked fairly closely at \n", " and \n", " \nin two-dimensional computer graphics.  In this section and the\n", ", we will move that \ndiscussion into 3D.  Things are more complicated in three\ndimensions, but a lot of the basic concepts remain the\nsame.", "A coordinate system is a way of assigning numbers to points.\nIn two dimensions, you need a pair of numbers to specify a\npoint.  The coordinates are often referred to as ", " and\n", ", although of course, the names are arbitrary.  More than that,\nthe assignment of pairs of numbers to points is itself arbitrary to\na large extent.  Points and objects are real things, but\ncoordinates are just numbers that we assign to them so that\nwe can refer to them easily and work with them mathematically.\nWe have seen the power of this when we discussed transforms, \nwhich are defined mathematically in terms of coordinates but which have\nreal, useful physical meanings.", "In three dimensions, you need three numbers to specify a point.\n(That's essentially what it means to be three dimensional.)\nThe third coordinate is often called ", ".  The ", "-axis\nis perpendicular to both the ", "-axis and the ", "-axis.", "This demo illustrates a 3D coordinate\nsystem.  The positive directions of the ", ", ", ",\nand ", " axes are shown as big arrows.  The ", "-axis is green,\nthe ", "-axis is blue, and the ", "-axis is red. You can drag on the axes to rotate the image.", "\n", "\n", "This example is a 2D image, but it has a 3D look.  (The illusion is\nmuch stronger if you rotate the image.)  Several things\ncontribute to the effect.  For one thing, objects that are farther\naway from the viewer in 3D look smaller in the 2D image.  This is\ndue to the way that the 3D scene is \"projected\" onto 2D.  We will\ndiscuss projection in the ", ".\nAnother factor is the \"shading\" of the objects.  The objects are shaded\nin a way that imitates the interaction of objects with the light that\nilluminates them.  We will put off a discussion of lighting until\n", ".  In this section, we will concentrate on\nhow to construct a scene in 3D\u2014what we have referred to as\n", ".", "OpenGL programmers usually think in terms of a coordinate system in which\nthe ", "- and ", "-axes lie in the plane of the screen, and the ", "-axis is perpendicular\nto the screen with the positive direction of the ", "-axis pointing ", " the screen towards the viewer.   Now, the default\ncoordinate system in OpenGL, the one that you are using if you\napply no transformations at all, is similar but has the positive direction of the ", "-axis\npointing ", " the screen.  This is not a contradiction:  The coordinate\nsystem that is actually used is arbitrary.  It is set up by a transformation.\nThe convention in OpenGL is to work with a coordinate system in which the\npositive ", "-direction points toward the viewer and the negative\n", "-direction points away from the viewer.  The transformation into\ndefault coordinates reverses the direction of the ", "-axis.", "This conventional arrangement of the axes produces a \n", ".  This means that if\nyou point the thumb of your right hand in the direction of the positive\n", "-axis, then when you curl the fingers of that hand, they will curl\nin the direction from the positive ", "-axis towards the positive ", "-axis.\nIf you are looking at the tip of your thumb, the curl will be in the counterclockwise\ndirection.  Another way to think about it is that if you curl the figures of your\nright hand from the positive ", " to the positive ", "-axis, then your\nthumb will point in the direction of the positive ", "-axis.\nThe default OpenGL coordinate system (which, again, is hardly ever used)\nis a left-handed system.  You should spend some time trying to visualize\nright- and left-handed coordinates systems.  Use your hands!", "All of that describes the natural coordinate system from the viewer's point of view,\nthe so-called \"eye\" or \"viewing\" coordinate system.\nHowever, these ", " are not necessarily the natural coordinates on the world.\nThe coordinate system on the world\u2014the coordinate system in\nwhich the scene is assembled\u2014is referred to as\n", ".", "Recall that objects are not usually specified directly in world coordinates.\nInstead, objects are specified in their own coordinate system,\nknown as ", ", and then ", "\nare applied to place the objects into the world, or into more complex objects.\nIn OpenGL, object coordinates are the numbers that are used in the\n", " function to specify the vertices of the object.  However,\nbefore the objects appear on the screen, they are usually subject to a sequence\nof transformations, starting with a modeling transform.", "The basic transforms in 3D are extensions of the basic transforms that\nyou are already familiar with from 2D:  ", ", \n", ", and ", ".  We will look at\nthe 3D equivalents and see how they affect objects when applied as\nmodeling transforms.  We will also discuss how to use the transforms\nin OpenGL.", "Translation is easiest.  In 2D, a translation adds some number onto each coordinate.  The\nsame is true in 3D; we just need three numbers, to specify the amount of motion\nin the direction of each of the coordinate axes.  A translation by (", ")\ntransforms a point (", ") to the point (", ").\nIn OpenGL, this translation would be specified by the command", "or by the command", "The translation will affect any drawing that is done after the command is given.\nNote that there are two versions of the command.  The first, with a name ending in \"f\",\ntakes three ", " values as parameters.  The second, with a name ending in \"d\",\ntakes parameters of type ", ".  As an example,", "would translate objects by one unit in the ", " direction.", "Scaling works in a similar way: Instead of one scaling factor, you need three.  The\nOpenGL command for scaling is ", ", where the \"*\" can be either \"f\" or \"d\".\nThe command", "transforms a point (", ") to (", ").  That is,\nit scales by a factor of ", " in the ", " direction, ", " in the ", "\ndirection, and ", " in the ", " direction.  Scaling is about the origin;\nthat is, it moves points farther from or closed to the origin, (0,0,0).  For\nuniform scaling, all three factors would be the same.  You can use scaling by\na factor of minus one to apply a reflection.  For example,", "reflects objects through the ", "-plane by reversing the sign of the ", "\ncoordinate.  Note that a reflection will convert a right-handed coordinate system into\na left-handed coordinate system, and ", ".  Remember\nthat the left/right handed distinction is not a property of the world, just\nof the way that one chooses to lay out coordinates on the world.", "Rotation in 3D is harder.  In 2D, rotation is rotation about a point, which is usually\ntaken to be the origin.  In 3D, rotation is rotation about a line, which is called\nthe ", ".  Think of the Earth rotating\nabout its axis.  The axis of rotation is the line that passes through the North Pole \nand the South Pole.  The axis stays fixed as the Earth rotates around it,\nand points that are not on the\naxis move in circles about the axis.  Any line can be an axis of rotation, but\nwe generally use an axis that passes through the origin.  The most\ncommon choices for axis or rotation are the coordinates axes, that is,\nthe ", "-axis, the ", "-axis, or the ", "-axis.  Sometimes,\nhowever, it's convenient to be able to use a different line\nas the axis.", "There is an easy way to specify a line that\npasses through the origin:  Just specify one other\npoint that is on the line, in addition to the origin.   That's how things are\ndone in OpenGL:  An axis of rotation is specified by three numbers,\n(", "), which are not all zero.  The axis is the line\nthrough (0,0,0) and (", ").  To specify a rotation transformation in 3D,\nyou have to specify an axis and the angle of rotation about that axis.", "We still have to account for the difference between positive and negative\nangles.  We can't just say clockwise or counterclockwise.  If you look down on\nthe rotating Earth from above the North pole, you see a clockwise rotation; if you\nlook down on it from above the South pole, you see a counterclockwise rotation.\nSo, the difference between the two is not well-defined.  To define the\ndirection of rotation in 3D, we use the ", ", which\nsays:  Point the thumb of your right hand in the direction of the\naxis, from the point (0,0,0) to the point (", ") that determines the\naxis. Then the direction of rotation for positive angles is given by the\ndirection in which your fingers curl. I should emphasize that the right-hand rule\nonly works if you are working in a right-handed coordinate system.  If you have\nswitched to a left-handed coordinate system, then you need to use a\nleft-hand rule to determine the positive direction of rotation.", "You can use the following demo to help you understand rotation about an\naxis in three-dimensional space.  Use the buttons labeled \"+X\", \"-X\", and so on to\nmake the cube rotate about the coordinate axes, or enter any (", ") point\nand click \"Set\".  Drag your mouse on the image to rotate the \nscene.", "\n", "\n", "The rotation function in OpenGL is ", "(", ").\nYou can also use ", ".\nThe first parameter specifies the angle of rotation, measured in degrees.\nThe other three parameters specify the axis of rotation, which is the line\nfrom (0,0,0) to (", ").", "Here are a few examples of scaling, translation, and scaling in OpenGL:", "Remember that transforms are applied to objects that are drawn after\nthe transformation function is called, and that transformations apply to\nobjects in the opposite order of the order in which they appear in the code.", "Of course, OpenGL can draw in 2D as well as in 3D.  For 2D drawing in OpenGL,\nyou can draw on the ", "-plane, using zero for the ", " coordinate.\nWhen drawing in 2D, you will probably want to apply 2D versions of rotation, scaling,\nand translation.  OpenGL does not have 2D transform functions, but you can just use\nthe 3D versions with appropriate parameters:", "Modeling transformations are often used in ", ",\nwhich allows complex objects to be built up out of simpler objects.  See\n", ".  To review briefly:  In hierarchical modeling, an object can\nbe defined in its own natural coordinate system, usually using (0,0,0) as a reference\npoint.  The object can then be scaled, rotated, and translated to place it into\nworld coordinates or into a more complex object.  To implement this, we need\na way of limiting the effect of a modeling transformation to one object or to\npart of an object.  That can be done using a stack of transforms.  Before\ndrawing an object, push a copy of the current transform onto the stack.  After drawing\nthe object and its sub-objects, using any necessary temporary transformations,\nrestore the previous transform by popping it from the stack.", "OpenGL 1.1 maintains a stack of transforms and provides functions for\nmanipulating that stack.  (In fact it has several transform stacks, for different\npurposes, which introduces some complications that we will postpone to the \n", ".)  Since transforms are\nrepresented as ", ", the stack is actually a stack of matrices.\nIn OpenGL, the functions for operating on the stack are named ", "() and\n", "().", "These functions do not take parameters or return a value.  OpenGL keeps track of\na current matrix, which is the composition of all transforms that have been applied.\nCalling a function such as ", " simply modifies the current matrix.  When\nan object is drawn, using the ", " functions, the coordinates that are specified\nfor the object are transformed by the current matrix.  There is another function\nthat affects the current matrix:  ", "().  Calling\n", " sets the current matrix to be the ", ",\nwhich represents no change of coordinates at all and is the usual starting point for\na series of transformations.", "When the function ", "() is called, a copy of the current matrix is\npushed onto the stack.  Note that this does not change the current matrix; it just\nsaves a copy on the stack.  When ", "() is called, the matrix on the\ntop of the stack is popped from the stack, and that matrix replaces the current matrix.\nNote that ", " and ", " must always occur in\ncorresponding pairs; ", " saves a copy of the current matrix, and a corresponding\ncall to ", " restores that copy.  Between a call to ", " and\nthe corresponding call to ", ", there can be additional calls of these\nfunctions, as long as they are properly paired.  Usually, you will call ", "\nbefore drawing an object and ", " after finishing that object.  In between,\ndrawing sub-objects might require additional pairs of calls to those functions.", "As an example, suppose that we want to draw a cube.  It's not hard to draw each\nface using glBegin/glEnd, but let's do it with transformations.  We can start with\na function that draws a square in the position of the front face of the cube.\nFor a cube of size 1, the front face would sit one-half unit in front of the screen,\nin the plane ", "\u00a0=\u00a00.5, and it would have vertices at\n(-0.5,\u00a0-0.5,\u00a00.5), (0.5,\u00a0-0.5,\u00a00.5), (0.5,\u00a00.5,\u00a00.5), and (-0.5,\u00a00.5,\u00a00.5).\nHere is a function that draws the square.  The parameters are floating \npoint numbers in the range 0.0 to 1.0\nthat give the RGB color of the square:", "To make a red front face for the cube, we just need to call ", "(1,0,0).\nNow, consider the right face, which is perpendicular to the ", "-axis, in the plane\n", "\u00a0=\u00a00.5.  To make a right face, we can start with a front face and rotate\nit 90 degrees about the ", "-axis.  Think about rotating the front face (red) to the position\nof the right face (green) in this illustration by rotating the front face about the ", ":", "\n", "So, we can draw a green right face for the cube with", "The calls to ", " and ", " ensure that the rotation that\nis applied to the square will not carry over to objects that are drawn later.\nThe other four faces can be made in a similar way, by rotating the front face about the\ncoordinate axes.  You should try to visualize the rotation that you need in each case.\nWe can combine it all into a function that draws a cube.  To make it more interesting,\nthe size of the cube is a parameter:", "The sample program ", " uses this function to\ndraw a cube, and lets you rotate the cube by pressing the arrow keys.\nA Java version is ", ", and a web version is\n", ".  Here is an image of the cube, rotated\nby 15 degrees about the ", "-axis and -15 degrees about the ", "-axis to make\nthe top and right sides visible:", "\n", "For a more complex example of hierarchical modeling with ", " and\n", ", you can check out an OpenGL equivalent of the \"cart and windmills\"\nanimation that was used as an example in ", ".  \nThe three versions of the example are:\n", ",\n", ", and\n", ".\nThis program is an example of hierarchical 2D graphics in OpenGL."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.2}, {"section_title": "Lights, Camera, Action", "chapter_id": "Chapter 4", "section_id": "Section 4.4", "content": ["A scene in computer graphics can be a complex collection of objects, each with\nits own ", ".  In ", ",\nwe saw how a ", " can be used to organize all the objects in a 2D\nscene.  ", " a scene means traversing the\nscene graph, rendering each object in the graph as it is encountered.\nFor 3D graphics, scene graphs must deal with a larger variety of objects,\nattributes, and transforms.  For example, it is often useful to consider lights\nand cameras to be objects and to be able to include them in scene graphs.  In this\nsection, we consider scene graphs in 3D, and how to treat cameras and lights\nas objects.", "When designing scene graphs, there are many options to consider.  For example,\nshould transforms be properties of object nodes, or should there be separate nodes\nto represent transforms?  The same question can be asked about attributes.\nAnother question is whether an attribute value should apply only to the node\nof which it is a property, or should it be inherited by the children of that node?", "A fundamental choice is the shape of the graph.  In general, a scene graph can\nbe a ", ", or \"dag,\" which is a tree-like structure except\nthat a node can have several parents in the graph.  The scene graphs in\n", " were dags.  This has the advantage that a\nsingle node in the graph can represent several objects in the scene, since in a\ndag, a node can be encountered several times as the graph is traversed.  On the other\nhand, representing several objects with one scene graph node can lead to a lack of flexibility,\nsince those objects will all have the same value for any property encoded in\nthe node.  So, in some applications, scene graphs are required to be trees.  \nIn a tree, each node has a unique parent, and the node will be encountered only\nonce as the tree in traversed.  The distinction between trees and dags will show\nup when we discuss camera nodes in scene graphs.", "We have seen how the functions ", " and ", " are used\nto manipulate the transform stack.  These functions are useful when traversing a\nscene graph: When a node that contains a transform is encountered during a traversal\nof the graph, ", " can be called before applying the transform.  Then, after the\nnode and its descendants have been rendered, ", " is called to restore the\nprevious modelview transformation.", "Something similar can be done for attributes such as color and material, if it is assumed \nthat an attribute value in a scene graph node should be inherited as the default value of\nthat attribute for children of the node.  OpenGL 1.1 maintains an attribute stack, which is\nmanipulated using the functions ", " and ", ".  In addition\nto object attributes like the current color, the attribute stack can store global\nattributes like the global ambient color and the enabled state of the depth test.\nSince there are so many possible attributes, ", " does not simply\nsave the value of every attribute.  Instead, it saves a subset of the\npossible attributes.  The subset that is to be saved is specified as a parameter to\nthe function.  For example, the command", "will save a copy of each of the OpenGL state variables that can be enabled or\ndisabled.  This includes the current state of ", ", ", ",\n", ", and others.  Similarly,", "saves a copy of the current color, normal vector, and texture coordinates.  And", "saves attributes relevant to lighting such as the values of material properties and light properties,\nthe global ambient color, color material settings, and the enabled state for lighting and each of\nthe individual lights.  Other constants can be used to save other sets of attributes; see the\nOpenGL documentation for details.  It is possible to OR together several constants to combine\nsets of attributes.  For example,", "will save the attributes in both the ", " set and in the\n", " set.", "Calling ", "() will restore all the values that were saved by the\ncorresponding call to ", ".  There is no need for a parameter to\n", ", since the set of attributes that are restored is determined\nby the parameter that was passed to ", ".", "It should be easy to see how ", " and ", " can be used\nwhile traversing a scene graph:  When processing a node, before changing attribute values,\ncall ", " to save a copy of the relevant set or sets of attributes.\nRender the node and its descendants. Then call ", " to restore the\nsaved values.  This limits the effect of the changes so that they apply only to the node and\nits descendants.", "There is an alternative way to save and restore values.  OpenGL has a variety of \"get\" functions\nfor reading the values of various state variables.  I will discuss just some of them here.\nFor example,", "retrieves the current color value, as set by ", ".  The ", " parameter\nshould be an array of ", ", whose length is at least four.  The RGBA color components\nof the current color will be stored in the array.  Note that, later, you can simply call\n", "(", ") to restore the color.  The same function can be used\nwith different first parameters to read the values of different floating-point state variables.\nTo find the current value of the ", ", use", "This will set ", "[0] and ", "[1] to be the ", " and ", " coordinates\nof the lower left corner of the current viewport, ", "[2] to be its width, and ", "[3]\nto be its height. To read the current values of material properties, use", "The ", " must be ", " or ", ".  The property must be\n", ", ", ", ", ", ", ", or ", ".\nThe current value of the property will be stored in ", ", which must be of length at least\nfour for the color properties, or length at least one for ", ".  There is\na similar command, ", ", for reading properties of lights.", "Finally, I will mention ", "(", "), which can be used to check the\nenabled/disabled status of state variables such as ", " and ", ".\nThe parameter should be the constant that identifies the state variable.  The function returns 0 if the state\nvariable is disabled and 1 if it is enabled.  For example, ", "(", ")\ntests whether lighting is enabled.  Suppose that a node in a scene graph has an attribute\n", " to tell whether that node (and its descendants) should be rendered with lighting\nenabled.  Then the code for rendering a node might include something like this:", "Since ", " can be used to push large\ngroups of attribute values, you might think that it would\nbe more efficient to use ", " and the ", " family of commands\nto read the values of just those state variables that you are \nplanning to modify.  However, recall that OpenGL can queue a number\nof commands into a batch to be sent to the graphics card, and those commands\ncan be executed by the ", " at the same time that your program\ncontinues to run.  A ", " command can require your \nprogram to communicate with the graphics card and wait for the response.\nThis means that any pending OpenGL commands will have to be sent to the\ngraphics card and executed before the ", " command can complete.\nThis is the kind of thing that can hurt performance.  \nIn contrast, calls to ", " and ", " can\nbe queued with other OpenGL commands and sent to the graphics\ncard in batches, where they can be executed efficiently by\nthe graphics hardware.  In fact, you should generally prefer\nusing ", "/", " instead of a\n", " command when possible.", "Let's turn to another aspect of modeling.  Suppose that we want to implement a\nviewer that can be moved around in the world like other objects.  Sometimes, such\na viewer is thought of as a moving camera.  The camera is used to take pictures of\nthe scene.  We want to be able to apply transformations\nto a camera just as we apply transformations to other objects.  The position\nand orientation of the camera determine what should be visible when the scene is \nrendered.  And the \"size\" of the camera, which can be affected by a scaling transformation,\ndetermines how large a field of view it has.   But a camera is not\njust another object.  A camera really represents the viewing transformation that\nwe want to use.  Recall that modeling and viewing transformations have opposite effects:\nMoving objects to the right with a modeling transform is equivalent to moving the\nviewer to the left with a viewing transformation.  (See ", ".)\nTo apply a modeling transformation to the camera, we\nreally want to apply a viewing transformation to the scene as a whole, and that viewing transformation\nis the ", " of the camera's modeling transformation.", "The following illustration shows a scene viewed from a moving camera.  The camera starts\nin the default viewing position, at the origin, looking in the direction of the negative ", "-axis.\nThis corresponds to using the identity as the viewing transform.  For the second image,\nthe camera has moved forward by ten units.  This would correspond to applying the modeling\ntransformation ", "(0,0,\u221210) to the camera (since it is moving in the negative\n", "-direction).  But to implement this movement as a change of view, \nwe want to apply the inverse operation as a viewing transformation.  So, the viewing\ntransform that we actually apply is ", "(0,0,10).  This can be seen, \nif you like, as a modeling transformation\nthat is applied to all the ", " objects in the scene:  Moving the camera ten units in\none direction is equivalent to moving all the other objects 10 units in the opposite direction.", "\n", "For the third image, the camera has rotated in place by 21 degrees to the right\u2014a 21-degree\nclockwise rotation about the ", "-axis\u2014", " it has been translated.  This can be \nimplemented by the transformation ", "(21,0,1,0)\u2014a 21-degree counterclockwise\nrotation about the ", "-axis\u2014applied ", " the translation. Remember that the\ninverse of a composition of transformations is the composition of their inverses, in the opposite\norder.  Mathematically, using ", " to represent the inverse of a\ntransformation ", ", we have that \n", " for\ntwo transformations ", " and ", ".", "The images in the illustration are from the following demo.  The demo lets you move around in a scene.  More accurately, of course, it \nlets you change the viewing transformation to see the scene from different viewpoints.", "\n", "\n", "When using scene graphs, it can be useful to include a camera object in the graph.  That is,\nwe want to be able to include a node in the graph that represents the camera, and we want to\nbe able to use the camera to view the scene.  It can even be useful to have several cameras\nin the scene, providing alternative points of view.  To implement this, we need to be able\nto render a scene from the point of view of a given camera.  From the previous discussion,\nwe know that in order to do that, we need to use a viewing transformation that is the\ninverse of the modeling transformation that is applied to the camera object.\nThe viewing transform must be applied before any of the objects in the scene are rendered.", "When a scene graph is traversed, a modeling transformation can be applied at any node.\nThe modeling transform that is in effect when a given node is encountered is the composition\nof all the transforms that were applied at nodes along the path that led to given node.\nHowever, if the node is a camera node, we don't want to apply that modeling transform;\nwe want to apply its inverse as a viewing transform.  To get the inverse, we can\nstart at the camera node and follow the path backwards, applying the inverse of\nthe modeling transform at each node.", "\n", "To easily implement this, we can add \"parent pointers\" to the scene graph data structure.  \nA parent pointer for a node is a link to the parent of that node in the graph. Note that this only works\nif the graph is a tree; in a tree, each node has a unique parent, but that is not true in a general\ndirected acyclic graph.  It is possible to move up the tree by following parent pointers.", "We this in mind, the algorithm for rendering the scene from the point of view of a camera\ngoes as follows: Set the modelview transform to be the identity, by calling ", "().\nStart at the camera node, and follow parent pointers until you reach the root of the tree.\nAt each node, apply the ", " of any modeling transformation in that node.\n(For example, if the modeling transform is translation by (a,b,c), call\n", "(", ").)  Upon reaching the root, the viewing\ntransform corresponding to the camera has been established.  Now, traverse the scene graph\nto render the scene as usual.  During this traversal, camera nodes should be ignored.", "Note that a camera can be attached to an object, in the sense that the camera and the object\nare both subject to the same modeling transformation and so move together as a unit.\nIn modeling terms, the camera and the object\nare sub-objects in a complex object.  For example, a camera might be attached\nto a car to show the view through the windshield of that car.  If the car moves, because its\nmodeling transformation changes, the camera will move along with it.  ", "It can also be useful to think of lights as objects, even as part of a complex object.\nSuppose that a scene includes a model\nof a lamp.  The lamp model would include some geometry to make it visible, but if it\nis going to cast light on other objects in the scene, it also has\nto include a source of light.  This means that the lamp is a complex\nobject made up of an OpenGL light source plus some geometric objects.\nAny modeling transformation that is applied to the lamp should\naffect the light source as well as the geometry.  In terms of the\nscene graph, the light is represented by a node in the graph,\nand it is affected by modeling transformations in the same\nway as other objects in the scene graph.  You can even have\nanimated lights\u2014or animated objects that include lights\nas sub-objects, such as the headlights on a car.", "Recall from ", " that a light source is subject to the\nmodelview transform that is in effect at the time the position of the\nlight source is set by ", ".  If the light is represented as a node in\na scene graph, then the modelview transform that we need is the one that\nis in effect when that node is encountered during a traversal of the scene\ngraph.  So, it seems like we should just traverse the graph and set the position\nof the light when we encounter it during the traversal.", "But there is a problem:  Before any geometry is rendered,\nall the light sources that might affect that geometry must already be\nconfigured and enabled.  In particular, the lights' positions must be set\nbefore rendering any geometry.  This means that you can't simply set the\nposition of light sources in the scene graph as you traverse the graph in the\nusual way.  If you do that, objects that are drawn before the\nlight is encountered won't be properly illuminated by the\nlight.  Similarly,\nif the light node contains values for any other properties of\nthe light, including the enabled/disabled state of the light,\nthose properties must be set before rendering any geometry.", "One solution is to do two traversals of the scene graph, the first\nto set up the lights and the second to draw the geometry.  Since\nlights are affected by the modelview transformation, you have to\nset up the modeling transform during the first traversal\nin exactly the same way that you do in the second traversal.\nWhen you encounter the lights during the first traversal,\nyou need to set the position of the light, since setting the\nposition is what triggers the application of the current modelview\ntransformation to the light.  You also need to set any other\nproperties of the light.  During the first traversal, geometric\nobjects in the scene graph are ignored.  During the second traversal, when\ngeometry is being rendered, light nodes can be ignored."], "chapter_title": "OpenGL 1.1: Light and Material", "id": 4.4}, {"section_title": "Light and Material in OpenGL 1.1", "chapter_id": "Chapter 4", "section_id": "Section 4.2", "content": ["In this section, we will see how to use light and material in OpenGL.\nThe use of light and material must be enabled by calling\n", "(", ").  When lighting is disabled,\nthe color of a vertex is simply the current color as set by\n", ".  When lighting is enabled, the color of a vertex\nis computed using a mathematical formula that takes into account\nthe ", " of the scene and the ", "\nproperties that have been assigned to the vertex, as discussed\nin the ", ".\nNow it's time to learn about the OpenGL commands that are used\nto configure lighting and to assign materials to objects.", "It is common for lighting to be turned on for ", " \nsome parts of a scene, but turned off for other parts.  We will say that \nsome objects are \"lit\" while others aren't.  For example, wireframe\nobjects are usually drawn with lighting disabled, even if they\nare part of a scene in which solid objects are lit.  But note that it is illegal to call\n", " or ", " between calls to ", "\nand ", ", so it is not possible for part of a primitive\nto be lit while another part ", " is unlit.\n(I should note that\nwhen lighting is enabled, it is applied to point and line primitives \nas well as to polygons, even though it rarely makes sense to do so.)  Lighting\ncan be enabled and disabled by calling ", "\nand ", " with parameter ", ".  Light and\nmaterial settings don't have to be changed when lighting is turned off, \nsince they are simply ignored when lighting is disabled. ", "To light a scene, in addition to enabling ", ",\nyou must configure at least one source of light.  For very basic\nlighting, it often suffices to call", "This command turns on a ", " that shines\nfrom the direction of the viewer into the scene.  (Note that the last\ncharacter in ", " is a zero.)  Since it shines from the\ndirection of the viewer, it will illuminate everything that the user\ncan see.  The light is white, with no specular component; that is,\nyou will see the ", " of objects, without\nany ", ".\nWe will see later in this section how to change the characteristics\nof this light source and how to configure additional sources.  But first,\nwe will consider materials and normal vectors.", "Material properties are vertex ", " in\nthat same way that color is a vertex attribute.  That is, the OpenGL state\nincludes a current value for each of the material properties.  When a vertex\nis generated by a call to one of the ", " functions, a copy of each\nof the current material properties is stored, along with the vertex coordinates.\nWhen a primitive that contains the vertex is rendered, the material properties \nthat are associated with the vertex are used, along with information about lighting, \nto compute a color for the vertex.", "This is complicated by the fact that polygons are two-sided, and the ", "\nand ", " of a polygon can have different materials.  This means that,\nin fact, two sets of material property values are stored for each vertex: the front\nmaterial and the back material.  (The back material isn't actually used unless you turn\non two-sided lighting, which will be discussed below.)", "With all that in mind, we will look at functions for setting the current values of \nmaterial properties.  For setting the ", ",\n", ", ", ",\nand ", " material colors, the function is", "The first parameter can be ", ", ", ", or\n", ".  It tells whether you are setting a material property value for the\nfront face, the back face, or both.  The second parameter tells which material property\nis being set.  It can be ", ", ", ", ", ",\n", ", or ", ".  Note that it is possible to\nset the ambient and diffuse colors to the same value with one call to ", "\nby using ", " as the property name.\nThe last parameter to ", " is an array containing four ", " numbers.  The \nnumbers give the ", " color components as values in the range from 0.0 to\n1.0; values outside this range are actually allowed, and will be used in lighting computations,\nbut such values are unusual.  Note that an alpha component is required, but it is\nused only in the case of diffuse color:  When the vertex color is computed, its alpha component\nis set equal to the alpha component of the diffuse material color.", "The ", " material property is a single  number rather than an\narray, and there is a different function for setting its value (without the \"v\" at the\nend of the name):", "Again, the ", " can be ", ", ", ", or\n", ".  The ", " ", " be ", ".  And the\nvalue is a ", " in the range 0.0 to 128.0.", "Compared to the large number of versions of ", " and ", ",\nthe options for setting material are limited.  In particular, it is not possible to\nset a material color without defining an array to contain the color component values.\nSuppose for example that we want to set the ambient and diffuse colors to a bluish green.\nIn C, that might be done with", "With my JavaScript simulator for OpenGL, this would look like", "And in the ", " API for Java, where where methods with array\nparameters have an additional parameter to give the starting index of the data\nin the array, it becomes", "In C, the third parameter is actually a pointer to ", ", which allows\nthe flexabilty of storing the values for several material properties in one array.\nSuppose, for example, that we have a C array", "where the first four numbers in the array specify an ambient color; the next\nfour, a diffuse color; the next four, a specular color; and the last number, a shininess\nexponent.  This array can be used to set all the material properties:", "Note that the last function is ", " rather than ", ",\nand that its third parameter is a number rather than a pointer.\nSomething similar can be done in Java with\t", "The functions ", " and ", " can be called\nat any time, including between calls to ", " and ", ".\nThis means that different vertices of a primitive can have different\nmaterial properties.", "So, maybe you like ", " better than ", "?  If so,\nyou can use it to work with material as well as regular color.  If you\ncall", "then some of the material color properties will track the color.  By default,\nsetting the color will also set the current front and back, ambient and diffuse material \nproperties.  That is, for example, calling", "will, if lighting is enabled, have the same effect as calling", "where ", " contains the values 1, 0, 0, 1.\nYou can change the material property that tracks the color using", "where ", " can be ", ", ", ", or\n", ", and ", " can be ", ", ", ", \n", ", ", ", or ", ".\nNeither ", " nor ", " can be called between\ncalls to ", " and ", ", so all of the vertices of a primitive\nmust use the same setting.", "Recall that when ", " or ", " is used to\ndraw a primitive, the color values for the vertices of the primitive can be taken from a color\narray, as specified using ", ".  (See ", ".)\nThere are no similar arrays for material properties.  However, if a color\narray is used while lighting is enabled, and if ", " is also\nenabled, then the color array will be used as the source for the values of\nthe material properties that are tracking the color.", "\n", " are essential to lighting calculations.\n(See ", ".)\nLike color and material, normal vectors are attributes of vertices.  The OpenGL state includes\na current normal vector, which is set using functions in the family ", ".  When a vertex is specified\nwith ", ", a copy of the current normal vector is saved as an attribute of the\nvertex, and it is used as the normal vector for that vertex when the color of the vertex is\ncomputed by the lighting equation.  Note that the normal vector for a vertex must be specified\n", " ", " is called for that vertex.", "Functions in the family ", " include ", ", ", ",\n", ", and ", ".  As usual, a \"v\" means that the values are\nin an array, \"f\" means that the values are ", ", and \"d\" means that\nthe values are ", ".  (All normal vectors have three components).\nSome examples:", "For a polygon that is supposed to look flat, the same normal vector is used\nfor all of the vertices of the polygon.  For example, to draw one side of a cube,\nsay the \"top\" side, facing in the direction of the positive ", "-axis:", "Remember that the normal vector should point out of the front face of the polygon,\nand that the front face is determined by the order in which the vertices are generated.\n(You might think that the front face should be determined by the direction in which\nthe normal vector points, but that is ", " how its done.  If a normal vector for a vertex\npoints in the wrong direction, then lighting calculations will not give the correct color\nfor that vertex.)", "When modeling a smooth surface, normal vectors should be chosen perpendicular to\nthe surface, rather than to the polygons that approximate the surface.  (See ", ".)\nSuppose that we want to draw the side of a cylinder with radius 1 and\nheight 2, where the center of the cylinder is at (0,0,0) and the axis lies\nalong the ", "-axis.  We can approximate the surface using a single \ntriangle strip.  The top and bottom edges of the side of a cylinder are circles.\nVertices along the top edge will have coordinates\n(", "(", "),", "(", "),1) and vertices along the bottom edge\nwill have coordinates (", "(", "),", "(", "),\u22121), where ", "\nis some angle. The normal vector\npoints in the same direction as the radius, but its ", "-coordinate is zero\nsince it points directly out from the side of the cylinder.  \nSo, the normal vector to the side of the cylinder at both of these\npoints will be (", "(", "),", "(", "),0).  Looking down the\n", "-axis at the top of the cylinder, it looks like this:", "\n", "When we draw the side of the cylinder as a triangle strip, we have to generate\npairs of vertices on alternating edges.  The normal vector is the same for the two\nvertices in the pair, but it is different for different pairs.  Here is the code:", "When we draw the top and bottom of the cylinder, on the other hand, we want a flat\npolygon, with the normal vector pointing in the direction (0,0,1) for the top and\nin the direction (0,0,\u22121) for the bottom:", "Note that the vertices for the bottom are generated in the opposite order from\nthe vertices for the top, to account for the fact that the top and bottom face in\nopposite directions.  As always, vertices need to be enumerated in counterclockwise order,\nas seen from the front.", "When drawing a primitive with ", " or ", ",\nit is possible to provide a different normal for each vertex by using a\nnormal array to hold the normal vectors.  The normal array works in the same\nway as the color array and the vertex array.  To use one, you need to enable\nthe use of a normal array by calling", "The coordinates for the normal vectors must be stored in an array\n(or in an ", " for ", "), and the location\nof the data must be specified by calling", "The ", " specifies the type of values in the array. It can be\n", ", ", ", or ", ".  The ", " is an \ninteger, which is usually\u00a00, meaning that there is no extra data in the array\nbetween the normal vectors.  And ", " is the array (or buffer) that\nholds the normal vectors, with three numbers for each normal.", "With this setup, when ", " or ", " is used to\ndraw a primitive, the normal vectors for the primitive will be pulled from the array.\nNote that if ", " is not enabled, then all of the normal vectors\nfor the primitive will be the same, and will be equal to the current normal vector\nas set by ", ".", "The lighting equation assumes that normal vectors are ", ", \nthat is, that they have length equal to one.  The default in OpenGL is to use normal vectors as\nprovided, even if they don't have length one, which will give incorrect results.  However,\nif you call", "then OpenGL will automatically convert every normal vector into a unit normal that \npoints in the same direction.", "Note that when a ", " is\napplied, normal vectors are transformed along with vertices; this is necessary because\na transformation can change the direction in which a surface is facing.  A scaling transformation\ncan change the length of a normal vector, so even if you provided unit normal vectors, they\nwill not be unit normals after a scaling transformation.  However, if you have enabled\n", ", the transformed normals will automatically be converted back to unit\nnormals.  My recommendation is to ", " enable ", " as part of\nyour OpenGL initialization.  The only exception would be if all of the normal vectors\nthat you provide are of length one and you do not apply any scaling transformations.", "OpenGL 1.1 supports at least eight light sources, which are identified by the\nconstants ", ", ", ", ..., ", ".  (An OpenGL\nimplementation might allow additional lights.)  Each light source can be configured\nto be either a ", " or a ", ",\nand each light can have its own diffuse, specular, and ambient intensities.\n(See ", ".)", "By default, all of the light sources are disabled.  To enable a light, call\n", "(", "), where ", " is one of the constants ", ",\n", ",\u00a0....  However, just enabling a light does not give any illumination,\nexcept in the case of ", ", since all light intensities are zero by \ndefault, with the single exception of the diffuse color of light number\u00a00.\nTo get any light from the other light sources, you need to change some of their\nproperties.  Light properties can be set using the functions", "The first parameter is one of the constants ", ", ", ", ..., ", ".\nIt specifies which light is being configured.  The second parameter says which property of the\nlight is being set.  It can be ", ", ", ", ", ",\nor ", ".  The last parameter is an array that contains at least four ", "\nnumbers, giving the value of the property.", "For the color properties, the four numbers in the array specify the red,\ngreen, blue, and alpha components of the color.  (The alpha component is not actually\nused for anything.)  The values generally lie in the range 0.0 to 1.0, but can lie\noutside that range; in fact, values larger than 1.0 are occasionally useful.\nRemember that the diffuse and specular colors of a light tell how the light interacts\nwith the diffuse and specular material colors, and the ambient color is simply added\nto the global ambient light when the light is enabled.  For example, to set up light\nzero as a bluish light, with blue specular highlights, that adds a bit of blue to the\nambient light when it is turned on, you might use:", "It would likely take some experimentation to figure out exactly what values\nto use in the arrays to get the effect that you want.", "The ", " property of a light is quite a bit different.  It is used both\nto set whether the light is a point light or a directional light, and to set its position or direction.\nThe property value for ", " is an array of four numbers (", "),\nof which at least one must be non-zero.  When the fourth number, ", ", \nis zero, then the light is directional and the point (", ")\nspecifies  the direction of the light:  The light rays shine in the direction\nof the line ", " the point (", ") ", " the origin.\nThis is related to ", ": The source of the light\ncan be considered to be a point at infinity in the direction of \n(", ").", "On the other hand, if the fourth\nnumber, ", ", is non-zero, then the light is a point light, and it is located\nat the point (", ").  Usually, ", " is\u00a01. The\nvalue (", "1) gives a point light at (", ").\nAgain, this is really homogeneous coordinates.", "The default position for all lights is (0,0,1,0), representing\na directional light shining from the positive direction of the\n", "-axis, towards the negative direction of the ", "-axis.", "One important and potentially confusing fact about lights is that\nthe position that is specified for a light is transformed by the\n", " that is in effect ", " using ", ".  Another way of saying this is that\nthe position is set in ", ", not in \n", ".  Calling ", " with the\nproperty set to ", " is very much like calling ", ".\nThe light position is transformed in the same way that the vertex coordinates\nwould be transformed.  For example,", "puts the light in the same place as", "For a directional light, the direction of the light is transformed\nby the rotational part of the modelview transformation.", "There are three basic ways to use light position.  It is easiest to think\nin terms of potentially animated scenes.", "\n", ", if the position is set before any modelview transformation is applied, then\nthe light is fixed with respect to the viewer.  For example, the default\nlight position is effectively set to (0,0,1,0) while the modelview transform\nis the identity.  This means that it shines in the direction of the negative\n", "-axis, ", ", where the\nnegative ", "-axis points into the screen.  Another way of saying \nthis is that the light always shines from the direction of the viewer \ninto the scene.  It's like the light is attached to the viewer.  If the\nviewer moves about in the world, the light moves with the viewer.", "\n", ", if the position is set after the viewing transform\nhas been applied and before any modeling transform is applied, then\nthe position of the light is fixed in world coordinates.  It will not\nmove with the viewer, and it will not move with objects in the scene.\nIt's like the light is attached to the world.", "\n", ", if the position is set after a modeling transform has\nbeen applied, then the light is subject to that modeling transformation.\nThis can be used to make a light that moves around in the scene as the \nmodeling transformation changes.  If the light is subject to the same\nmodeling transformation as an object, then the light will move around\nwith that object, as if it is attached to the object.", "The sample program ", " or\n", " uses multiple moving, colored\nlights and lets you turn them on and off to see the effect. The demo below is a JavaScript version\nof the same program. The program lets you see how light from\nvarious sources combines to produce the visible color of an object.\nThe source code provides examples of configuring lights and using material\nproperties.", "\n", "\n", "In addition to the properties of individual light sources, the OpenGL\nlighting system uses several global properties. There are only three such \nproperties in OpenGL 1.1.  One of them is the\nglobal ambient light, which is ambient light that doesn't come from\nthe ambient color property of any light source.  Global ambient light\nwill be present in the environment even if all of ", ", \n", ", ... are disabled.  By default, the global\nambient light is black (that is, its RGB components are all zero).  \nThe value can be changed using the function", "where the ", " must be ", " and\nthe ", " is an array containing four numbers giving the RGBA\ncolor components of the global ambient light as numbers in the range\n0.0 to 1.0.  In general, the global ambient light level should be quite low.\nFor example, in\u00a0C:", "The alpha component of the color is usually set to 1, but it is not\nused for anything.  For JOGL, as usual, there is an extra parameter to\nspecify the starting index of the data in the array, and the example\nbecomes:", "The other two light model properties are options that can be\neither off or on.  The properties are ", "\nand ", ". They can be set using the function", "with a ", " equal to 0 or 1 to indicate whether the option\nshould be off or on.  You can use the symbolic constants\n", " and ", " for the value, but these\nare just names for 0 and 1.", "\n", " is used to turn on ", ".\nRecall that a polygon can have two sets of material properties, a\nfront material and a back material.  When two-sided lighting is off,\nwhich is the default, only the front material is used; it is used \nfor both the front face and the back face of the polygon.  Furthermore, the same normal vector is\nused for both faces.  Since those vectors point\u2014or at least are supposed\nto point\u2014out of the front face, they don't give the correct result for the back face.\nIn effect, the back face looks like it\nis illuminated by light sources that lie in front of the polygon, but the back face\nshould be illuminated by the lights that lie behind the polygon.", "On the other hand, when two-sided lighting is on, the back material is used on the back\nface and  the direction of the normal vector is reversed when it is used\nin lighting calculations for the back face.", "You should use two-sided lighting whenever there are back faces that might\nbe visible in your scene.  (This will not be the case when your scene consists\nof \"solid\" objects, where the back faces are hidden inside the solid.) With\ntwo-sided lighting, you have the option of using the same material on both faces\nor specifying different materials for the two faces.  For example, to put\na shiny purple material on front faces and a duller yellow material on back faces:", "This little demo shows\nwhat these materials look like on a cylinder that has no top, so that you can see\nthe back faces on the inside surface:", "\n", "\n", "The third material property, ", ",  is much less \nimportant.  It has to do with the direction from a surface to the viewer in the\nlighting equation.  By default, this direction is always taken to point directly\nout of the screen, which is true for an ", " but\nis not for not accurate for a ", ".  If you turn on the\nlocal viewer option, the true direction to the viewer is used.\nIn practice, the difference is usually not very noticeable."], "chapter_title": "OpenGL 1.1: Light and Material", "id": 4.2}, {"section_title": "Image Textures", "chapter_id": "Chapter 6", "section_id": "Section 6.4", "content": ["\n", " play an essential role in 3D graphics,\nand support for ", " is built into\nmodern ", " on the hardware level.  In this section,\nwe look at the WebGL API for image textures. Image textures\nin OpenGL\u00a01.1 were covered in ", ".  Much of\nthat section is still relevant in modern OpenGL, including WebGL.  So,\nas we cover image textures in WebGL, much of the material will not be\nnew to you.  However, there is one feature that is new since OpenGL\u00a01.1:\n", ".", "A texture unit, also called a texture mapping unit (", ") or\na texture processing unit (TPU),\nis a hardware component in a GPU  that does sampling.  ", "\nis the process of computing a color from an image texture and ", ".\nMapping a texture image to a surface is a fairly complex operation, since it requires more\nthan just returning the color of the ", " that contains some given texture\ncoordinates.  It also requires applying the appropriate ", "\nor ", ", possibly using ", " if available.\nFast texture sampling is one of the key requirements for good GPU performance.", "Texture units are not to be confused with ", ".\nWe encountered texture objects in ", ".  A texture object\nis a data structure that contains the color data for an image texture, and possibly for\na set of mipmaps for the texture, as well as the values of texture properties such\nas the minification and magnification filters and the ", ".\nA texture unit must access a texture object to do its work.  The texture unit is the\nprocessor; the texture object holds the data that is processed.", "(By the way, I should really be more careful about throwing around the terms \"GPU\" and\n\"hardware.\"  Although a texture unit probably does use an actual hardware component in the\nGPU, it could also be emulated, more slowly, in software.  And even if there is\nhardware involved, having eight texture units does not necessarily mean that there are\neight hardware components; the texture units might share time on a smaller number of\nhardware components.  Similarly, I said previously that texture objects are stored\nin memory in the GPU, which might or might not be literally true in a given case.\nNevertheless, you will probably find it conceptually easier to think of a texture unit \nas a piece of hardware and a texture object as a data structure in the GPU.)", "In ", ", texture lookup is done using ", ".\nA sampler variable is a variable in a shader program of type ", " or ", ".\nA ", " is used to do lookup in a standard texture image; a ", " is used\nto do lookup in a ", " (", ").\nThe value of a sampler variable is a reference to a texture unit.  The value tells which texture\nunit is invoked when the sampler variable is used to do texture lookup.\nSampler variables must be declared as global uniform variables.  \nIt is not legal for a shader program to assign a value to a sampler variable.  The\nvalue must come from the JavaScript side.", "On the JavaScript side, the available texture units are numbered 0, 1, 2,\u00a0..., where\nthe maximum value is implementation dependent.  The number of units can be determined as the value of\nthe expression", "(Please remember, again, that ", " here is the name of a JavaScript variable that refers to the\nWebGL context, and that the name is up to the programmer.)", "As far as JavaScript is concerned, the value of a sampler variable is an integer.  If you\nwant a sampler variable to use texture unit number 2, then you set the value of the\nsampler variable to\u00a02.  This can be done using the function ", "\n(", ").  For example, suppose a shader program declares a sampler\nvariable\n", "To set its value from JavaScript, you need the location of the variable in the\nshader program.  If ", " is the shader program, the location is obtained by\ncalling", "Then, you can tell the sampler variable to use texture unit number 2 by calling", "Note that the integer value is not accessible in GLSL.  The integer tells the\nsampler which texture unit to use, but there is no way for the shader program to\nfind out the number of the unit that is being used.", "To use an image texture, you also need to create a texture object, and you need to load\nan image into the texture object.  You might want to set some properties of the texture\nobject, and you might want to create a set of mipmaps for the texture.  And\nyou will have to associate the texture object with a texture unit.  All this is\ndone on the JavaScript side.", "The command for creating a texture object is ", "().  The command\nin OpenGL\u00a01.1 was ", ".  The WebGL command is easier to use.\nIt creates a single texture object and returns a reference to it.  For example,", "This just allocates some memory for the object.  In order to use it, you must\nfirst \"bind\" the texture object by calling ", ".  For example,", "The first parameter, ", ", is the texture target.  This target is\nused for working with an ordinary texture image.  There is a different target for\ncubemap textures.", "The function ", " is used to load an image into the currently bound\ntexture object.  We will come back to that in the next subsection.  But remember that\nthis command and other commands always apply to the currently bound texture object.\nThe texture object is not mentioned in the commands; the texture object must be bound\nbefore the command is called.", "You also need to tell a texture unit to use the texture object.  Before you can\ndo that, you need to make the texture unit \"active,\" which is done by calling the function\n", ".  The parameter is one of the constants\n", ", ", ", ", ",\u00a0..., which represent\nthe available texture units.  (The values of these constants are ", " 0, 1, 2,\u00a0....)\nInitially, texture unit number 0 is active.  To make texture unit number 2 active, for example, use", "(This function should really have been called ", "). If you then call", "to bind a texture object, while texture unit 2 is active, then\nthe the texture object ", " is bound to texture unit number 2.  The binding\njust tells the texture unit which texture object to use.  That is, \ntexture unit 2 will do ordinary texture lookups using the image and settings that are stored in ", ".\nA texture object can be bound to several texture units at the same time.  However, a given\ntexture unit can have only one bound ", " at a time.", "So, working with texture images in WebGL involves working with texture objects, texture\nunits, and sampler variables.  The relationship among the three is illustrated in this\npicture:", "\n", "A sampler variable uses a texture unit, which uses a texture object, which holds a texture image.\nThe JavaScript commands for setting up this chain are shown in the illustration.  To apply a texture\nimage to a primitive, you have to set up the entire chain. Of course, you also have to\nprovide texture coordinates for the primitive, and \nyou need to use the sampler variable in the shader program to access the texture.", "Suppose that you have several images that you would like to use on several different\nprimitives.  Between drawing primitives, you need to change the texture image that will be\nused.  There are at least three different ways to manage the images in WebGL:", "I don't know how options 2 and 3 compare in terms of efficiency.  Note that you\nare only ", " to use more than one texture unit if you want to apply more\nthan one texture image to the same primitive.  To do that, you will need several\nsampler variables in the shader program.  They will have different values so that\nthey refer to different texture units, and the color of a pixel will somehow depend\non samples from both images.  This picture shows two textures being combined\nin simple ways to compute the colors of pixels in a textured square:", "\n", "In the image on the left, a ", " \"brick\" image is multiplied by an \"Earth\" image;\nthat is, the red component of a pixel is computed by multiplying the red component from\nthe brick texture by the red component from the Earth texture, and same for green and blue.\nOn the right, the same Earth texture is subtracted from a \"cloth\" texture.  Furthermore,\nthe pattern is distorted because the texture coordinates were  modified before being\nused to sample the textures, using the formula ", " ", " \n0.25", "(6.28*", ").  That's the kind of thing that\ncould only be done with programmable shaders!  The images are taken from\nthe following demo.\nTry it out!", "\n", "\n", "You might want to view the ", "\nto see how the textures are programmed.  Two texture units are used.  The values of two\nuniform sampler variables, ", " and ", ", are set during\ninitialization with the code", "The values are never changed.  The program uses several texture images.  There is a\ntexture object for each image.  The texture objects are stored in an array,\n", ".  Two popup menus allow the user to select which texture images\nare applied to the primitive.  This is implemented in the drawing routine by binding\nthe two selected texture objects to texture units 0 and\u00a01, which are the units used\nby the two sampler variables.  The code for that is:", "Getting images into the texture objects is another question, which we turn to next.", "An image can be loaded into a texture object using the function ", ".\nFor use with WebGL, this function usually has the form\n", "The target is ", " for ordinary textures; there are other targets for\nloading cubemap textures.  The second parameter is the mipmap level, which is 0 for the\nmain image.  Although it is possible to load individual mipmaps, that is rarely done.\nThe next two parameters give the format of the texture inside the texture object and\nin the original image.  In WebGL, the two format parameters must have the same value.\nSince web images are stored in RGBA format, there is rarely a need to use anything else.\nBut you can use ", " if you don't need the alpha component.  And by\nusing ", " or ", ", you can convert the\nimage to grayscale.  (", " is a weighted average\nof red, green, and blue that approximates the perceived brightness of a color.)\nThe fourth parameter is always going to be ", ", indicating that\nthe colors in the image are stored using one byte for each color component.  Although\nother values are possible, they don't really make sense for web images.", "The last  parameter in the call to ", " is the image.  Ordinarily,\n", " will be a ", " image element that has been loaded asynchronously\nby JavaScript.   The ", " can also be a ", " element.\nThis means that you can draw on a canvas, using the ", " 2D graphics\nAPI, and then use the canvas as the source for a texture image.  You can even do \nthat with an ", " that is not visible on the web page.", "The image is loaded into the texture object that is currently bound to ", " in\nthe currently active texture unit.  There is no default texture object; that is, if no texture \nhas been bound when ", " is called, an error occurs.  The active texture unit\nis the one that has been selected using ", ", or is texture unit 0\nif ", " has never been called.  A texture object is bound to the active texture unit\nby ", ".  This was discussed earlier in this section.", "Using images in WebGL is complicated by the fact that images are loaded\nasynchronously.  That is, the command for loading an image just starts the\nprocess of loading the image.  You can specify a callback function that will\nbe executed when the loading completes.  The image won't actually be available\nfor use until after the callback function is called.  When loading an\nimage to use as a texture, the callback function should load the image into\na texture object.  Often, it will also call a rendering function to draw\nthe scene, with the texture image.", "The sample program ", " is an example\nof using a single texture on a triangle.  Here is a function that is\nused to load the texture image in that program.  The texture object is\ncreated before the function is called.", "Note that image textures for WebGL should be power-of-two textures.  That is,\nthe width and the height of the image should each be a power of 2, such as\n128, 256, or 512.  You can, in fact, use non-power-of-two textures, but you can't\nuse mipmaps with such textures, and the only texture repeat mode that is supported\nby such textures is ", "\n", "There are several parameters associated with a texture object, including the\ntexture repeat modes and the minification and magnification filters.  They can\nbe set using the function ", ".   The setting applies to\nthe currently bound texture object.  Most of the details are\nthe same as in OpenGL 1.1 (", ").  For example, the minification\nfilter can be set to ", " using", "Recall that the default minification filter won't work without mipmaps.\nTo get a working texture, you have to change the minification filter or install\na full set of mipmaps.  Fortunately, WebGL has a function that will generate\nthe mipmaps for you:", "The texture repeat modes determine what happens when texture coordinates \nlie outside the range 0.0 to 1.0.  There is a separate repeat mode for\neach direction in the texture coordinate system.\nIn WebGL, the possible values are ", ", ", ",\nand ", ".  The default is ", ".  The mode\n", " was called ", " in OpenGL 1.1, and ", " \nis new in WebGL.  With ", ", the texture image is repeated to\ncover the entire plane, but every other copy of the image is reflected.  This\ncan eliminate visible seams between the copies.  To set a texture\nto use mirrored repeat in both directions, use", "In WebGL, texture coordinates are usually input to the vertex shader as an attribute\nof type ", ".  They are communicated to the fragment shader in a varying variable.\nOften, the vertex shader will simply copy the value of the attribute into the varying\nvariable.  Another possibility is to apply an affine ", "\nto the coordinates in the vertex shader before passing them on the the fragment shader.\nIn the fragment shader, the texture coordinates are used to sample a\ntexture.  The GLSL function for sampling an ordinary texture is", "where ", " is the uniform variable of type ", " that\nrepresents the texture, and ", " is a ", " containing\nthe texture coordinates. The return value is an RGBA color, represented as a\nvalue of type ", ".  As a very minimal example, here is a fragment shader that\nsimply uses the sampled value from the texture as the color of the pixel.", "This shader is from the sample program ", ".", "Textures are sometimes used on primitives of type ", ".  In that case,\nit's natural to get the texture coordinates for a pixel from the special fragment\nshader variable ", ".  A point is rendered as a square, and the\ncoordinates in ", " range from 0.0 to 1.0 over that square.\nSo, using ", " means that one copy of the texture will be\npasted onto the point.  If the ", " primitive has more than one\nvertex, you will see a copy of the texture at the location of each vertex.\nThis is an easy way to put an image, or multiple copies of an image, into a\nscene.  The technique is sometimes referred to as \"point sprites.\"", "The following demo draws a single textured primitive of type\n", ", so you can see what it looks like.  In the demo, only\na circular cutout from each square point is drawn.", "\n", "\n", "The pixel data for a texture image in WebGL is stored in memory starting with the\nrow of pixels at the bottom of the image and working up from there.  When WebGL creates the\ntexture by reading the data from an image, it assumes that the image uses the same format.  However,\nimages in a web browser are stored in the opposite order, starting with the pixels\nin the top row of the image and working down.  The result of this mismatch is that\ntexture images will appear upside down.  You can account for this by modifying\nyour texture coordinates.  However, you can also tell WebGL to invert the\nimages for you as it \"unpacks\" them.  To do that, call", "Generally, you can do this as part of initialization.  Note however that\nfor ", " primitives, the coordinate system used by ", "\nis already upside down, with the ", "-coordinate increasing from top to bottom.\nSo, if you are loading an image for use on a ", " primitive, you might\nwant to set ", " to its default value,\u00a00.", "We have seen how to create a texture from an image or canvas element using ", ".\nThere are several more ways to make an image texture in WebGL.  First of all, the function\n", "which was covered in ", " also exists in WebGL.\nThis function copies data from the color buffer (where WebGL renders its images)\ninto the currently bound texture object.  The data is taken from the\nrectangular region in the color buffer with the specified ", " and\n", " and with its lower left corner at (", ").  The\n", " is usually ", ".  For WebGL, the ", "\nmust be zero. For example,", "This takes the texture data from a 256-pixel square in the bottom left corner\nof the color buffer.  (In a later chapter, we will see that it is actually possible, and more efficient, for\nWebGL to render an image directly to a texture object, using something called a \"framebuffer.\")", "More interesting, perhaps, is the ability to take the texture data directly from\nan array of numbers.  The numbers will become the color component values for the\npixels in the texture.  The function that is used for this is an alternative version\nof ", ":", "and a typical function call would have the form", "Compared to the original version of ", ", there are three extra \nparameters, ", ", ", ", and ", ". The ", "\nand ", " specify the size of the texture image.  For WebGL, the\n", " must be zero, and the ", " and ", "\nmust be the same.", "The last parameter in this version of ", " must be a typed array of \ntype ", " or ", ", depending \non the ", " of the texture.  My examples will use ", " \nand texture format ", " or ", ".", "For an RGBA texture, four color component values are needed for each pixel.  The values\nwill be given as unsigned bytes, with values ranging from 0 to 255, in a ", ".\nThe length of the array will be 4", " (that is,\nfour times the number of pixels in the image).  The data for the bottom row of pixels comes\nfirst in the array, followed by the row on top of that, and so on, with the pixels in \na given row running from left to right.\nAnd within the data for one pixel, the red component comes first, followed by the\nblue, then the green, then the alpha.", "As an example of making up texture data from scratch, let's make a 16-by-16 texture image, \nwith the image divided into four 8-by-8 squares that are colored red, white, and blue.  The code uses the fact that\nwhen a typed array is created, it is initially filled with zeros.  We just have to\nchange some of those zeros to 255.", "The last line is there because the default minification filter won't work without\nmipmaps.  The texture uses the default magnification filter, which is also\n", ".  This texture is used on the leftmost square in the image shown below.  The image\nis from the sample program ", ".", "\n", "Note the blending along the edges between colors in the leftmost square.  The blending is\ncaused by the ", " magnification filter.  The second square uses the same\ntexture, but with the ", " magnification filter, which eliminates the blending.\nThe same effect can be seen in the next two squares, which use a black/white\ncheckerboard pattern, one with ", " as the magnification filter and\none using ", ".  The texture is repeated ten times horizontally and vertically\non the square.  In this case, the texture is a tiny 2-by-2 image with two black\nand two white pixels.", "As another example, consider the rightmost square in the image.  The gradient \neffect on that square comes from a texture.  The texture size is 256-by-1 pixels,\nwith the color changing from black to white along the length of the texture.  One\ncopy of the texture is mapped to the square.  For\nthe gradient texture, I used ", " as the texture format, which means\nthat the data consists of one byte per pixel, giving the grayscale value for that\npixel.  The texture can be created using", "See the ", " for more detail.", "We encountered cubemap textures in ", ", where saw how they\nare used in ", " for ", " \nand ", ".  WebGL has built-in support for cubemap\ntextures.  Instead of representing an ordinary image texture, a texture object\ncan hold a cubemap texture.  And two texture objects can be bound to a texture\nobject simultaneously, one holding an ordinary texture and one holding a\ncubemap texture.  The two textures are bound to different targets, ", "\nand ", ".\nA\u00a0texture object, ", ", is bound to the cubemap target in the currently\nactive texture unit by calling", "A given texture object can be either a regular texture or a cubemap texture, not both.\nOnce it has been bound to one texture target, it cannot be rebound to the other target.", "A cubemap texture consists of six images, one for each face of the cube.\nA texture object that holds a cubemap texture has six image slots, identified by\nthe constants", "The constants are used as the targets in ", " and ", ",\nin place of ", ".\n(Note that there are six targets for loading images into a cubemap texture object, but only\none target, ", ", for binding the texture object to a texture unit.)\nA cubemap texture is often stored as a set of six images, which must be loaded\nseparately into a texture object.  Of course, it is also possible for WebGL to create a cubemap by rendering the\nsix images.", "As usual for images on the web, there is the problem\nof asynchronous image loading to be dealt with.  Here, for example, is a function\nthat creates a cubemap texture in my sample program ", ":", "The images for a cubemap must all be the same size.  They must be square.\nThe size should, as usual, be a power of two.  For a cubemap texture, texture \nparameters such as the minification filter are set using the target ", ",\nand they apply to all six faces of the cube.\nFor example, ", "Similarly, ", " will generate mipmaps for all six faces (so\nit should not be called until all six images have been loaded).\nAlso, it is recommended that for a cube map texture, the texture wrap mode should\nbe set to ", ", to avoid the possibility of visible seams between\nthe faces of the cube.  \nHowever, some implementations seem to ignore the repeat mode.  Instead, they do correct\nblending of the images across the boundaries between faces of the cube.", "In a shader program, a cube map texture is represented by a uniform\nvariable of type ", ".  The texture is sampled using the GLSL\nfunction ", ".  For example,", "The first parameter is the ", " variable that represents the texture.\nThe second parameter is a ", ".  Cube map textures are not sampled using\nregular texture coordinates.  Instead, a 3D ", " is used.  The idea is to\npick out a point in the texture.  The texture lies on the surface of a cube.\nTo use a vector to pick out a point in the texture, cast a ray from the center of the cube\nin the direction given by the vector, and check where that ray intersects\nthe cube.  That is, if you put the starting point of the vector at the center \nof the cube, it points to the point on the cube where the texture is to be sampled.", "Since we aren't doing 3D graphics in this chapter, we can't use cube maps\nin the ordinary way.  The sample program ", " uses\na cube map in an interesting, if not very useful way.  The program uses\n2D texture coordinates.  The fragment shader transforms a pair of 2D texture coordinates\ninto a 3D vector that is then used to sample the cubemap texture.  The effect is\nsomething like a photograph produced by a fisheye camera.  Here's what it looks like.", "\n", "The picture on the left imitates a fisheye camera with a 170-degree field of view.\nOn the right the field of view is 330-degrees, so that pixels near the edge\nof the disk actually show parts of the cube that lie behind the camera.", "For each picture, the program draws a square with texture coordinates ranging from 0.0 to 1.0.\nIn the texture coordinate system, pixels at a distance greater than 0.5 from the point (0.5,0.5)\nare colored white.  Within the disk of radius 0.5, each circle around the center is mapped\nto a circle on the unit sphere centered at the point (0,0,0).  That point is then used\nas the direction vector for sampling the cubemap texture.  The point in the texture that appears at the center\nof the disk is the point  where the cube intersects the positive z-axis, that is, \nthe center of the \"positive z\" image from the cube map.  You don't actually need to understand\nthis, but here, for your information, is the fragment shader that does the work:"], "chapter_title": "Introduction to WebGL", "id": 6.4}, {"section_title": "First Examples", "chapter_id": "Chapter 6", "section_id": "Section 6.2", "content": ["We are ready to start working towards our first WebGL\nprograms.  This section begins with a few more details about the\nWebGL graphics context, followed by a short introduction to\n", ", the programming language for WebGL ", ".\nWith that in hand, we can turn to the\nstandard first example: the RGB color triangle.", "We saw in ", " that a WebGL graphics context \nis created by the function ", ", where ", " \nis a reference to the ", " element where the graphics context \nwill draw.  This function takes an optional second parameter that can be used to\nset the value of certain options in the graphics context.  The second parameter\nis only needed if you want to give a non-default value to at least one\nof the options.  The parameter is a JavaScript object whose properties \nare the names of the options.  Here is an example of context creation\nwith options:", "All of the options are boolean-valued.  I will discuss the most useful\nones here:", "\n", " \u2014 determines whether the drawing\nbuffer has an ", " component.\nThis is the alpha component for the image canvas as a whole.  If there is an alpha\ncomponent, then it is possible for pixels in the canvas to be transparent or\ntranslucent, letting the background (on the web page behind the canvas) show \nthrough.  The default value is ", ".  It is safe to set the value to ", ",\nif you want the canvas to be fully opaque.  Setting it to false does not stop\nyou from doing ", " of the drawing color with the image\ncolor; the RGB color components can still be computed by blending.", "\n", " \u2014 determines whether a ", "\nis allocated.  The default value is ", ".\nYou only need a depth buffer if you enable the ", ".\nThe depth buffer is generally not needed for 2D graphics.  If your application\ndoesn't need it, eliminating the depth buffer can save some memory in the GPU.", "\n", " \u2014 is used to request that\n", " be applied to the image.  A WebGL implementation might\nignore the request, for example if antialiasing is not supported by the GPU.\nThe default value is ", ".  Antialiasing can improve the quality of an image,\nbut it can also significantly increase the computation time.", "\n", " \u2014 determines whether the contents\nof the drawing buffer are discarded after the image has been copied to the web page.\nThe default value is ", ".  The drawing buffer is internal to WebGL.  Its\ncontents only become visible on the screen when the web browser copies the \nimage onto the web page.  The default value for ", "\nmeans that once that happens, WebGL can discard its own copy of the image,\nwhich allows the GPU to free up resources for other operations.\nAs long as your rendering functions completely\nredraw the image every time they called, the default is fine.  You should set\nthe value to ", " only if you need to keep the image around so that\nyou can add to it incrementally over time.", "The ", " will cover GLSL more thoroughly.\nBut you will need to know something about the language to understand the examples\nin this section.", "A ", " or ", " \ncan contain global variable declarations, type definitions,\nand function definitions.  One of the functions must be ", "(), which is the\nentry point for the shader; that is, it is the function that is called by the GPU\nto process the vertex or fragment.  The ", "() routine takes no parameters\nand does not return a value, so it takes the form", "(Alternatively, it can be declared as ", "(", ").)", "Control structures in GLSL are limited.  ", " statements take\nthe same form as in C or Java.  But some limitations are placed on the ", " loop syntax,\nand ", " and ", " loops are not allowed.  Data structures include\narrays and ", ", again with some limitations.  We will cover all this in some\ndetail in the next section.", "GLSL's strength lies in its built-in data types and functions for working with\nvectors and matrices.  In this section, we will only need the data types ", ",\n", ", ", ", and ", ".  These types represent, respectively,\n1, 2, 3, or 4 floating point numbers.  Variable declarations are similar to C.\nSome examples are:", "\n", ", ", ", and ", " variables were discussed in\n", ".  They are used for communication between JavaScript and\nthe shader program and between the vertex shader and the fragment shader.\nIn the above examples, I used the prefixes \"a_\", \"u_\", and \"v_\" in the names of the\nvariables, but that is not required.", "It is common to construct a value for a vector from individual numbers or\nfrom shorter vectors.  GLSL has a flexible notation for doing this.\nUsing the variables declared in the above examples, we can write\n", "In the last assignment statement, ", " is the special built-in\nvariable that is used in the vertex shader to give the coordinates of the\nvertex.  ", " is of type ", ", requiring four numbers, \nbecause the coordinates are specified as ", "\n(", ").  The special variable ", " in\nthe fragment shader is also a ", ", giving the coordinates of\nthe pixel as homogeneous coordinates.  And ", " is a ", ",\ngiving the four ", " components for the pixel.", "A vertex shader needs, at a minimum, an attribute to give the coordinates\nof the vertex.  For 2D drawing, it's natural for that attribute to be of\ntype ", ".  If we assume that the values of the attributes are already\nexpressed in clip coordinates, then the complete source code for the vertex\nshader could be as simple as:", "For a corresponding minimal fragment shader, we might simply draw\neverything in yellow.", "The strange first line in this fragment shader has not been explained, but\nsomething like it is required.  It will be explained in the \n", ".", "We are ready to look at our first full WebGL example, which will draw the\nusual RGB color triangle, as shown here:", "\n", "The source code can be found in ", ".\nThe code includes the usual ", "() and ", "() functions\nas discussed in ", " and ", ",\nexcept that I have turned off the \"alpha\" and \"depth\" options in the WebGL\ncontext.  I won't discuss them further.", "The example uses an attribute of type ", " to specify the coordinates\nof the vertices of the triangle.  Coordinates range from \u22121 to 1 in the\ndefault WebGL coordinate system.  For the triangle, the vertex coordinates that\nI use are in that range, so no coordinate transformation is needed.\nSince the color is different at each vertex of the triangle, the vertex color\nis also an attribute.  I use an attribute of type ", " for the vertex colors,\nsince no alpha component is needed in this program.", "The color of interior pixels in the triangle is interpolated from the\ncolors at the vertices.  The interpolation means that we need a varying variable\nto represent the color.  A varying variable is assigned a value in the\nvertex shader, and its value is used in the fragment shader.", "It looks like\nwe need two color variables: an attribute and a varying variable.  We can't\nuse the same variable for both purposes.  The attribute carries the vertex color\nfrom JavaScript into the vertex shader; the varying variable carries the color\nfrom the vertex shader to the fragment shader.  In this case, the color value going\nout of the vertex shader is the same as the value coming in, so the shader just\nhas to copy the value from the color attribute to the varying variable.\nThis pattern is actually fairly common. Here is the vertex shader:", "The fragment shader only has to copy the incoming color value from the\nvarying variable into  ", ", which specifies the outgoing color for the fragment:\n", "In order to compile the shader program, the source code for the shaders has to\nbe in JavaScript strings.  In this case, I construct the strings by concatenating\nconstant strings representing the individual lines of code.  For example, the\nfragment shader source code is included in the JavaScript script as the\nglobal variable", "The line feed character, \"\\n\", at the end of each line is not required, but it \nallows the GLSL compiler to include a meaningful line number in any error message that \nit generates.", "Also on the JavaScript side, we need a global variable for the\nWebGL context.  And we need to provide values for the attribute variables.\nThe rather complicated process was discussed in\n", ".  We need global variables to represent the\nlocation of each attribute in the shader program, and to represent the ", "\nthat will hold the attribute values.  I use the variables", "The graphics context is created in the ", "() function. The other variables\nare initialized in the function ", "() that is called from ", "().  That\nfunction also creates the shader program, using the ", "() function\nfrom ", ":", "To set up the values for an attribute, we need six different JavaScript commands (and more if \nyou count placing the attribute values into a ", ").  The commands\n", " and ", " will most likely be called just once for\neach attribute, so I put them in my initialization routine.  The other four commands\nare in ", "(), the function that draws the image.  In this program, ", "() \nis called just once, so the division of the code into two functions is not really necessary,\nbut in general, a draw function is meant to be called many times. (It would be a particularly\nbad idea to create a new VBO every time ", "() is called.)", "Before drawing the triangle, the ", "() function fills the\ncanvas with a black background.  This is done using the WebGL functions ", "\nand ", ", which have exactly the same functionality as the OpenGL 1.1\nfunctions ", " and ", ".  Here is the code:", "In this function, the variable ", " contains values for the attribute\nnamed \"a_coords\" in the vertex shader. That attribute represents the ", " and ", " coordinates \nof the vertex. Since the attribute is of type ", ", two numbers are required for\neach vertex.  Similarly, the variable ", " contains values for the \"a_color\" attribute\nin the vertex shader, with three numbers per vertex.", "We have now accounted for all the pieces of the RGB triangle program.  Read the\ncomplete ", "\nto see how it fits together.", "Our next example will introduce a few new features.  The example\nis a simple interactive program where the user can place\nshapes in a canvas by clicking the canvas with the mouse.  Properties of the\nshape are taken from a set of popup menus.  The properties include the color and degree of transparency of\nthe shape, as well as which of \nseveral possible shapes is drawn.  The shape is centered at the point where the user clicks.", "The sample program\nis ", ". Here is a\ndemo version of the program so you can see how it works.", "\n", "\n", "In the RGB triangle example, ", " is an attribute, since a different\ncolor is assigned to each vertex of the triangle primitive.  In the ", " program,\nall vertices, and in fact all pixels, in a primitive have the same color.  That means\nthat color can be a uniform variable.  The example also allows transparency, so\ncolors need an alpha component as well as the RGB components.  It was convenient\nin the program to treat the alpha and RGB components as separate quantities, so I\nrepresent them as two separate uniform variables in the shader program.  The color\nand alpha uniforms are used in the fragment shader to assign the fragment's color.\nIn fact, that's the only thing the fragment shader does, so the complete source\ncode is as follows:", "To work with a uniform variable on the JavaScript side, we need to know its location\nin the shader program. The program gets the locations of the two uniform variables in\nthe ", "() function using the commands", "The program has two popup menus that let the user select\nthe color and alpha that are to be used for drawing\na primitive.  When a shape is drawn, the values from the menus\ndetermine the values of the uniforms:", "Values for uniform variables are set using the ", " family of functions.\nIn this case, ", "[", "] is an array of three numbers holding the RGB\ncolor components for the color, so the function ", " is used to set\nthe value: The \"3f\" means that 3 floating point values are provided, and the \"v\" means\nthat the three values are in an array.  Note that three floating point values are required to\nmatch the type, ", ", of the uniform variable in the shader.  The value of ", "\nis a single floating point number, so the corresponding uniform variable is set using\n", ".", "In order for the alpha component of the color to have any effect, alpha blending\nmust be enabled. This is done as part of initialization with\nthe two commands", "The first line enables use of the alpha component.  The second tells how the\nalpha component is to be used.  The \"blendFunc\" used here is appropriate for\ntransparency in 2D.  The same commands were used in ", " in\nOpenGL\u00a01.1.", "When the program starts, the user sees a blank white canvas.  When the user clicks\nthe canvas, a shape is added.  When the user clicks again, a second shape is added\u2014and\nthe first shape better still be there!  However, this is not the default behavior for\nWebGL!", "When the user clicks the canvas, an event-handler function for the mousedown event\nis called.  The shape is drawn in that function.  When the function returns, WebGL\ntells the web browser that the image has been modified, and the web browser copies the\nnew image to the screen.  Once that happens, as discussed earlier in this section, the\ndefault behavior for WebGL is to discard the image.  But this means that the second\nmouse click is drawing on a blank canvas, since the shape from the first mouse click has\nbeen erased.", "To fix this problem, the ", " option in the WebGL graphics\ncontext must be set to ", ".  The ", " program creates the\ncontext with", "Note that this program does not have a ", "() function that redraws\nthe entire image.  All the drawing is done in the mouse-handling function, ", ".\nThings could have been done differently.  The program could have used a data structure\nto store information about the shapes that have been drawn.  Clicking the canvas would\nadd an item to the list, and the entire image would then be redrawn, including the\nnew shape.  In the actual program, however, the only record of what's in the image is\nthe image itself.  (In the terminology of ", ", it\nis a ", " rather than a ", ".)", "WebGL uses a default ", " in which each of the coordinates ranges\nfrom \u22121 to 1.  Of course, we would like to use a more convenient coordinate \nsystem, which means that we need to apply a coordinate transformation to transform the\ncoordinates that we use into the default coordinate system.  In the ", "\nprogram, the natural coordinate system is pixel coordinates on the canvas.  In the pixel\ncoordinate system, the ", "-coordinate ranges from ", " at the left to\n", " at the right, and ", " ranges from 0 at the top to\n", " at the bottom.  The equations for transforming pixel coordinates\n(", ") to default coordinates (", ") are", "In WebGL, the coordinate transformation is usually applied in the vertex shader.\nIn this case, to implement the transformation, the vertex shader just needs to know the\nwidth and height of the canvas.  The program provides the width and height to the\nvertex shader as uniform variables.  The original, pixel coordinates of the vertex\nare input to the vertex shader as an attribute.  The shader applies the coordinate\ntransformation to compute the value of ", ", which must be expressed\nin the default coordinate system.  Here is the vertex shader source code:", "Transformations can be much more complicated than this, especially in 3D, but the\ngeneral pattern holds:  Transformations are represented by uniform variables and are\napplied in the vertex shader.  In general, transformations are implemented as \n", ".  We will see later that uniform variables can\nbe matrices and that the shader language GLSL has good support for matrix operations.", "In order to draw a shape, we need to store the pixel coordinates for that shape\nin a ", "; then, we have to load the values from that array into\nthe buffer associated with the \"a_coords\" attribute;  and finally, we must call ", "\nto do the actual drawing.  The coordinates for the shape can be computed based on\nwhat type of shape is being drawn and on the point where the user clicked.\nFor example, the coordinate array for a circle is created by the following\ncode, where ", " and ", " are the pixel coordinates for the point that was\nclicked:", "The circle is approximated as a 32-sided regular polygon, with a radius \nof 50 pixels.  Two coordinates are\nrequired for each vertex, so the length of the array is 64.  The code for the\nother shapes is similar.  Once the array has been created,  the shape is drawn\nusing", "In the last line, ", "/2 is the number of vertices in the shape,\nsince the array holds two numbers per vertex.\nNote also that the last parameter to ", " is ", ",\nwhich is appropriate when the data in the VBO will only be used once and then\ndiscarded.", "Although the demo version of the sample program has the same functionality,\nI implemented shape drawing differently in the two versions.  Notice that all\ncircles in the program are the same; they are just in different locations.\nIt should be possible to draw the circle in its own ", ",\nand then apply a ", " to move the circle to\nits desired position in the scene.  This is the approach that I take in the\ndemo version of the program.", "There are four kinds of shape: circles, squares, triangles, and stars.  In the\ndemo version, I create a separate VBO for each kind of shape.  The VBO for\na shape contains vertex coordinates for that shape in object coordinates, with\nthe shape centered at (0,0).  Since the object coordinates will never change, the VBO can\nbe created once and for all as part of program initialization.  For example,\nthe VBO for the circle is created with", "Note the use of ", " in the last line.  It is appropriate since\nthe data can be reused to draw many different circles.", "To draw a shape with its center at (", "), a ", " must be\napplied to the coordinates in the VBO.  I added the translation to the vertex shader,\nwith a new uniform variable to represent the translation amount:", "You would probably find it worthwhile to read the full source code for the\n", " as well as the\n", ".", "The final example in this section demonstrates the ", " primitive.\nA ", " primitive is basically a set of disconnected vertices.  By default,\neach vertex is rendered as a single pixel.  However, a program can specify a larger\nsize.  In OpenGL\u00a01.1, this was done with the function ", "().  In \nWebGL, that function does not exist.  Instead, the size is under the control of the\nvertex shader.", "When working on one of the vertices of a ", " primitive, the vertex shader should\nassign a value to the special built-in variable ", ".  The variable is\nof type ", ".  It gives the size of the vertex, in pixels.  The vertex\nis rendered as a square, centered at the vertex position, whose width and height are given\nby ", ".  What this really means is that the fragment shader will be\ncalled once for each pixel in that square.  Note that there is an implementation-dependent\nlimit on the size of points, which can be fairly small.  The only size that is guaranteed \nto exist is one pixel, but most implementations seem to support point sizes at least up to \n64 pixels, and possibly much larger.", "When the fragment shader is called for a ", " primitive, it is\nprocessing one pixel in the square of pixels surrounding the vertex.  The special\nfragment shader variable ", " tells the shader the location of\nthe pixel within that square.  The value of ", " is an input to\nthe shader.  The type of ", " is ", ",\nso it has two floating point components.  The value of each component is in the\nrange 0 to\u00a01.  The first component, ", ", is 0 at the left\nedge of the square and 1 at the right.  The second component, ", ",\nis 0 at the top of the square and 1 at the bottom.  So, for example, the value\nis (0,0) at the top-left corner, (1,0) at the top-right corner, and\n(0.5,0.5) at the center of the square.  (That, at least, is what the specification\nsays, but some implementations incorrectly put (0,0) at the bottom left corner.)", "If the fragment shader uses ", "\nin its computation, the color of the square can vary from pixel to pixel.  As a simple\nexample, setting", "would render each vertex in the primitive as a square color gradient whose color\nvaries horizontally from black on the left edge of the square to red on the right edge.\nIn the sample program, I use ", " to render the vertex as a disk\ninstead of a square.  The technique uses a new GLSL statement, ", ", which\nis available only in the fragment shader.  When the fragment shader executes the\nstatement", "the fragment shader terminates, and all further processing of the pixel is prevented.\nIn particular, the color of the pixel in the image does not change.  I use ", " if the\ndistance from ", " to the center, (0.5,0.5), is greater than 0.5.\nThis discards pixels that do not lie in the disk of radius\u00a00.5.\nGLSL has a function for computing the distance between two vectors, so the test\nin the fragment shader is written\n", "The sample program is ", ".  It shows an\nanimation of colored disks moving in the canvas and bouncing off the edges.\nAll of the disks are drawn in one step as a single primitive of type ", ".\nThe size of the disks is implemented as a uniform variable, so that all the disks\nhave the same size, but the uniform size can be different in different frames of the animation.\nIn the program, the user controls the size with a popup menu.  Here is a\ndemo version of the program, with the same functionality:\n", "\n", "\n", "In the program, the user can select whether the disks have random colors or\nare all colored red.  Since each disk is a vertex of a single POINTS primitive,\nthe fact that the disks can have different colors means that the color has\nto be given by an attribute variable.  To implement random colors, a \n", " is filled with random numbers, three for\neach vertex.  The values are loaded into a VBO, and the values for the color\nattribute are taken from the VBO.  But what happens when all the disks are\nred?  Do we have to fill an array with multiple copies of \"1,\u00a00,\u00a00\" and\nuse that data for the attribute?  In fact, we don't.  If we disable the\nVertexAtrribArray for the color attribute, then that attribute will have\nthe same value for every vertex.  The value is specified by the ", "\nfamily of functions.  So, in the sample program, the code for providing \nvalues for the color attribute is", "See the ", " for full\ndetails of the example.", "It is a sad fact that OpenGL programmers often find themselves looking at a blank\nscreen, with no clear indication of what went wrong.  In many cases, this is due to\na programming logic error, such as accidently drawing a region of 3D space that contains\nno geometry.  However, sometimes it's due to an error in the use of the API. In WebGL,\nand in OpenGL more generally, an error such as an illegal parameter value will not in general crash \nthe program or produce any automatic notification of the error.  Instead, when WebGL\ndetects such an error, it ignores the illegal function call, and it\nsets the value of an error code that gives some indication of the nature of the error.", "A program can check the current value of the error code by calling ", "().\nThis function returns an integer error code.  The return value is ", " if\nno error has occurred.  Any other return value means that an error has occurred.\nOnce an error code has been set, it stays set until ", "() is called,\neven if other, correct WebGL operations have been executed in the meantime.\nCalling ", "() retrieves the value of the error code and resets its\nvalue to ", ".  (So, if you call ", "() twice in a row,\nthe second call will always return ", ".)  This means that when \n", "() returns an error, the error might actually have been generated\nby an instruction that was executed some time ago.", "As an example, consider a call to ", "(", ").\nIf ", " is not one of the seven legal WebGL primitives, then WebGL will\nset the error code to ", ".  If ", " or ", "\nis negative, the error code is set to ", ".  If no shader program\nhas been installed with ", ", the error is ", ".\nIf no data has been specified for an enabled vertex attribute, an error of type\n", " occurs.  These four error codes are, in fact, the most common.", "It is both impractical and inefficient to call ", " after each\nWebGL function call.  However, when something goes wrong, it can be used as\na debugging aid.  When I suspect an error, I might insert code such as", "at several points in my code.  The numeric value of ", " is zero.\nAny non-zero value means that an error occurred at some point before the call\nto ", ".  By moving the output statements around in the code, I\ncan narrow in on the statement that actually produced the error.", "Note that some browsers might automatically output certain information about \nincorrect use of WebGL to their console.  It's always a good idea to check the\nbrowser console when running a WebGL program that is under development!"], "chapter_title": "Introduction to WebGL", "id": 6.2}, {"section_title": "Image Textures", "chapter_id": "Chapter 4", "section_id": "Section 4.3", "content": ["Uniformly colored 3D objects look nice enough, but they are a little\nbland.  Their uniform colors don't have the visual appeal of, say, a brick wall or a plaid couch. \nThree-dimensional objects can be made to look more interesting and more realistic by adding\na ", " to their surfaces.  A texture, in general, is some sort\nof variation from pixel to pixel within a single ", ".  \nWe will consider only\none kind of texture:  ", ".\nAn image texture can be applied to a surface to make the color of the surface\nvary from point to point, something like painting a copy of the image onto the\nsurface. Here is a picture that shows six objects with various image textures:", "\n", "(Topographical Earth image, courtesy ", ".  \nBrick and metal textures from ", ".  \nEarthAtNight image taken from the ", "\nweb site; it is also a NASA/JPL image.  Copies of the images can be found in the folder named\n", " in either the ", "\nor ", " folder inside the source folder of the web site download.  Images\nfrom that folder will be used in several examples in this book.)", "Textures might be the most complicated part of OpenGL, and they are a part that has survived,\nand become more complicated,\nin the most modern versions since they are so vital for the efficient creation of realistic\nimages.  This section covers only part of the OpenGL 1.1 texture API.  We will see more\nof textures in later chapters.", "Note that an image that is used as a texture should have a width and a height that are powers of two,\nsuch as 128, 256, or 512.  This is a requirement in OpenGL 1.1.  The requirement might be\nrelaxed in some versions, but it's still a good idea to use \n", "  Some of the things discussed\nin this section will not work with non-power-of-two textures, even on modern systems.", "When an image texture is applied to a surface,  the default behavior is to multiply\nthe ", " components of pixels on the surface by the color components\nfrom the image.  The surface color will\nbe modified by light effects, if lighting is turned on, before it is multiplied by the\ntexture color.  It is common to use white as the surface color.  If a different color is\nused on the surface, it will add a \"tint\" to the color from the texture image.", "When a texture is applied to a surface, each point on the surface has to correspond to a point\nin the texture.  There has to be a way to determine how this mapping is computed.  For that,\nthe object needs ", ".   As is generally the\ncase in OpenGL, texture coordinates are specified for each vertex of a primitive.  Texture\ncoordinates for points inside the primitive are calculated by interpolating the values\nfrom the vertices of the primitive.", "A texture image comes with its own 2D coordinate system.  Traditionally, ", " used for the horizontal\ncoordinate on the image and ", " is used for the vertical coordinate.  \nThe ", "\u00a0coordinate is\na real number that ranges from 0 on the left of the image to 1 on the\nright, while ", " ranges from 0 at the bottom to 1 at the top.  Values of ", "\nor ", " outside of the range 0 to 1 are not inside the image, but such values are\nstill valid as texture coordinates.  Note that texture coordinates are not based on\npixels.  No matter what size the image is, values of ", " and ", " between 0 and\n1 cover the entire image.", "To draw a textured primitive, we need a pair of numbers (", ") for each vertex.\nThese are the texture coordinates for that vertex.  They tell which\npoint in the image is mapped to the vertex.  For example, suppose that we\nwant to apply part of an ", " image to a triangular primitive. Let's say that  the area\nin the image that is to be mapped onto the primitive is the triangle shown here outlined in thick orange:\n", "\n", "The vertices of this area have (", ") coordinates (0.3,0.1), (0.45,0.6), and\n(0.25,0.7).  These coordinates from the image\nshould be used as the texture coordinates for the vertices of the triangular primitive.\n", "The texture coordinates of a vertex are an ", " of the vertex, \njust like color, normal vectors, and material properties.   Texture coordinates\nare specified by the family of functions ", ", including the functions\n", "(", "), ", "(", "), ", "(", "),\nand ", "(", ").  The OpenGL state includes a current set of\ntexture coordinates, as specified by these functions.  When you specify\na vertex with ", ", the current texture coordinates are copied and become\nan attribute that is associated with the vertex.  As usual, this means that the\ntexture coordinates for a vertex must be specified ", " ", "\nis called.  Each vertex of a primitive will\nneed a different set of texture coordinates.", "For example, to apply the triangular region in the image shown above to the triangle\nin the ", "-plane with vertices at (0,0), (0,1), and (1,0), we can say:", "Note that there is no particular relationship between the (", ") \ncoordinates of a vertex, which give its position in space, and the (", ")\ntexture coordinates associated with the vertex.  In fact, in this case, the triangle that I am drawing has\na different shape from the triangular area in the image, and that piece of the\nimage will have to be stretched and distorted to fit.  Such distortion occurs in most\nuses of texture images.", "Sometimes, it's difficult to decide what texture coordinates to use.  One case where it's\neasy is applying the complete texture to a rectangle.   Here is a code segment that draws\na square in the ", "-plane, with appropriate texture coordinates to map\nthe entire image onto the square:", "Unfortunately, the standard shapes in the GLUT library do not come with texture coordinates\n(except for the teapot, which does).  I\u00a0have written a set of\nfunctions for drawing similar shapes that do come with texture coordinates.  The functions\ncan be found in ", " for JOGL\nor in ", " (plus the corresponding header file\n", ") for\u00a0C.\nOf course, there\nare many ways of applying a texture to a given object. If you use my functions, you are\nstuck with my decision about how to do so.", "The sample program ", " or ", "\nlets you view several different texture images on my textured shapes.", "One last question: What happens if you supply texture coordinates that\nare not in the range from 0 to 1?  It turns out that such values are legal.\nBy default, in OpenGL\u00a01.1, they behave as though the entire ", "-plane is filled\nwith copies of the image. For example, if the texture coordinates for a square\nrange from 0 to 3 in both directions, instead of 0 to 1, then you get nine copies\nof the image on the square (three copies horizontally by three copies vertically).", "To draw a textured primitive using ", " or ", ",\nyou will need to supply the texture coordinates in a ", ",\nin the same way that you supply vertex coordinates, colors, and normal vectors.\n(See ", ".)  The details are similar:  You have to\nenable the use of a texture coordinate array by calling", "and you have to tell OpenGL the location of the data using the function", "The ", ", for us, will always be 2.  (OpenGL also allows 3 or 4 texture coordinates,\nbut we have no use for them.)  The ", " can be ", ", ", ",\nor ", ".  The ", " will ordinarily be zero, to indicate that there is\nno extra data between texture coordinates in the array.  The last parameter is an\narray or pointer to the data, which must be of the type indicated by the ", ".\nIn ", ", as usual, you would use an ", " instead of an array.", "When a texture is applied to a surface, the pixels in the texture\ndo not usually match up one-to-one with pixels on the surface,\nand in general, the texture must be stretched or shrunk as it\nis being mapped onto the surface.  Sometimes, several pixels in\nthe texture will be mapped to the same pixel on the surface.  In this case,\nthe color that is applied to the surface pixel must somehow be computed\nfrom the colors of all the texture pixels that map to it.\nThis is an example of \"filtering\"; in particular, it uses a\n", " because the texture is being shrunk.\nWhen one pixel from the texture covers more than one pixel on the\nsurface, the texture has to be magnified, and we need a\n", ".", "One bit of terminology before we proceed:  The pixels in a texture are referred to\nas ", ", short for \"texture pixel\" or \"texture element\", and I will use\nthat term from now on.", "When deciding how to apply a texture to a point on a surface,\nOpenGL knows the texture coordinates for that point.  Those texture\ncoordinates correspond to one point in the texture, and that point\nlies in one of the texture's texels.  The easiest thing to do\nis to apply the color of that texel to the point on the surface.\nThis is called \"nearest neighbor filtering.\"\nIt is very fast, but it does not usually give good results.\nIt doesn't take into account the difference in size between the\npixels on the surface and the texels in the image.  An improvement on nearest neighbor\nfiltering is \"linear filtering,\" which can take an average of\nseveral texel colors to compute the color that will be applied to\nthe surface.", "The problem with linear filtering is that it will be very inefficient\nwhen a large texture is applied to a much smaller surface area.  In this\ncase, many texels map to one pixel, and computing the average of so\nmany texels becomes very inefficient.  There is a neat solution for\nthis:  ", ".", "A mipmap for a texture is a scaled-down version of that texture.\nA complete set of mipmaps consists of the full-size texture, a half-size\nversion in which each dimension is divided by two,\na quarter-sized version, a one-eighth-sized version, and so on.\nIf one dimension shrinks to a single pixel, it is not reduced further,\nbut the other dimension will continue to be cut in half until it\ntoo reaches one pixel.  In any case, the final mipmap consists of\na single pixel.  Here are the first few images in the set of mipmaps for a brick\ntexture:", "\n", "You'll notice that the mipmaps become small very quickly.  The total\nmemory used by a set of mipmaps is only about one-third more than the\nmemory used for the original texture, so the additional memory requirement \nis not a big issue when using mipmaps.", "Mipmaps are used only for minification filtering.  They\nare essentially a way of pre-computing the bulk of the averaging\nthat is required when shrinking a texture to fit a surface.  To texture a pixel,\nOpenGL can first select the mipmap whose texels most closely match the\nsize of the pixel.  It can then do linear filtering on that mipmap\nto compute a color, and it will have to average at most a few texels \nin order to do so.", "In newer versions of OpenGL, you can get OpenGL to generate mipmaps\nautomatically.  In OpenGL 1.1, if you want to\nuse mipmaps, you must either load each mipmap individually, or you must\ngenerate them yourself.  (The GLU library has a method, ", "\nthat can be used to generate a set of mipmaps for a 2D texture.)  However, my sample\nprograms do not use mipmaps.", "OpenGL can actually use one-dimensional and three-dimensional textures, as well as\ntwo-dimensional.  Because of this, many OpenGL functions dealing with textures\ntake a ", " as a parameter, to tell whether\nthe function should be applied to one, two, or three dimensional textures.  For us,\nthe only texture target will be ", ".", "There are a number of options that apply to textures, to control the details of how\ntextures are applied to surfaces. Some of the options can be set using the\n", "() function, including two that have to do with filtering.\nOpenGL supports several different filtering techniques for minification\nand magnification.  The filters can be set using ", "():", "The values of ", " and ", " are constants that specify the\nfiltering algorithm.  For the ", ", the only options are \n", " and ", ", giving nearest neighbor and\nlinear filtering.  The default for the MAG filter is ", ", and\nthere is rarely any need to change it.  For ", ", in addition\nto ", " and ", ", there are four options that\nuse mipmaps for more efficient filtering.  The default MIN filter\nis ", ", which does averaging between\nmipmaps and nearest neighbor filtering within each mipmap.  For even \nbetter results, at the cost of greater inefficiency, you can use\n", ", which does averaging both between and\nwithin mipmaps.  The other two options are ", "\nand ", ".", "\n", "  If you are ", " using mipmaps for a texture,\nit is imperative that you change the minification filter for that texture\nto ", " or, less likely, ", ".\nThe default MIN filter ", " mipmaps, and if mipmaps are not available, then\nthe texture is considered to be improperly formed, and OpenGL ignores it!\nRemember that if you don't create mipmaps and if you don't change the minification\nfilter, then your texture will simply be ignored by OpenGL.", "There is another pair of texture parameters to control how texture coordinates\noutside the range 0 to 1 are treated.  As mentioned above, the default is to \nrepeat the texture.  The alternative is to \"clamp\" the texture.  This means that \nwhen texture coordinates outside the range 0 to 1 are specified, those values\nare forced into that range:  Values less than 0 are replaced by 0, and values\ngreater than 1 are replaced by 1.  Values can be clamped separately in the ", "\nand ", " directions using", "Passing ", " as the last parameter restores the default\nbehavior.  When clamping is in effect, texture coordinates outside the\nrange 0 to 1 return the same color as a texel that lies along the outer\nedge of the image.  Here is what the effect looks like on two textured squares:", "\n", "The two squares in this image have ", " and ", " texture coordinates that range from\n\u22121 to 2.  The original image lies in the center of the square.  For the square on the left, the texture\nis repeated.  On the right, the texture is clamped.", "When a texture is applied to a primitive,\nthe texture coordinates for a vertex determine which point in the texture is mapped\nto that vertex.  Texture images are 2D, but OpenGL also supports one-dimensional textures and \nthree-dimensional textures.  This means that texture coordinates cannot be restricted to\ntwo coordinates.  In fact, a set of texture coordinates in OpenGL is\nrepresented internally in the form of ", ",\nwhich are referred to as (", ",", ",", ",", ").  We have\nused ", " to specify texture ", " and ", " coordinates, but a call to\n", "(", ",", "), for example, is really just shorthand for\n", "(", ",", ",0,1).", "Since texture coordinates are no different from vertex coordinates, they can\nbe transformed in exactly the same way.  OpenGL maintains a ", "\nas part of its state, along with the ", " \nand ", "\ntransformations.  The current value of each of the three transformations is stored\nas a ", ".\nWhen a texture is applied to an object, the texture coordinates that were\nspecified for its vertices are transformed by the texture matrix.  The transformed\ntexture coordinates are then used to pick out a point in the texture.  Of course, the\ndefault texture transform is the ", ", which doesn't change the coordinates.", "The texture matrix can represent scaling, rotation, translation and combinations\nof these basic transforms.  To specify a texture transform, you have to use\n", "() to set the matrix mode to ", ".  With this mode in\neffect, calls to methods such as ", ", ", ", and ", "\nare applied to the texture matrix.  For example to install a texture transform that scales\ntexture coordinates by a factor of two in each direction, you could say:", "Since the image lies in the ", "-plane, only the first two parameters of\n", " matter.  For rotations, you would use (0,0,1) as the axis of rotation,\nwhich will rotate the image within the ", "-plane.", "Now, what does this actually mean for the appearance of the texture on a surface? In the example,\nthe scaling transform multiplies each texture coordinate by 2.  For example, if a\nvertex was assigned 2D texture coordinates (0.4,0.1), then after the texture transform is applied,\nthat vertex will be mapped to the point (", ",", ") = (0.8,0.2)\nin the texture.  The texture coordinates vary ", " on the surface\nas they would without the scaling transform.  A region on the surface that would map\nto a 1-by-1 square in the texture image without the transform will instead map\nto a 2-by-2 square in the image\u2014so that a larger piece of the image will be\nseen inside the region.  In other words, the texture image will be ", " by\na factor of two on the surface!  More generally, the effect of a texture transformation\non the appearance of the texture is the ", " of its effect \non the texture coordinates.  (This is exactly analogous to the inverse relationship between a viewing\ntransformation and a modeling transformation.)  If the texture transform is translation to\nthe right, then the texture moves to the left on the surface.  If the texture transform\nis a counterclockwise rotation, then the texture rotates clockwise on the surface.", "I mention texture \ntransforms here mostly to show how OpenGL can use transformations\nin another context.  But it is sometimes useful to transform a texture to make it fit\nbetter on a surface.  And for an unusual effect, you might even animate the texture transform\nto make the texture image move on the surface.  Here is a demo that lets you\nexperiment with texture transforms and see the effect.  On the left, you see the region in \nthe ", "-plane for ", " and ", " between \u22121 and 2.  A box outlines the\nregion in the texture that maps to a region on the 3D object with texture coordinates\nin the range 0 to 1.  You can drag the sliders to apply texture transforms to see how the\ntransforms affect the box and how they affect the texture on the object.  See the help text\nin the demo for more information.", "\n", "\n", "It's about time that we looked at the process of getting an image into OpenGL so\nthat it can be used as a texture.  Usually, the image starts out in a file.\nOpenGL does not have functions for loading images from a file.  For now, we\nassume that the file has already been loaded from the file into the computer's memory.  \nLater in this section, I will explain how that's done in C and in Java.", "The OpenGL function for loading image data from the computer's\nmemory into a 2D texture is ", "(), which takes the form:", "The ", " should be ", ".  The ", "\nshould ordinarily be\u00a00.  The value 0 is for loading the main texture; a larger value is used to load \nan individual mipmap.  The\n", " tells OpenGL how you want the texture data to be stored in OpenGL texture\nmemory. It is ordinarily ", " to store an 8-bit red/green/blue component for\neach pixel.  Another possibility is ", ", which adds an alpha component.\nThe ", " and ", " give the size of the image; the values should\nusually be powers of two.  The value of ", " should be 0; the only other\npossibility is 1, which indicates that a one-pixel border has been added around the\nimage data for reasons that I will not discuss.  The last three parameters describe the image data.  The\n", " tells how the original image data is represented in the computer's memory, such as\n", " or ", ".  The ", " is usually ", ",\nindicating that each color component is represented as a one-byte value in the\nrange 0 to 255.  And ", " is a pointer to the start of the actual\ncolor data for the pixels.  The pixel data has to be in a certain format, but that\nneed not concern us here, since it is usually taken care of by the functions that are\nused to read the image from a file.  (For JOGL, the pointer would be replaced by a buffer.)", "This all looks rather complicated, but in practice, a call to ", "\ngenerally takes the following form, except possibly with ", " replaced with ", ".", "Calling this function will load the image into the texture,\nbut it does not cause the texture to be used.  For that, you also have to\ncall", "If you want to use the texture on some objects but not others, you can\nenable ", " before drawing objects that you want to be\ntextured and disable it before drawing untextured objects.  You can also change\nthe texture that is being used at any time by calling ", ".", "Texture images for use in an OpenGL program usually come from an\nexternal source, most often an image file.  However, OpenGL is itself\na powerful engine for creating images.  Sometimes, instead of loading\nan image file, it's convenient to have OpenGL create the image internally,\nby rendering it.  This is possible because OpenGL can read texture\ndata from its own color buffer, where it does its drawing.  To\ncreate a texture image using OpenGL, you just have to draw the\nimage using standard OpenGL drawing commands and then load that\nimage as a texture using the method", "In this method, ", " will be ", "; ", "\nshould be zero; the ", " will ordinarily be ", "\nor ", "; ", " and ", " specify\nthe lower left corner of the rectangle from\nwhich the texture will be read; ", " and ", "\nare the size of that rectangle; and ", " should be 0.\nAs usual with textures, the ", " and ", " should\nordinarily be powers of two. A call to ", " \nwill typically look like ", "The end result is that the specified rectangle from\nthe color buffer will be copied to texture memory and will become the\ncurrent 2D texture.  This works in the same way as a call to\n", "(), except for the source of the image data.", "An example can be found in the JOGL program ", " or\nin the C version ", ".\nThis program draws the windmill-and-cart scene from  ", " and then uses that\ndrawing as a texture on 3D objects.  Here is a demo version of the program.", "\n", "\n", "The texture can be animated!  For the animation, a new texture is drawn for each frame.\nAll the work is done in the display method.  In that method,\nthe current frame of the windmill-and-cart scene is first drawn as a 2D scene with\nlighting disabled.  This picture is not shown on the computer screen;\nthe drawing is done off-screen and the image will be erased and replaced with the\n3D image before it's ever shown on screen.  The ", "()\nfunction is then called to copy the\nscene into the current texture.  Then, the color buffer is cleared, lighting is\nenabled, and a 3D projection is set up, before finally drawing the 3D object\nthat is seen on the computer screen.", "Everything that I've said so far about textures was already true for OpenGL 1.0.\nOpenGL 1.1 introduced a new feature called ", "\nto make texture handling more efficient.\nTexture objects are used when you need to work with several texture images in the same program.\nThe usual method for loading texture images, ", ", transfers data from your\nprogram into the graphics card.  This is an expensive operation, and switching among\nmultiple textures by using this method can seriously degrade a program's performance.\nTexture objects offer the possibility of storing texture data for multiple textures on the graphics\ncard.  With texture objects, you can switch from one texture object to another with a single, \nfast OpenGL command:  You just have to tell OpenGL which texture object you want to use.\n(Of course, the graphics card has only a limited amount of memory for storing textures,\nand you aren't guaranteed that all of your texture objects will actually be stored on the graphics card.\nTexture objects that don't fit in the graphics card's memory are no more efficient\nthan ordinary textures.)", "Texture objects are managed by OpenGL and the graphics hardware.\nA texture object is identified by an integer ID number.  To use a texture object,\nyou need to obtain an ID number from OpenGL.  This is done with the ", "\nfunction:", "This function can generate multiple texture IDs with a single call.  The first parameter\nspecifies how many IDs you want.  The second parameter says where the generated IDs will be stored.\nIt should be an array whose length is at least ", ".  \nFor example, if you plan to use three texture objects, you can say", "You can then use ", "[0], ", "[1], and ", "[2] to refer\nto the textures.", "Every texture object has its own state, which includes  the\nvalues of texture parameters such as ", " as well as the\ntexture image itself.  To work with a specific texture object, you must first\ncall", "where ", " is the texture ID returned by ", ".\nAfter this call, any use of ", ", ", ", or ", " \nwill be applied to the texture object with ID ", ".", "Similarly, when a textured primitive is rendered, the texture that is used is the\none that was most  recently bound using ", ".\nA typical pattern would be to load and configure a number of textures\nduring program initialization:", "Then, while rendering a scene, you would call ", " every time\nyou want to switch from one texture image to another texture image.  This would be much more\nefficient than calling ", " every time you want to switch textures.", "OpenGL 1.1 reserves texture ID zero as the default texture object, which is bound initially.\nIt is the texture object that you are using if you never call ", ".  This means\nthat you can write programs that use textures without ever mentioning ", ".\n(However, I should note that when we get to WebGL, that will no longer be true.)", "The small sample program ", " shows how to use texture\nobjects in C.  In is available only in C since, as we will see, JOGL has its own way of working\nwith texture objects.", "We have seen how to load texture image data from memory into OpenGL.\nThe problem that remains is how to get the image data into memory before\ncalling ", ".  One possibility is to compute the data\u2014you can\nactually have your program generate texture data on the fly.  More likely,\nhowever, you want to load it from a file.  This section looks at how that\nmight be done in C.  You will probably want to\nuse a library of image-manipulation functions.  Several free image processing libraries are available.  I will\ndiscuss one of them, ", ".\nFreeImage can be obtained from ", ",\nbut I was able to use it in Linux simply by installing the package ", ".\nTo make it available to my program, I added ", " to the\ntop of my C program, and I added the option ", " to the ", " command to make\nthe library available to the compiler.  (See the sample program\n", " for an example that uses\nthis library.)  Instead of discussing FreeImage in detail, I present a\nwell-commented function that uses it to load image data from a file:", "After this function has been called, the data that we need for\n", "() is in the global variables ", ",\n", ", and ", " (or ", " is 0\nto indicate that the attempt to load the image failed).  There\nis one complication:  FreeImage will store the color components\nfor a pixel in the order red/green/blue on some platforms but in\nthe order blue/green/red on other platforms.  The second data format\nis called ", " in OpenGL.  If you use the wrong format\nin ", "(), then the red and blue components of the\ncolor will be reversed.  To tell the difference, you can use\nthe FreeImage constant ", ", which tells the position\nof the red color component in pixel data.  This constant will be\n0 if the format is ", " and will be 2 if the format\nis ", ".  So, to use the texture in OpenGL, you might\nsay:", "To be even more careful, you could check that the width and the height\nof the image are powers of two.  If not, you can resize it using\nthe function ", "() from the FreeImage library.", "We turn finally to using texture images in Java.  \nJOGL comes with several classes that make it fairly easy to use textures in Java,\nnotably the classes ", " and\n", " in package\n", " and\n", " in package\n", ".  For an example of using\ntextures with JOGL, see the sample program\n", ".", "An object of type ", " represents\na texture that has already been loaded into OpenGL.  Internally,\nit uses a texture object to store the texture and its\nconfiguration data.  If ", " is an object of type ", ",\nyou can call", "to use the texture image while rendering objects.  The parameter, ", ", as usual,\nis a variable of type ", " the represents the OpenGL drawing context.\nThis function is equivalent\nto calling ", " for the OpenGL texture object that is\nused by the Java ", ".  You still need to\nenable ", " by calling\n", "(", " or, equivalently,", "You can set texture parameters in the usual way, by calling\n", "() while the texture is bound, \nbut it is preferable to use a method from the ", "\nclass to set the parameters:", "This will automatically bind the texture object before setting\nthe texture parameter.  For example, ", "So, once you have a ", ", it's pretty easy\nto use.  But there remains the problem of creating ", " objects.\nFor that, you can use static methods in the ", "\nand ", " classes.  For example, if ", "\nis the name of an image file (or a path to such a file), then you can say", "to load a texture from the file into a ", " object, ", ". \nThe ", " parameter here, and in\nall the methods we will look at, tells JOGL whether or not to create mipmaps for\nthe texture; by passing ", ", we automatically get a full set of mipmaps!", "\n", " Java's texture creation functions will only work\nwhen an OpenGL context is \"current.\"  This will be true in the event-handling\nmethods of a ", ", including\nthe ", "() and ", "() methods.  However, it will ", "\nbe true in ordinary methods and constructors.", "Of course, in Java, you are more likely to store the image as\na resource in the program than as a separate file.  If ", " is\na path to the image resource, you can load the image into a texture with", "The third parameter to this version of ", " specifies the image type\nand can be given as a string containing a file suffix such as \"png\" or \"jpg\"; the value ", "\ntells OpenGL to autodetect the image type, which should work in general.\n(By the way, all the texture-loading code that I discuss here can throw\nexceptions, which you will have to catch or otherwise handle in some way.)", "One problem with all this is that textures loaded in this way will be\nupside down!  This happens because Java stores image data from the top row\nof the image to the bottom, whereas OpenGL expects image data to be stored\nstarting with the bottom row.  If this is a problem for you, you can flip\nthe image before using it to create a texture.  To do that, you have to\nload the image into a ", " and then load that into a\ntexture using the ", " class.\nFor example, assuming ", " is a path to an image resource\nin the program:\n", "The ", " class is defined in\npackage ", ".  Here, I obtained a\n", " by reading it from a\nresource.  You could also read it from a file\u2014or even draw it using\nJava 2D graphics."], "chapter_title": "OpenGL 1.1: Light and Material", "id": 4.3}, {"section_title": "The Programmable Pipeline", "chapter_id": "Chapter 6", "section_id": "Section 6.1", "content": ["OpenGL 1.1 used a ", " for graphics processing.\nData is provided by a program and passes through a series of processing stages that ultimately\nproduce the pixel colors seen in the final image.  The program can enable and disable\nsome of the steps in the process, such as the ", " and ", "\ncalculations.  But there is no way for it to change what happens at each stage.  The functionality\nis fixed.", "OpenGL 2.0 introduced a ", ".  It became possible\nfor the programmer to replace certain stages in the pipeline with their own programs.\nThis gives the programmer complete control over what happens at that stage.  In OpenGL 2.0,\nthe programmability was optional; the complete fixed-function pipeline was still available\nfor programs that didn't need the flexibility of programmability.  WebGL uses a programmable\npipeline, and it is ", ".  There is no way to use WebGL without writing\nprograms to implement part of the graphics processing pipeline.", "The programs that are written as part of the pipeline are called ", ".\nFor WebGL, you need to write a ", ", which is called once for\neach vertex in a ", ", and a \n", ", which is called once for each\npixel in the primitive.  Aside from these two programmable stages,\nthe WebGL pipeline also contains several stages from the original fixed-function pipeline.\nFor example, the depth test is still part of the fixed functionality, and it can be enabled or\ndisabled in WebGL in the same way as in OpenGL\u00a01.1.", "In this section, we will cover the basic structure of a WebGL program and how data\nflows from the JavaScript side of the program into the graphics pipeline and through\nthe vertex and fragment shaders.", "(This book covers WebGL 1.0.  Version 2.0 is under development,\nand will be backward compatible with version\u00a01.0.  Note that later versions of OpenGL have introduced\nadditional programmable stages into the pipeline, in addition to the vertex and fragment shaders,\nbut they are not part of WebGL and are not covered in this book.)", "To use WebGL, you need a WebGL graphics context.  The graphics context is a JavaScript object\nwhose methods implement the JavaScript side of the WebGL API.\nWebGL draws its images in an ", ", the same kind of ", " element\nthat is used for the 2D API that was covered in ", ". A graphics\ncontext is associated with a particular canvas and can be obtained by calling the function\n", "(\"webgl\"), where ", " is a ", " object\nrepresenting the canvas.  Some older browsers might require \"experimental-webgl\" as\nthe parameter to ", ", so the code for creating a WebGL context often looks\nsomething like this:", "Here, ", " is the WebGL graphics context.\nThis code might require some unpacking.  This is JavaScript code that would occur as part of\na script in the source code for a web page.  The first line assumes that the HTML source\nfor the web page includes a canvas element with id=\"webglcanvas\", such as", "In the second line, ", "(\"webgl\") will return ", "\nif the web browser does not support \"webgl\" as a parameter to ", ";\nin that case, the second operand of the ", " operator will be evaluated.\nThis use of ", " is a JavaScript idiom, which uses the fact that\n", " is considered to be ", " when used in a boolean context.\nSo, the second line, where the WebGL context is created, is equivalent to:\n", "It is possible that ", "(\"experimental-webgl\") is also ", ",\nif the browser supports the 2D canvas API but does not support WebGL.  Furthermore, if\nthe browser has no support at all for ", ", the code will throw an exception.\nSo, I will use a function of the following form for initialization of my WebGL programs:\n", "In this function, ", " and ", " are global variables. And \n", "() is a function defined elsewhere in the script \nthat initializes the graphics context, including\ncreating and installing the shader programs.  The ", "() function could\nbe called, for example, by the ", " event handler for the ", "\nelement of the web page:", "Once the graphics context, ", ", has been created, it can be used to call\nfunctions in the WebGL API.  For example, the command for enabling the depth\ntest, which was written as ", "(", ") in OpenGL,\nbecomes", "Note that both functions and constants in the API are referenced throught the\ngraphics context.  The name \"gl\" for the graphics context is conventional, but remember that\nit is just an ordinary JavaScript variable whose name is up to the programmer.", "Drawing with WebGL requires a shader program, which consists of a vertex shader and\na fragment shader.  Shaders are written in the language ", "\u00a0ES\u00a01.0 (the\nOpenGL Shader Language for Embedded Systems, version 1.0).  GLSL is based on the C programming\nlanguage.  The vertex shader and fragment shader are separate programs, each with its own\n", "() function.  The two shaders are compiled separately and then \"linked\" to produce\na complete shader program.  The JavaScript API for WebGL includes functions for compiling\nthe shaders and then linking them.  To use the functions, the source code for the shaders\nmust be JavaScript strings.  Let's see how it works.  It takes three steps to create the\nvertex shader.", "The functions that are used here are part of the WebGL graphics context, ", ",\nand ", " is the string that contains the source code for the\nshader.  Errors in the source code will cause the compilation to fail silently.\nYou need to check for compilation errors by calling the function\n", "which returns a boolean value to indicate whether the compilation succeeded. In\nthe event that an error occurred, you can retrieve an error message with", "which returns a string containing the result of the compilation.  (The exact format\nof the string is not specified by the WebGL standard.  The string is meant to be\nhuman-readable.)", "The fragment shader can be created in the same way.  With both shaders in hand,\nyou can create and link the program.  The shaders need to be \"attached\" to the\nprogram object.  The code takes the form:", "Even if the shaders have been successfully compiled, errors can occur when they\nare linked into a complete program.  For example, the vertex and fragment shader\ncan share certain kinds of variable.  If the two programs declare such variables with the same name but\nwith different types, an error will occur at link time.   Checking for link errors\nis similar to checking for compilation errors in the shaders.", "The code for creating a shader program is always pretty much the same, so it is\nconvenient to pack it into a reusable function.  Here is the function that I use for\nthe examples in this chapter:", "There is one more step:  You have to tell the WebGL context to use the program.\nIf ", " is a program identifier returned by the above function, this is done\nby calling", "It is possible to create several shader programs.  You can then switch from one\nprogram to another at any time by calling ", ", even in the middle of\nrendering an image.  (", ", for example, uses a different program for each\ntype of ", ".)", "It is advisable to create any shader programs that you need as part of initialization.\nAlthough ", " is a fast operation, compiling and linking are rather slow,\nso it's better to avoid creating new programs while in the process of drawing an image.", "Shaders and programs that are no longer needed can be deleted to free up the resources\nthey consume. Use the functions ", "(", ") and\n", "(", ").", "The WebGL graphics pipeline renders an image.  The data that defines the image\ncomes from JavaScript.  As it passes through the pipeline, it is processed by \nthe current vertex shader and fragment shader as well as by the fixed-function\nstages of the pipeline.  You need to understand how data is placed by JavaScript\ninto the pipeline and how the data is processed as it passes through the pipeline.", "The basic operation in WebGL is to draw a ", ".\nWebGL uses just seven of the OpenGL primitives that were introduced in\n", ".  The primitives for drawing ", "\nand ", " have been removed.  The remaining primitives draw\npoints, line segments, and triangles.  In WegGL, the seven types of primitive are identified by\nthe constants ", ", ", ", ", ", ", ",\n", ", ", ", and ", ", where \n", " is a WebGL graphics context.", "When WebGL is used to draw a primitive, there are two general categories of data\nthat can be provided for the primitive.  The two kinds of data are referred to\nas ", " \n(or just \"attributes\") and ", "\n(or just \"uniforms\").  A primitive is defined by its type and by a list of ", ".\nThe difference between attributes and uniforms is that a uniform variable has a single value\nthat is the same for the entire primitive, while the value of an attribute variable can\nbe different for different vertices.", "One attribute that is always specified is the coordinates of the vertex.  \nThe vertex coordinates must be an attribute since each vertex\nin a primitive will have its own set of coordinates.  Another possible attribute is color.\nWe have seen that OpenGL allows you to specify a different color for each vertex of\na primitive.  You can do the same thing in WebGL, and in that case the color will be an\nattribute.  On the other hand, maybe you want the entire primitive to have the same,\n\"uniform\" color; in that case, color can be a uniform variable.  Other quantities\nthat could be either attributes or uniforms, depending on your needs, include\n", " and ", ".\n", ", if they are used,\nare almost certain to be an attribute, since it doesn't really make sense for\nall the vertices in a primitive to have the same texture coordinates.  If a \n", " is to be applied to the primitive, it would naturally\nbe represented as a uniform variable.", "It is important to understand, however, that WebGL does not come with ", "\npredefined attributes, not even one for vertex coordinates.  In the programmable pipeline, the attributes and uniforms\nthat are used are entirely up to the programmer.  As far as WebGL is concerned,\nattributes are just values that are passed into the vertex shader.  Uniforms\ncan be passed into the vertex shader, the fragment shader, or both.  WebGL does not\nassign a meaning to the values.  The meaning is entirely determined by what the shaders\ndo with the values.  The set of attributes and uniforms that are used in drawing a\nprimitive is determined by the source code of the shaders that are in use\nwhen the primitive is drawn.", "To understand this, we need to look at what happens in the pipeline in a little more detail.\nWhen drawing a primitive, the JavaScript program will specify values for any attributes\nand uniforms in the shader program.  For each attribute, it will specify an array of values,\none for each vertex.  For each uniform, it will specify a single value.  The values will\nall be sent to the ", " before the primitive is drawn.  When drawing the primitive,\nthe GPU calls the vertex shader once for each vertex.  The attribute values for the vertex\nthat is to be processed\nare passed as input into the vertex shader.  Values of uniform variables are also passed \nto the vertex shader.  Both attributes and uniforms are represented as\nglobal variables in the shader, whose values are set before the shader is called.", "As one of its outputs, the vertex shader must specify the coordinates of the vertex in \nthe ", " (see ", ").\nIt does that by assigning a value to a special variable named ", ".\nThe position is often computed by applying a transformation to the\nattribute that represents the coordinates in the ", ",\nbut exactly how the position is computed is up to the programmer.", "After the positions of all the vertices in the primitive have been computed,\na fixed-function stage in the pipeline clips away the parts of the primitive whose coordinates\nare outside the range of valid clip coordinates (\u22121 to 1 along each coordinate axis).\nThe primitive is then ", "; that is, it is\ndetermined which pixels lie inside the primitive.  The fragment shader is then called once for\neach pixel that lies in the primitive.  The fragment shader has access to uniform variables\n(but not attributes).  It can also use a special variable named ", "\nthat contains the clip coordinates of the pixel.  Pixel coordinates are computed\nby ", " the values of ", "\nthat were specified by the vertex shader.  The interpolation is done by another fixed-function \nstage that comes between the vertex shader and the fragment shader.\n", "Other quantities besides coordinates can work in much that same way.  That is, the vertex shader\ncomputes a value for the quantity at each vertex of a primitive.  An interpolator takes the values\nat the vertices and computes a value for each pixel in the primitive.  The value for a given\npixel is then input into the fragment shader when the shader is called to process that pixel.\nFor example, color in OpenGL follows this pattern: The color of an interior pixel of a primitive\nis computed by interpolating the color at the vertices.  In GLSL, this\npattern is implemented using ", ".", "A varying variable is declared both in the vertex shader and in the fragment shader.\nThe vertex shader is responsible for assigning a value to the varying variable.\nThe interpolator takes the values from the vertex shader and computes a value for each\npixel.  When the fragment shader is executed for a pixel, the value of the varying variable\nis the interpolated value for that pixel.  The fragment shader can use the value in its\nown computations. (In newer versions of GLSL, the term \"varying variable\" has been\nreplaced by \"out variable\" in the vertex shader and \"in variable\" in the fragment\nshader.)", "Varying variables exist to communicate data from the vertex shader to the fragment shader.\nThey are defined in the shader source code.  They are not used or referred to in the\nJavaScript side of the API.  Note that it is entirely up to the programmer to decide \nwhat varying variables to define and what to do with them.", "We have almost gotten to the end of the pipeline.  After all that, the job of the\nfragment shader is simply to specify a color for the pixel.  It does that by assigning\na value to a special variable named ", ".  That value will then\nbe used in the remaining fixed-function stages of the pipeline.", "To summarize:  The JavaScript side of the program sends values for attributes and\nuniform variables to the GPU and then issues a command to draw a primitive.  The GPU\nexecutes the vertex shader once for each vertex.  The vertex shader can use the values\nof attributes and uniforms. It assigns values to ", " and to any\nvarying variables that exist in the shader.  After clipping, rasterization, and interpolation, the\nGPU executes the fragment shader once for each pixel in the primitive.  The fragment\nshader can use the values of varying variables, uniform variables, and ", ".\nIt computes a value for ", ".  This diagram summarizes the flow\nof data:", "\n", "The diagram is not complete.  There are a few more special variables that I haven't mentioned.  \nAnd there is the\nimportant question of how ", " are used.  \nBut if you understand the diagram, you have a good\nstart on understanding WebGL.", "It's time to start looking at some actual WebGL code. We will concentrate on the\nJavaScript side first, but you need to know a little about GLSL.  GLSL has some\nfamiliar basic data types: ", ", ", ", and\n", ".  But it also has some new predefined data types to represent\n", " and ", ".\nFor example, the data type ", " represents a vector in 3D.  The value of\na ", " variable is a list of three floating-point numbers.  Similarly,\nthere are data types ", " and ", " to represent 2D and 4D vectors.", "Global variable declarations in a vertex shader can be marked as ", ",\n", ", or ", ".  A variable declaration with none of these modifiers\ndefines a variable that is local to the vertex shader.  Global variables in a\nfragment can optionally be modified with ", " or ", ", or they\ncan be declared without a modifier.  A varying variable should\nbe declared in both shaders, with the same name and type.  This allows the GLSL\ncompiler to determine what attribute, uniform, and varying variables are used in \na shader program.", "The JavaScript side of the program needs a way to refer to particular attributes\nand uniform variables.  The function ", "\ncan be used to get a reference to a uniform variable in a shader program, where ", "\nrefers to the WebGL graphics context.  It takes\nas parameters the identifier for the compiled program, which was returned by ", ",\nand the name of the uniform variable in the shader source code.  For example,\nif ", " identifies a shader program that has a uniform variable named ", ", then\nthe location of the ", " variable can be obtained with the JavaScript statement\n", "The location ", " can then be used to set the value of the\nuniform variable:", "The function ", " is one of a family of functions that can be\nreferred to as a group as ", ".  This is similar to the family ", " in\nOpenGL\u00a01.1.  The ", " represents a suffix that tells the number and type of\nvalues that are provided for the variable.  In this case, ", " takes\nthree floating point values, and it is appropriate for setting the value of a\nuniform variable of type ", ".  The number of values can be 1, 2, 3, or 4.\nThe type can be \"f\" for floating point or \"i\" for integer.  (For a boolean uniform,\nyou should use ", " and pass 0 to represent ", " or \n1 to represent ", ".)  If a \"v\" is added to the suffix, then the values\nare passed in an array.  For example,", "There is another family of functions for setting the value of uniform matrix\nvariables.  We will get to that later.", "The value of a uniform variable can be set any time after the shader\nprogram has been compiled, and the value remains in effect until it is changed\nby another call to ", ".", "Turning now to attributes, the situation is more complicated, because an attribute can take a\ndifferent value for each vertex in a primitive.  The basic idea is that the complete set of data \nfor the attribute is copied in a single operation from a JavaScript array into memory that is \naccessible to the GPU.  Unfortunately, setting things up to make that operation possible is non-trivial.", "First of all, a regular JavaScript array is not suitable for this purpose.  For efficiency, we need\nthe data to be in a block of memory holding numerical values in successive memory locations, and\nregular JavaScript arrays don't have that form.  To fix this problem, a new kind of array,\ncalled ", ", was introduced into JavaScript.  We encountered\ntyped arrays briefly in ", ".\nA typed array can only hold numbers of a specified type.  There are different kinds of typed array for different\nkinds of numerical data.  For now we will use  ", ",\nwhich holds 32-bit floating point numbers.", "A typed array has a fixed length, which is assigned when it is created by a constructor.  The constructor\ntakes two forms:  One form takes an integer parameter giving the number of elements in the array; the\nother takes a regular Java array of numbers as parameter and initializes the typed array to\nhave the same length and elements as the array parameter.  For example:", "Once you have a typed array, you can use it much like a regular array.  The length\nof the typed array ", " is ", ", and its elements are referred to as\n", ", ", ", ", ", and so on.  \nWhen you assign a value to an element of a ", ", the value is\nconverted into a 32-bit floating point number.  If the value cannot be interpreted as a number,\nit will be converted to ", ", the \"not-a-number\" value.", "Before data can be transferred from JavaScript into an attribute variable, it must be placed\ninto a typed array.  When possible, you should work with typed arrays directly, rather than working\nwith regular JavaScript arrays and then copying the data into typed arrays.", "For use in WebGL, the attribute data must be transferred into a ", " (vertex buffer \nobject).  VBOs were introduced in OpenGL\u00a01.5 and were discussed briefly in ", ".\nA\u00a0VBO is a block of memory that is accessible to the GPU.  To use a VBO, you must first call\nthe function ", "() to create it.  For example,", "Before transferring data into the VBO, you must \"bind\" the VBO:", "The first parameter to ", " is called the \"target\".  It specifies how the VBO will\nbe used. The target ", " is used when the buffer is being used to store values\nfor an attribute.   Only one VBO at a time can be bound to a given target.", "The function that transfers data into\na VBO doesn't mention the VBO\u2014instead, it uses the VBO that is currently bound.\nTo copy data into that buffer, use ", "(). For example:", "The first parameter is, again, the target.  The data is transferred into the VBO that\nis bound to that target.  The second parameter is the typed array that holds the data on the JavaScript side.\nAll the elements of the array are copied into the buffer, and the size of the array determines\nthe size of the buffer.  Note that this is a straightforward transfer of raw data bytes; WebGL\ndoes not remember whether the data represents floats or ints or some other kind of data.", "The third parameter to ", " is one of the constants\n", ", ", ", or ", ".  It is\na hint to WebGL about how the data will be used, and it helps WebGL to manage the\ndata in the most efficient way.  The value ", " means that you intend to\nuse the data many times without changing it.  For example, if you will use the same\ndata throughout the program, you can load it into a buffer once, during initialization,\nusing ", ".  WebGL will probably store the data on the graphics card\nitself where it can be accessed most quickly by the graphics hardware.  The second value,\n", ", is for data that will be used only once, then discarded.  (It can be\n\"streamed\" to the card when it is needed.)  The value ", " is somewhere\nbetween the other two values; it might be used for data that will be used a couple of times\nand then discarded.", "Getting attribute data into VBOs is only part of the story.  You also have to tell WebGL to\nuse the VBO as the source of values for the attribute.  To do so, first of all, you need\nto know the location of the attribute in the shader program.  You can determine that\nusing ", ".  For example,", "This assumes that ", " is the shader program and \"a_color\" is the name of the\nattribute variable in the vertex shader.  This is entirely analogous to ", ".", "Although an attribute usually takes different values at different vertices, it is possible to\nuse the same value at every vertex.  In fact, that is the default behavior.  The single attribute value\nfor all vertices can be\nset using the family of functions ", ", which work similarly to\n", ".  In the more usual case, where you want to take the values of an attribute from a VBO,\nyou must enable the use of a VBO for that attribute.  This is done by calling", "where the parameter is the location of the attribute in the shader program, as returned\nby a call to ", "().   This command has nothing\nto do with any particular VBO.  It just turns on the use of buffers for the specified attribute.\nGenerally, it is reasonable to call this method just once, during initialization.", "Finally, before you draw a primitive that uses the attribute data, you have to tell WebGL which buffer \ncontains the data and how the bits in that buffer are to be interpreted.  This is done with \n", "().  A VBO must be bound to the ", " target when\nthis function is called.  For example,", "Assuming that ", " refers to the VBO and ", " is\nthe location of the attribute, this tells WebGL to take values for\nthe attribute from that buffer.  Often, you will call ", "() just before calling\n", "(), but that is not necessary if the desired buffer is already bound. ", "The first parameter to ", " is the attribute location.  \nThe second is the number of values per vertex.  For example, if you are providing values \nfor a ", ", the second parameter will be 2 and\nyou will provide two numbers per vertex; for a ", ", the second parameter would be 3;\nfor a ", ", it would be\u00a01.  The third parameter specifies the type of each value.\nHere, ", " indicates that each value is a 32-bit floating point number.  Other values\ninclude ", ", ", ", ", ",\nand ", " for integer values.  Note\nthat the type of data does not have to match the type of the attribute variable;\nin fact, attribute variables are always floating point.  However, the parameter value does\nhave to match the data type in the buffer.  If the data came from a ", ",\nthen the parameter must be ", ".\nI will always use ", ", 0, and 0 for the remaining three\nparameters.  They add flexibility that I won't need; you can look them up in the documentation\nif you are interested.", "There is a lot to take in here.  Using a VBO to provide values for an attribute requires\nsix separate commands, and that is in addition to generating the data and placing it in\na typed array.  Here is the full set of commands:", "However, the six commands will not usually occur at the same point in the JavaScript\ncode.  the first three commands are often done as part of initialization.  \n", " would be called whenever the data for the attribute needs to be changed.\n", " must be called before ", "\nor ", ", since it establishes the VBO that is used by those\ntwo commands.  Remember that all of this must be done for every attribute that is\nused in the shader program.", "After the shader program has been created and values have been set up for the uniform\nvariables and attributes, it takes just one more command to draw a primitive:", "The first parameter is one of the seven constants that identify WebGL primitive types,\nsuch as ", ", ", ", and ", ".  The second\nand third parameters are integers that determine which subset of available vertices is used for the primitive.\nBefore calling ", ", you will have placed attribute values for some number of vertices\ninto one or more VBOs.  When the primitive is rendered, the attribute values are pulled from the\nVBOs.  The ", " is the starting vertex number of the data within the VBOs,\nand ", " is the number of vertices in the primitive.  Often, ", "\nis zero, and ", " is the total number of vertices for which data is available.\nFor example, the command for drawing a single triangle might be", "The use of the word \"array\" in ", " and ", " might be a little\nconfusing, since the data is stored in vertex buffer objects rather than in JavaScript arrays.\nWhen ", " was first introduced in OpenGL\u00a01.1, it used ordinary arrays rather\nthan VBOs.  Starting with OpenGL 1.5, ", " could be used either with ordinary arrays\nor VBOs.  In WebGL, support for ordinary arrays was dropped, and ", " can only\nwork with VBOs, even though the name still refers to arrays.", "We encountered the original version of ", " in ", ".\nThat section also introduced an alternative function for drawing primitives, ", ",\nwhich can be used for drawing indexed face sets.  A ", " function is also available\nin WebGL.  With ", ", attribute data is not used in the order in which it occurs in \nthe VBOs.  Instead, there is a separate list of indices that determines the order in which\nthe data is accessed.", "To use ", ", an extra VBO is required to hold the list of indices.\nWhen used for this purpose, the VBO must be bound to the target ", "\nrather than ", ".   The VBO will hold integer values, which can be of type \n", " or ", ".  The values can be loaded from\na JavaScript typed array of type ", " or ", ".\nCreating the VBO and filling it with data is again a multi-step process.  For example,\n", "Assuming that the attribute data has also been loaded into VBOs, ", " can then be used\nto draw the primitive.  A call to ", " takes the form", "The first parameter is a primitive type such as ", ".  The ", "\nis the number of vertices in the primitive.  The ", " specifies the type\nof data that was loaded into the VBO; it will be either ", "\nor ", ".  The ", " is the starting point in the VBO of\nthe data for the primitive; it is usually zero.  (Note that the starting point is given\nin terms of bytes, not vertex numbers.) A typical example would be", "We will have occasion to use this function later.  If you find it confusing, you should\nreview ", ".  The situation is much the same in WebGL as it was\nin OpenGL\u00a01.1."], "chapter_title": "Introduction to WebGL", "id": 6.1}, {"section_title": "Introduction to Lighting", "chapter_id": "Chapter 4", "section_id": "Section 4.1", "content": ["\n", " is one of the most important considerations for\nrealistic 3D graphics.  The goal is to simulate light sources\nand the way that the light that they emit interacts with\nobjects in the scene.  Lighting calculations are disabled by\ndefault in OpenGL.  This means that when OpenGL applies color to \na vertex, it simply uses the current color value as set by the\none of the functions ", ".  In order to get OpenGL\nto do lighting calculations, you need to enable lighting by\ncalling ", "(", ").  If that's all you\ndo, you will find that your objects are all completely black.\nIf you want to see them, you have to turn on some lights.", "The properties of a surface that determine how it interacts \nlight are referred to as the ", " of the surface.\nA surface can have several different material properties.\nBefore we study the OpenGL ", " for\nlight and material, there are a few general ideas about\nlight and material properties that you need to understand.  Those ideas are\nintroduced in this section.  We postpone discussion of how lighting is actually done in \nOpenGL 1.1 until the ", ".", "When light strikes a surface, some of it will be reflected.  Exactly\nhow it reflects depends in a complicated way on the nature of the\nsurface, what I am calling the material properties of the surface.\nIn OpenGL (and in many other computer graphics systems), the complexity\nis approximated by two general types of reflection, \n", " and ", ".", "\n", "In perfect specular (\"mirror-like\") reflection, an incoming ray of light is reflected\nfrom the surface intact.  The reflected ray makes the same angle with the\nsurface as the incoming ray.  A viewer can see the reflected ray only\nif the viewer is in exactly the right position, somewhere along the path of the\nreflected ray.  Even if the entire surface is illuminated by the light\nsource, the viewer will only see the reflection of the light source at\nthose points on the surface where the geometry is right.  Such reflections\nare referred to as ", ".  In practice,\nwe think of a ray of light as being reflected not as a single perfect\nray, but as a cone of light, which can be more or less narrow.", "\n", "Specular reflection from a very shiny surface produces very narrow cones of\nreflected light; specular highlights on such a material are small and sharp.\nA duller surface will produce wider cones of reflected light and bigger,\nfuzzier specular highlights.  In OpenGL, the material property that\ndetermines the size and sharpness of specular highlights is called\n", ".  Shininess in OpenGL is a number in the range\n0 to 128.  As the number increases, specular highlights get smaller.\nThis image shows eight spheres that differ only in the value of the\nshininess material property:", "\n", "For the sphere on the left, the shininess is 0, which leads to an ugly specular \"highlight\"\nthat almost covers an entire hemisphere.  Going from left to right, the shininess increases by\n16 from one sphere to the next.", "In pure diffuse reflection, an incoming ray of light is scattered in\nall directions equally.  A viewer would see reflected light from\nall points on the surface.  If the incoming light arrives in parallel rays\nthat evenly illuminate the surface, then the surface would appear to the viewer\nto be evenly illuminated.  (If different rays strike the surface at different angles, as they\nwould if they come from a nearby lamp or if the surface is curved, then the amount \nof illumination at a point depends on the angle at which the ray hits the surface\nat that point.)", "When light strikes a surface, some wavelengths of light can be\nabsorbed, some can be reflected diffusely, and some can be reflected\nspecularly.  The degree to which a material reflects light of different\nwavelengths is what constitutes the color of the material.  We now see\nthat a material can have two different colors\u2014a ", " that tells \nhow the material reflects light diffusely, and a ", " that tells how it reflects\nlight specularly.  The diffuse color is the basic color of the\nobject.  The specular color determines the color of specular highlights.\nThe diffuse and specular colors can be the same; for example, this is often true\nfor metallic surfaces.  Or they can be different; for example, a plastic surface\nwill often have white specular highlights no matter what the diffuse color.", "here is a demo that\nlets you experiment with the material properties that we have discussed so far.  Read the\nhelp text in the demo for more information.", "\n", "\n", "OpenGL goes even further.  In fact, there are two more colors\nassociated with a material.  The third color is the\n", " of the material, which tells how the surface\nreflects ", ".  Ambient light refers\nto a general level of illumination that does not come directly from\na light source.  It consists of light that has been reflected and\nre-reflected so many times that it is no longer coming from any \nparticular direction.  Ambient light is why shadows are not absolutely\nblack.  In fact, ambient light is only a crude approximation for the\nreality of multiply reflected light, but it is better than ignoring\nmultiple reflections entirely.  The ambient color of a material \ndetermines how it will reflect various wavelengths of ambient light.\nAmbient color is generally set to be the same as the diffuse color.", "The fourth color associated with a material is an ", ",\nwhich is not really a color in the same sense as the first three color properties.\nThat is, it has nothing to do with how the surface reflects light.\nThe emission color is color that does not come\nfrom any external source, and therefore seems to be emitted by the\nmaterial itself.  This does not mean that the object is giving off\nlight that will illuminate other objects, but it does mean that\nthe object can be seen even if there is no source of light (not\neven ambient light).  In the presence of light, the object will\nbe brighter than can be accounted for by the light that illuminates\nit, and in that sense it appears to glow.  The emission color\nis usually black; that is, the object has no emission at all.", "Each of the four material color properties is specified in terms of three numbers\ngiving the ", " (red, green, and blue) components of the color.\nReal light can contain an infinite number of different wavelengths.\nAn RGB color is made up of just three components, but the nature\nof human color vision makes this a pretty good approximation for most purposes.\n(See ", ".)  Material colors can also have\n", ", but the only alpha\ncomponent that is ever used in OpenGL is the one for the diffuse material color.", "In the case of the red, blue, and green components of the\nambient, diffuse, or specular color,  the term \"color\" really\nmeans reflectivity.  That is, the red component of a color gives the\nproportion of red light hitting the surface that is reflected by that\nsurface, and similarly for green and blue.   There are three different\ntypes of reflective color because there are three different types\nof light in OpenGL, and a material can have a different \nreflectivity for each type of light.", "Leaving aside ambient light, the light in an environment comes from a light\nsource such as a lamp or the sun.  In fact, a lamp and the sun are examples of\ntwo essentially different kinds of light source: a ", "\nand a ", ".  A point light source is located at\na point in 3D space, and it emits light in all directions from that point.\nFor a directional light, all the light comes from the same direction, so that\nthe rays of light are parallel.  The sun is considered to be a directional\nlight source since it is so far away that light rays from the sun\nare essentially parallel when they get to the Earth .", "\n", "A light can have color.  In fact, in OpenGL, each light source has three colors: an\nambient color, a diffuse color, and a specular color.\nJust as the color of a material is more properly referred\nto as reflectivity, color of a light is more properly referred to as \n", "\nor energy.  More exactly, color refers to how the light's energy is\ndistributed among different wavelengths.  Real light can\ncontain an infinite number of different wavelengths; when the wavelengths are\nseparated, you get a spectrum or rainbow containing a continuum of colors.\nLight as it is usually modeled on a computer contains only the three basic colors,\nred, green, and blue.  So, just like material color, light color is specified by \ngiving three numbers representing the red, green, and blue intensities of the\nlight.", "The diffuse intensity of a light is the aspect of the light that interacts with\ndiffuse material color, and the specular intensity of a light is what interacts\nwith specular material color.  It is common for the diffuse and specular light intensities\nto be the same.", "The ambient intensity of a light works a little differently.  Recall that\nambient light is light that is not directly traceable to any light source.\nStill, it has to come from somewhere and we can imagine that turning on\na light should increase the general level of ambient light in the environment.\nThe ambient intensity of a light in OpenGL is added to the general level of\nambient light.  (There can also be global ambient light, which is not associated\nwith any of the light sources in the scene.)  Ambient light interacts with the ambient color of\na material, and this interaction has no dependence on the position of the\nlight sources or viewer.  So, a light doesn't have to shine on an object for the object's\nambient color to be affected by the light source; the light source just has\nto be turned on. ", "I should emphasize again that this is all just an approximation, and in this\ncase not one that has a basis in the physics of the real world.  Real light sources\ndo not have separate ambient, diffuse, and specular colors, and some computer\ngraphics systems model light sources using just one color.", "The visual effect of a light shining on a surface depends on the properties of the surface\nand of the light.  But it also depends to a great extent on the angle at which the light\nstrikes the surface.  The angle is essential to specular reflection and also affects diffuse reflection.\nThat's why a curved, lit surface looks different at different points,\neven if its surface is a uniform color.  To calculate this angle, OpenGL needs to know the direction in\nwhich the surface is facing.  That direction is specified by a ", "\nthat is perpendicular to the surface.\nAnother word for \"perpendicular\" is \"normal,\" and a non-zero vector that is perpendicular to a surface\nat a given point is called a ", " to that surface.  When used in lighting\ncalculations, a normal vector must have length equal to one.  A normal vector of length one\nis called a ", ".  For proper lighting calculations in OpenGL,\na unit normal must be specified for each vertex.  However, given any normal vector,\nit is possible to calculate a unit normal from it by dividing the vector by its length.\n(See ", " for a discussion of vectors and their lengths.)", "Since a surface can be curved, it can face different directions at different points.\nSo, a normal vector is associated with a particular point on a surface.  In OpenGL,\nnormal vectors are actually assigned only to the vertices of a \n", ". The\nnormal vectors at the vertices of a primitive are used to do lighting calculations\nfor the entire primitive.", "Note in particular that you can assign different normal vectors at each vertex of\na polygon.\nNow, you might be asking yourself, \"Don't all the normal vectors to a polygon point in\nthe same direction?\" After all, a polygon is flat; the perpendicular direction to the polygon\ndoesn't change from point to point.  This is true, and if your objective is to display a\n", " object whose sides are flat polygons, \nthen in fact, all the normals of each of\nthose polygons should point in the same direction.  On the other hand, polyhedra are often used\nto approximate curved surfaces such as spheres.  If your real objective\nis to make something that looks like a curved surface, then you want to use normal vectors\nthat are perpendicular to the actual surface, not to the polyhedron that approximates it.\nTake a look at this example:", "\n", "The two objects in this picture are made up of bands of rectangles.  The two objects have\nexactly the same geometry, yet they look quite different.  This is because different\nnormal vectors are used in each case.  For the top object, the band of rectangles is supposed\nto approximate a smooth surface.  The vertices of the rectangles\nare points on that surface, and I really didn't want to see the rectangles at all\u2014I wanted\nto see the curved surface, or at least a good approximation.  So for the top object, \nwhen I specified the normal vector at each of the vertices, I used\na vector that is perpendicular to the surface rather than one perpendicular to the rectangle.\nFor the object on the bottom, on the other hand,\nI was thinking of an object that really ", " a band of rectangles, and I used normal vectors\nthat were actually perpendicular to the rectangles.  Here's a two-dimensional illustration that shows the\nnormal vectors that were used for the two pictures:", "\n", "The thick blue lines represent the rectangles, as seen edge-on from above.\nThe arrows represent the normal vectors.  Each rectangle has two normals, one at each endpoint.\n", "In the bottom half of the illustration, two rectangles that meet at a point have different\nnormal vectors at that point.  The normal vectors for a rectangle are actually perpendicular to the rectangle.\nThere is an abrupt change in direction as you move from one rectangle to the next, so where one rectangle\nmeets the next, the normal vectors to the two rectangles are different.  The visual effect on\nthe rendered image is an abrupt change in shading that is perceived as a corner or edge between the\ntwo rectangles.", "In the top half, on the other hand, the vectors are perpendicular to a curved surface that \npasses through the endpoints of the rectangles.  When two rectangles share a vertex, \nthey also share the same normal at that vertex.  Visually, this eliminates the abrupt \nchange in shading, resulting in something that looks more like a smoothly curving surface.", "The two ways of assigning normal\nvectors are called \"flat shading\" and \"smooth shading\".  Flat shading makes a surface look\nlike it is made of flat sides or facets.  Smooth shading makes it look more like a smooth\nsurface.  This demo will help you to understand these concepts.\nIt shows a ", " being used to approximate a sphere, with your choice of\nsmooth or flat shading.  Use the sliders to control the number of polygons in the mesh.", "\n", "\n", "The upshot of all this is that you get to make up whatever normal vectors suit your purpose.\nA normal vector at a vertex is whatever you say it is,\nand it does not have to be literally perpendicular to the polygon.  The normal vector that you choose\nshould depend on the object that you are trying to model.", "There is one other issue in choosing normal vectors:  There are always two possible unit normal vectors\nat a point on a surface, pointing in opposite directions.  A polygon in 3D has two faces, facing in opposite directions.\nOpenGL considers one of these to be the ", "\nand the other to be the ", ".  OpenGL tells them apart by the\norder in which the vertices are specified.  (See ", ".)\nThe default rule is that the order of the vertices is\ncounterclockwise when looking at the front face and is clockwise when looking at the back face.\nWhen the polygon is drawn on the screen, this rule lets OpenGL tell whether it is the front\nface or the back face that is being shown.  When specifying a normal vector for the polygon,\nthe vector should point out of the front face of the polygon.  This is another example of\nthe right-hand rule.  If you curl the fingers of your right hand in the direction in which the\nvertices of the polygon were specified, then the normal vector should point in the direction of\nyour thumb.  Note that when you are looking at the front\nface of a polygon, the normal vector should be pointing towards you.  If you are looking at the back\nface, the normal vector should be pointing away from you.\n", "It can be a difficult problem to come up with the correct normal vectors for an object.\nComplex geometric models often come with the necessary normal vectors included.  This is true,\nfor example, for the solid shapes drawn by the ", " library.", "What does it actually mean to say that OpenGL performs \"lighting\ncalculations\"?  The goal of the calculation is to produce a color,\n(", "), for a point on a surface.  In OpenGL\u00a01.1, lighting calculations\nare actually done only at the vertices of a primitive.  After the color of each\nvertex has been computed, colors for interior points of the primitive are obtained\nby interpolating the vertex colors.", "The alpha component of the vertex color, ", ", is easy: It's simply the alpha component of\nthe diffuse material color at that vertex.  The calculation of\n", ", ", ", and ", " is fairly complex and rather mathematical, and you don't necessarily\nneed to understand it.  But here is a short description of how it's done...", "Ignoring alpha components, let's assume that the ambient, diffuse,\nspecular, and emission colors of the material have RGB components\n(", "), (", "), \n(", "),  and (", "),\nrespectively.  Suppose that the global ambient intensity, which represents ambient light that\nis not associated with any light source in the environment, is (", ").\nThere can be several point and directional light sources, \nwhich we refer to as light number 0, light number 1, light number 2, and so on.\nWith this setup, the red component of the vertex color will be:", "where ", " is the contribution to the color that comes from light number 0;\n", " is the contribution from light number 1; and so on.\nA similar equation holds for the green and blue components of the color.\nThis equation says that the emission color, ", ", is simply added to any other contributions\nto the color.  And the contribution of global ambient light is obtained by multiplying\nthe global ambient intensity, ", ", by the material ambient color, ", ".\nThis is the mathematical\nway of saying that the material ambient color is the fraction of the ambient light\nthat is reflected by the surface.", "The terms I", ",  I", ", and so on, represent the contribution to the final color from\nthe various light sources in the environment.\nThe contributions from the light sources are complicated.  Consider just one of the\nlight sources.\nNote, first of all, that if a light source is disabled (that is, if it is turned off), then the contribution\nfrom that light source is zero.  For an enabled light source, we have to look at\nthe geometry as well as the colors:", "\n", "In this illustration, ", " is the normal vector at the point whose color\nwe want to compute.  ", "\u00a0is a vector that points back along the direction from which the light\narrives at the surface.\n", "\u00a0is a vector that points in the direction of the viewer. And \n", " is the direction of the reflected ray, that is, the direction in which a light\nray from the source would be reflected specularly when it strikes the surface at the point in\nquestion.  The angle between ", " and ", " is the same as the angle\nbetween ", " and ", "; this is a basic fact about the physics of light.\nAll of the vectors are unit vectors, with length 1.\nRecall that for unit vectors ", " and ", ", the inner product\n", "\u00a0\u00b7\u00a0", " is equal to the cosine of the angle between the\ntwo vectors.  Inner products occur at several points in the lighting equation,\nas the way of accounting for the angles between various vectors.", "Now, let's say that the light has ambient, diffuse, and specular color components\n(la", ",la", ",la", "), (ld", ",ld", ",ld", "), \nand (ls", ",ls", ",ls", ").  Also, let ", " be the value of the\nshininess property of the material.  \nThen, assuming that the light is enabled, the contribution of this\nlight source to the red component of the vertex color can be computed as\n", "with similar equations for the green and blue components.  The first term,\n", " accounts for the contribution of the ambient light\nfrom this light source to the color of the surface.  This term is added to the color\nwhether or not the surface is facing the light.", "The value of ", " is 0 if the surface is facing away from the \nlight and is 1 if the surface faces the light; that is,\nit accounts for the fact that the light only illuminates one side of the surface.\nTo test whether ", " is 0 or 1, we can check whether ", "\u00b7", " \nis less than 0.  This dot product is the cosine of the angle between ", " and ", ";\nit is less than 0 when the angle is greater than 90 degrees, which would mean that the\nnormal vector is on the opposite side of the surface from the light.\nWhen ", " is zero, there is no diffuse or specular contribution from the \nlight to the color of the vertex.", "The diffuse component of the color, before adjustment by ", ",\nis given by ", ".  This represents the\ndiffuse intensity of the light times the diffuse reflectivity of the material, multiplied\nby the cosine of the angle between ", " and ", ".  The angle is involved because for\na larger angle, the same amount of energy from the light is spread out over a greater area:", "\n", "As the angle increases from 0 to 90 degrees, the cosine of the angle decreases from 1 to 0,\nso the larger the angle, the smaller the value of ", " and\nthe smaller the contribution of diffuse illumination to the color.", "For the specular component, recall that a light ray is reflected specularly as a cone\nof light.  The reflection vector, ", ", is at the center of the cone.  The closer the\nviewer is to the center of the cone, the more intense the specular reflection. The distance\nof the viewer from the center of the cone depends on the angle between ", " and ", ",\nwhich appears in the equation as the dot product ", ".  Mathematically,\nthe specular contribution to the color is given by\n", ".\nTaking the maximum of 0 and ", " ensures that the specular contribution is\nzero if the angle between ", " and ", " is greater than 90 degrees.  Assuming that\nis not the case, ", " is equal to the ", ".\nNote that this dot product is raised to the exponent ", ", which is\nthe material's shininess property.\nWhen ", " is 0, ", " is 1, and there is no dependence on the angle; \nin that case, the result is the sort of huge and undesirable specular highlight\nthat we have seen for shininess equal to zero.  For positive values of shininess,\nthe specular contribution is maximal when the angle between ", " \nand ", " is zero, and it decreases as the\nangle increases.  The larger the shininess value, the faster the rate of decrease.  The\nresult is that larger shininess values give smaller, sharper specular highlights.\n", "Remember that the same calculation is repeated for every enabled light\nand that the results are combined to give the final vertex color.  It's\neasy, especially when using several lights, to end up with color components larger\nthan one.  In the end, before the color is used to color a pixel on the screen,\nthe color components must be clamped to the range zero to one.  Values greater than\none are replaced by one.  This makes it easy to produce ugly\npictures in which large areas are a uniform white because all the color values\nin those areas exceeded one.  All the information that was supposed to be conveyed\nby the lighting has been lost.  The effect is similar to an over-exposed photograph.\nIt can take some work to find appropriate lighting levels to avoid this kind of\nover-exposure.", "(My discussion of lighting in this section leaves out some factors.  \nThe equation as presented doesn't take into account\nthe fact that the effect of a point light can depend on the distance to the light,\nand it doesn't take into account spotlights, which emit just a cone of light.  Both of these\ncan configured in OpenGL, but I won't discuss them in this book.  There are also many\naspects of light that are not captured by the simple model used in OpenGL.  One of the\nmost obvious omissions is shadows: Objects don't block light! Light shines right through them.\nWe will encounter some extensions to the model in later chapters when we discuss other graphics\nsystems.)"], "chapter_title": "OpenGL 1.1: Light and Material", "id": 4.1}, {"section_title": "GLSL", "chapter_id": "Chapter 6", "section_id": "Section 6.3", "content": ["You have seen a few short, simple examples of ", " programs\nwritten in ", ".  In fact, shader programs are often fairly short,\nbut they are not always so simple.  To understand the more complex shaders\nthat we will be using in the rest of this book, you will need to know more\nabout GLSL.  This section aims to give a short, but reasonably complete,\nintroduction to the major features of the language.  This is a rather\ntechnical section.  You should read it to get some familiarity with GLSL,\nand then use it as a reference when needed.", "The version of GLSL for WebGL 1.0 is GLSL ES 1.0.  However, the specification for GLSL ES 1.0\nlists a number of language features as being optional.  The WebGL specification\nmandates that the optional features in GLSL ES 1.0 are ", " supported in WebGL.\nThese unsupported features include some that you would probably consider\npretty basic, such as ", " loops and certain kinds of array indexing.\nThe justification for having optional features in GLSL\u00a0ES is that GPUs vary\nin the set of features that can be efficiently implemented, and GPUs for embedded\nsystems can be especially limited.  The justification for eliminating those optional\nfeatures in WebGL is presumably that WebGL programs are used on Web pages that can\nbe accessed by any device, so they should work on the full range of devices.", "Variables in GLSL must be declared before they are used.  GLSL is a strictly\ntyped language, and every variable is given a type when it is declared.", "GLSL has built-in types to represent scalars (that is, single values), vectors, and\nmatrices.  The scalar types are ", ", ", ", and ", ".\nA ", " might not support integers or booleans on the hardware level, so\nit is possible that the ", " and ", " types are actually\nrepresented as floating point values.", "The types ", ", ", ", and ", " represent vectors of two,\nthree, and four ", ".  There are also types to represent vectors\nof ", " (", ", ", ", and ", ") and\n", " (", ", ", ", and ", ").\nGLSL has very flexible notation for referring to the components of a vector.\nOne way to access them is with array notation.  For example, if ", " is a four-component\nvector, then its components can be accessed as ", "[0], ", "[1], ", "[2], and\n", "[3].  But they can also be accessed using the dot notation as\n", ", ", ", ", ", and", ".  The component names ", ",\n", ", ", ", and ", " are appropriate for a vector that holds coordinates.\nHowever, vectors can also be used to represent colors, and the components of ", "\ncan alternatively be referred to as ", ", ", ", ", ", and ", ".\nFinally, they can be referred to as ", ", ", ", ", ", and ", " \u2014\nnames appropriate for texture coordinates.", "Furthermore, GLSL allows you to use multiple component names after the dot, as in\n", " or ", " or even ", ".  The names can be in any order, and \nrepetition is allowed.  This is called ", ",\nand ", " is an example of a swizzler.  The notation ", " can be used in \nan expression as a two-component vector.  For example, if ", " is\n", "(1.0,2.0,3.0,4.0), then ", " is equivalent to ", "(3.0,1.0),\nand ", " is like ", "(2.0,2.0,2.0).  Swizzlers can even be used on\nthe left-hand side of an assignment, as long as they don't contain repeated components.\nFor example,", "A notation such as ", "(1.0,\u00a02.0) is referred to as a \"constructor,\" although\nit is not a constructor in the sense of Java or C++, since GLSL is not object-oriented,\nand there is no ", " operator.  A constructor in GLSL consists of a type name\nfollowed by a list of expressions in parentheses, and it represents a value of the type\nspecified by the type name.  Any type name can be used, including the scalar types.\nThe value is constructed from the values of the expressions in parentheses.  An expression\ncan contribute more than one value to the constructed value;  we have already seen this\nin examples such as", "Note that the expressions can be swizzlers:", "Extra values from the last parameter will be dropped.  This makes is possible to\nuse a constructor to shorten a vector.  However, it is not legal to have extra \nparameters that contribute no values at all to the result:", "As a special case, when a vector is constructed from a single scalar value,\nall components of the vector will be set equal to that value:", "When constructing one of the built-in types, type conversion will be applied\nif necessary.  For purposes of conversion, the boolean values ", " convert\nto the numeric values zero and one; in the other direction, zero converts to ", "\nand any other numeric value converts to ", ".  As far as I know, constructors\nare the ", " context in which GLSL does automatic type conversion.\nFor example, you need to use a constructor to assign an ", " value\nto a ", " variable, and it is illegal to add an ", "\nto a ", ":", "The built-in matrix types are ", ", ", ", and ", ".\nThey represent, respectively, two-by-two, three-by-three, and four-by-four matrices \nof floating point numbers.  The elements of a matrix can be \naccessed using array notation, such as ", "[2][1]. If a single index is used, as\nin ", "[2], the result is a vector.  For example, if ", " is of type\n", ", then ", "[2] is a ", ".  Arrays in GLSL, as in OpenGL,\nuse ", ".  This means that ", "[2] is column\nnumber 2 in ", " rather than row number 2 (as it would be in Java), and ", "[2][1] is\nthe element in column 2 and row\u00a01.", "A matrix can be constructed from the appropriate number of values, which can\nbe provided as scalars, vectors or matrices.  For example, a ", " can be\nconstructed from nine ", " or from three ", " parameters:", "Keep in mind that the matrix is filled in column-major order; that is, the\nfirst three numbers go into column\u00a00, the next three into column\u00a01, and the\nlast three into column\u00a02.", "As a special case, if a matrix ", " is constructed from a single scalar value, then\nthat value is put into all the diagonal elements of ", " (", "[0][0],\n", "[1][1], and so on).  The non-diagonal elements are all set equal to zero. \nFor example, ", "(1.0) constructs the four-by-four ", ".", "The only other built-in types are ", " and ", ", which\nare used for accessing textures. The sampler types can be used only in limited ways.\nThey are not numeric types and cannot be converted to or from numeric types.  The will\nbe covered in the ", ".", "A GLSL program can define new types using the ", " keyword.  The syntax is the\nsame as in C, with some limitations.  A struct is made up of a sequence\nof named members, which can be of different types.  The type of a member can be\nany of the built-in types, an array type, or a previously defined struct type.  For example,", "This defines a type named ", ".  The type can be used to declare variables:", "The members of the variable ", " are then referred to as ", ",\n", ", and ", ".  Struct types have constructors, but their\nconstructors do not support type conversion:  The constructor must contain a list of\nvalues whose types exactly match the types of the corresponding members in the struct.\nFor example,", "GLSL also supports arrays.  Only one-dimensional arrays are allowed.  The base type\nof an array can be any of the basic types or it can be a struct type.  The size of\nthe array must be specified in the variable declaration as an integer constant.  For \nexample", "There are no array constructors, and it is not possible to initialize an array as\npart of its declaration.", "Array indexing uses the usual syntax, such as ", "[0]\nor ", "[", "+1] or ", "[3].", ".\nHowever, there are some strong limitations on the expressions that can be used as array \nindices.  With one exception, an expression that is used as the index for an array\ncan contain only integer constants and ", " loop variables (that is, variables\nthat are used as loop control variables in ", " loops).  For example,\nthe expression ", "[", "+1] would only be legal inside a ", " of the form\n", ".  The single exception is that\narbitrary index expressions can be used for arrays of ", " in a\nvertex shader (and then only if the array does not contain samplers).", "Just as in C, there is no check for array index out of bounds errors.  It is up to\nthe programmer to make sure that array indices are valid.", "Variable declarations can be modified by various qualifiers.  You have seen\nexamples of the qualifiers ", ", ", ", and ", ".\nThese are called ", ".\nThe other possible storage qualifier is ", ", which means that the value\nof the variable cannot be changed after it has been initialized.  In addition,\nit is not legal to assign a value to an attribute or uniform variable; their\nvalues come from the JavaScript side, and they are considered to be read-only.\nThere are implementation-dependent limits on the numbers of attribute, uniform,\nand varying variables that can be used in a shader program; this is discussed\nin the last subsection of this section.", "The ", " qualifier can only be used for global variables in the\nvertex shader, and it only applies to the built-in floating point types\n", ", ", ", ", ", ", ", \n", ", ", ", and ", ".  (Matrix attributes are not supported directly\non the JavaScript side.  A matrix attribute has to be treated as a set of vector attributes,\none for each column.  Matrix attributes would be very rare, and I won't go into any\ndetail about them here.)", "Both the vertex shader and the fragment shader can use ", " variables.\nThe same variable can occur in both shaders, as long as the types in the two shaders are\nthe same.  Uniform variables can be of any type, including array and\nstructure types. Now,\nJavaScript only has functions for setting uniform values that are scalar variables,\nvectors, or matrices.  There are no functions for setting the values of structs or\narrays.  The solution to this problem requires treating every component of a\nstruct or array as a separate uniform value.  For example, consider the declarations", "The variable ", " contains twelve basic values, which are of type ", ", ", ",\nor ", ".  To work with the ", " uniform in JavaScript, we need twelve variables\nto represent the locations of the 12 components of the\nuniform variable.   When using ", " to get the location of\none of the 12 components, you need to give the full name of the component in the GLSL\nprogram.  For example: ", "(", ", \"", "\").\nIt is natural to store the 12 locations in an array of JavaScript objects that\nparallels the structure of the array of structs on the GLSL side.  Here is\ntypical JavaScript code to create the structure and use it to initialize the\nuniform variables:", "For uniform shader variables that are matrices, \nthe JavaScript function that is used to set the value of the uniform\nis ", " for a ", ",\n", " for a ", ", or ", " for a ", ".\nEven though the matrix is two-dimensional, the values are stored in a one dimensional array.\nThe values are loaded into the array in column-major order.  For example, if\n", " is a uniform ", " in the shader, then JavaScript can set\nits value to be the identity matrix with", "The second parameter ", " be ", ".  (In some other versions of OpenGL,\nthe second parameter can be set to ", " to indicate that the values are in row-major\ninstead of column-major order, but WebGL requires column-major order.)\nNote that the 3 in ", " refers to the number of rows and columns in\nthe matrix, not to the length of the array, which must be\u00a09.  (By the way, it is OK\nto use a typed array rather than a normal JavaScript array for the value of a uniform.)", "As for the ", " qualifier, it can be used only for the built-in floating\npoint types (", ") and for arrays of those types.\nA varying variable should be declared in both the vertex and fragment shader. (This is not\nactually a requirement; an error only occurs if the fragment shader tries to use the\nvalue of a varying variable that does not exist in the vertex shader.)\nA variable must have the same type in both shaders.  The variable is \nread-only in the fragment shader.\nThe vertex shader should write a value to the varying variable, and it can also read its\nvalue.", "Variable declarations can also be modified by ", ".\nThe possible precision qualifiers are ", ", ", ", and ", ".\nA precision qualifier sets the minimum range of possible values for an integer variable or the\nminimum range of values and number of decimal places for a floating point variable. \nGLSL doesn't assign a definite meaning to the precision qualifiers, but mandates some\nminimum requirements.  For example, ", " integers must be able to represent values\nin at least the range \u22122", " to 2", "; ", " integers, in the\nrange \u22122", " to 2", "; and ", " integers, in the range\n\u22122", " to 2", ".  It is possible\u2014and on desktop computers\nit is likely\u2014that all values are 32-bit values and the precision qualifiers have\nno real effect.  But GPUs in embedded systems can be more limited.", "A precision qualifier can be used on any variable declaration.  If the variable also has\na storage qualifier, the storage qualifier comes first.  For example", "A ", " variable can have different precisions in the vertex and in the fragment shader.\nThe default precision for integers and floats in the vertex shader is ", ".\nFragment shaders are not required to support ", ", although it is likely that\nthey do so, except on older mobile hardware.  In the fragment shader,\nthe default precision for integers is ", ", but floats do not have a default\nprecision.   This means that every floating point variable in the fragment shader has\nto be explicitly assigned a precision.  Alternatively, it is possible to set a default \nposition for floats with the statement", "This statement was used at the start of each of the fragment shaders\nin the ", ".  Of course, if the\nfragment shader does support ", ", this restricts the precision unnecessarily.\nYou can avoid that by using this code at the start of the fragment shader:", "This sets the default precision to ", " if it is available and\nto ", " if not.  The lines starting with \"#\" are preprocessor directives\u2014an aspect\nof GLSL that I don't want to get into.", "The last qualifier, ", ", \nis even more difficult to explain, and it has only a very limited use.  Invariance\nrefers to the requirement that when the same expression is used to compute the value of\nthe same variable (possibly in different shaders), then the value that is assigned to \nthe variable should be exactly the same in both cases.  This\nis not automatically the case.  For example, the values\ncan be different if a compiler uses different optimizations or evaluates the operands\nin a different order in the two expressions.  The ", " qualifier on the\nvariable will force the compiler to use exactly the same calculations for the two\nassignment statements.  The qualifier can only be used on declarations of varying variables.\nIt must be the first qualifier in the declaration.  For example,\n", "It can also be used to make the predefined variables such as ", " and\n", " invariant, using a statement such as", "Invariance can be important in a ", " that applies\ntwo or more shader programs in succession to compute an image.  It is important, for\nexample, that both shaders get the same answer when they compute ", "\nfor the same vertex, using the same expression in both vertex shaders.  Making ", " \ninvariant in the shaders will ensure that.", "Expressions in GLSL can use the arithmetic operators ", ", ", ", ", ", \n", ", ", " and\u00a0", " (but ", ",\u00a0", ", \nand\u00a0", " are not supported).  They are defined for the types ", "\nand ", ".  There is no automatic type conversion in expressions.  If ", "\nis of type ", ", the expression ", "1 is illegal.\nYou have to say ", "1.0 or ", "(1).", "The arithmetic operators have been extended in various ways to work with vectors and matrices.\nIf you use ", " to multiply a matrix and a vector, in either order, it does\nmatrix multiplication.  The types of the operands must match in the obvious way; for example,\na ", " can only be multiplied by a ", ", and the result is a ", ".\nWhen used with two matrices of the same size, ", " does matrix multiplication.", "If ", ", ", ", ", ", or ", " is used on a\nvector and a scalar of the same basic type, then the operation is performed on each element\nof the vector.  For example, ", "(3.0,3.0)\u00a0", "\u00a02.0 is the\nvector ", "(1.5,1.5), and 2", "ivec3(1,2,3) is the vector ", "(2,4,6).\nWhen one of these operators is applied to two vectors of the\nsame type, the operation is applied to each pair of components, and the result is \na vector.  For example, the value of", "is the vector ", "(5.2,-5.0,4.7).  Note in particular that the usual vector\narithmetic operations\u2014addition and subtraction of vectors, multiplication of a vector\nby a scalar, and multiplication of a vector by a matrix\u2014are written in the natural\nway is GLSL.", "The relational operators ", ", ", ", ", ", \nand\u00a0", " can only be applied to ", " and ", ",\nand the types of the two operands must match exactly.  However, the equality operators\n", " and ", " have been extended to work on all of the built-in\ntypes except sampler types.  Two vectors are equal only if the corresponding pairs of \ncomponents are all equal. The same is true for matrices.  The equality operators cannot be used\nwith arrays, but they do work for structs, as long as the structs don't contain any arrays\nor samplers; again, every pair of members in two structs must be equal for the structs\nto be considered equal.", "GLSL has logical operators ", ", ", ", ", ", \nand\u00a0", " (the last one being an exclusive or operation).  The operands\nmust be of type ", ".", "Finally, there are the assignment operators ", ", ", ", ", ", \n", ", and\u00a0", ", with the usual meanings.", "GLSL also has a large number of predefined functions, more than I can discuss here.\nAll of the functions that I will mention here require floating-point values as parameters,\neven if the function would also make sense for integer values.", "Most interesting, perhaps, are functions for vector algebra.  See ", "\nfor the definitions of these operations.  These functions have simple formulas, but they\nare provided as functions for convenience and because they might have efficient hardware\nimplementations in a GPU.  The function ", "(", ")\ncomputes the dot product ", "\u00b7", " of two vectors of the same length.  \nThe return value is a ", ";  ", "(", ") computes the\ncross product ", "\u00d7", ", where the parameters and return value are of type\n", "; ", "(", ") is the length of the vector ", " and\n", "(", ",", ") gives the distance between two vectors;\n", "(", ") returns a unit vector that points in the same direction as ", ".\nThere are also functions named ", " and ", " that can be used to\ncompute the direction of reflected and refracted light rays; I will cover them when\nI need to use them.", "The function ", "(", ") computes ", "(1\u2212", ") + ", ".\nIf ", " is a float in the range 0.0 to 1.0, then the return value is a linear mixture, or weighted average,\nof ", " and ", ".  This function might be used, for example, to do alpha-blending of two colors.\nThe function ", "(", ") clamps ", " to the range ", " to ", ";\nthe return value could be computed as ", "(", "(", "),", ").\nIf ", " is a vector representing a color, we could ensure that all of the components of\nthe vector lie in the range 0 to 1 with the command", "If ", " and ", " are floats, with ", ", then\n", "(", ") returns 0.0 for ", " less than ", " and\nreturns 1.0 for ", " greater than ", ".  For values of ", " between ", "\nand ", ", the return value is smoothly interpolated from 0.0 to 1.0.  Here is an\nexample that might be used in a fragment shader for rendering a ", "\nprimitive, with transparency enabled:", "This would render the point as a red disk, with the color fading smoothly from opaque\nto transparent around the edge of the disk, as ", " increases from 0.45 to 0.5.\nNote that for the functions ", ", ", ", and ", ", the ", "\nand ", " parameters can be vectors as well as floats.", "The usual mathematical functions are available in GLSL, including ", ", ", ", ", ",\n", ", ", ", ", ", ", ", ", ", ", ", ", ",\n", ", ", ", ", ", ", ", and ", ".  \nFor these functions, the parameters can be any of the types ", ",\n", ", ", ", or ", ".  The return value is of the same type,\nand the function is applied to each component separately.  For example,\nthe value of ", "(", "(16.0,9.0,4.0)) is the vector\n", "(4.0,3.0,2.0).  For ", " and ", ", there is also a \nsecond version of the function in which the first parameter is a\nvector and the second parameter is a ", ".  For those versions, each component\nof the vector is compared to the float; for example, ", "(", "(1.0,2.0,3.0),2.5)\nis ", ".", "The function ", "(", ") computes the modulus, or \nremainder, when ", " is divided by ", ".  The return value is computed as \n", "\u00a0\u2212 ", "floor(", "). \nAs with ", " and ", ", ", " can be either a vector or a float.\nThe ", " function can be used as a substitute for the ", " operator, which\nis not supported in GLSL.", "There are also a few functions for working with sampler variables that I will\ndiscuss in the next section.", "A GLSL program can define new functions, with a syntax similar to C.\nUnlike C, function names can be overloaded; that is, two functions can have\nthe same name, as long as they have different numbers or types of parameters.\nA function must be declared before it is used.  As in C, it can be declared by\ngiving either a full definition or a function prototype.", "Function parameters can be of any type.  The return type for a function can\nbe any type except for array types. A struct type can be a return type, as long as\nthe structure does not include any arrays.\nWhen an array is used a formal parameter, the length of the array must\nbe specified by an integer constant.  For example,", "Function parameters can be modified by the qualifiers ", ", ", ",\nor ", ".  The default, if no qualifier is specified, is ", ".\nThe qualifier indicates whether the parameter is used for input to the function,\noutput from the function, or both.  For input parameters, the value of the\nactual parameter in the function call is copied into the formal parameter\nin the function definition, and there is no further interaction between\nthe formal and actual parameters.  For output parameters, the value\nof the formal parameter is copied back to the actual parameter when\nthe function returns.  For an ", " parameter, the value is\ncopied in both directions.  This type of parameter passing is referred to as \n\"call by value/return.\"  Note that the actual parameter for an ", "\nor ", " parameter must be something to which a value can be\nassigned, such as a variable or swizzler.\n(All parameters in C, Java, and JavaScript are input parameters, but passing\na pointer as a parameter can have an effect similar to an ", " parameter.\nGLSL, of course, has no pointers.)  For example,", "Recursion is not supported for functions in GLSL.  This is a limitation of the\ntype of processor that is typically found in GPUs.  There is no way to implement\na stack of activation records.  Also, GLSL for WebGL does not support computations\nthat can continue indefinitely.", "The only control structures in GLSL for WebGL are the ", " statement and a very\nrestricted form of the ", " loop.  There is no ", " or ", "\nloop, and there is no ", " statement.", "\n", " statements are supported with the full syntax from C, including\n", " and ", ".", "In a ", " loop, the loop control variable must be declared in the loop,\nand it must be of type ", " or ", ".  The\ninitial value for the loop control variable must be a constant expression (that is,\nit can include operators, but all the operands must be literal constants or ", "\nvariables)  The code inside the loop is not allowed to change the value of\nthe loop control variable.  The test for ending the loop can only have the\nform ", ", where ", " is the loop\ncontrol variable, the ", " is one of the relational or equality operators,\nand the ", " is a constant expression.  Finally, the update expression\nmust have one of the forms ", ", ", ", \n", ", or ", ",\nwhere ", " is the loop control variable, and ", " is a constant\nexpression.  Of course, this is the most typical form for ", " loops in\nother languages.  Some examples of legal first lines for ", " loops:", "Note that a ", " loop of this restricted form will execute some definite,\nfinite number of iterations, which can be determined in advance.  In fact, the\nloop could be \"unrolled\" into a sequence of simple statements.  There is no possibility\nof an infinite loop.", "\n", " loops can include ", " and ", " statements.", "WebGL puts limits on certain resources that are used by WebGL and its GLSL programs,\nsuch as the number of attribute variables or the size of a texture image.\nThe limits are due in many cases to hardware limits in the ", ",\nand they depend on the device on which the program is running,\nand on the implementation of WebGL on that device.  The hardware limits will\ntend to be lower on mobile devices such as tablets and phones.  Although the limits can\nvary, WebGL imposes a set of minimum requirements that all implementations\nmust satisfy.", "For example, any WebGL implementation must allow at least 8 attributes in\na vertex shader.  The actual limit for a particular implementation might be\nmore, but cannot be less.  The actual limit is available in a GLSL program\nas the value of a predefined constant, ", ".  More conveniently,\nit is available on the JavaScript side as the value of the expression\n", "Attribute variables of type ", ", ", ", ", ", and ", " all \ncount as one attribute against the limit.  For a matrix-valued attribute, each column counts as a separate\nattribute as far as the limit goes.", "Similarly, there are limits on varying variables, and there are separate limits\non uniform variables in the vertex and fragment shaders.  (The limits are on the\nnumber of four-component \"vectors.\" There can be some packing of separate\nvariables into a single vector, but the packing that is used does not have to be optimal.\nNo packing is done for attribute variables.)  The limits must satisfy", "There are also limits in GLSL on the number of texture units, which means essentially\nthe number of texture images that can be used simultaneously.  These limits must\nsatisfy", "Textures are usually used in fragment shaders, but they can sometimes be useful\nin vertex shaders.  Note however, that ", " can be zero,\nwhich means that implementations are not required to allow\ntexture units to be used in vertex shaders.", "There are also limits on other things, including viewport size, texture image size,\nline width for line primitives, and point size for the ", " primitive.\nAll of the limits can be queried from the JavaScript side using ", "().", "The\nfollowing demo shows the actual values of the resource limits on the device\non which you are viewing this page. You can use it to check the capabilities\nof various devices on which you want your WebGL programs to run.  In general,\nthe actual limits will be significantly larger than the required minimum values.", "\n", "\n"], "chapter_title": "Introduction to WebGL", "id": 6.3}]