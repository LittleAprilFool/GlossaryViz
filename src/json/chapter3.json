[{"section_title": "null", "chapter_id": "Chapter 2", "section_id": "Section 2.0", "content": ["With this chapter, we begin our study of computer graphics by looking at the two-dimensional case.\nThings are simpler, and a lot easier to visualize, in 2D than in 3D, but most of\nthe ideas that are covered in this chapter will also be very relevant to 3D.", "The chapter begins with four sections that examine 2D graphics in a general way,\nwithout tying it to a particular programming language or graphics API.  The coding\nexamples in these sections are written in pseudocode that should make sense to\nanyone with enough programming background to be reading this book.\nIn the next three sections, we will take quick looks at 2D graphics in three\nparticular languages: Java with ", ",\nJavaScript with HTML ", " graphics, and SVG.  We will see how these\nlanguages use many of the general ideas from earlier in the chapter."], "chapter_title": "Two-Dimensional Graphics", "id": 2.0}, {"section_title": "null", "chapter_id": "Chapter 3", "section_id": "Section 3.0", "content": ["It is time to move on to computer graphics in three dimensions, although\nit won't be until Section\u00a02 of this chapter that we really get into 3D.\nYou will find that many concepts from 2D graphics carry over to 3D, but the move into\nthe third dimension brings with it some new features that take a while to\nget used to.", "Our focus will be ", ", a graphics API\nthat was introduced in 1992 and has gone through many versions and many \nchanges since then.  OpenGL is a  low-level graphics API, similar to the 2D APIs we have\ncovered.  It is even more primitive in some ways, but of course it is\ncomplicated by the fact that it supports 3D.", "For the next two chapters, the discussion is\nlimited to OpenGL\u00a01.1.  OpenGL 1.1 is a large API, and we will \nonly cover a part of it. The goal is to introduce 3D graphics concepts, \nnot to fully cover the API.  A significant part of what we cover here\nhas been removed from the most modern versions of OpenGL.  However,\nmodern OpenGL in its pure form has a very steep initial learning curve,\nand it is really not a good starting place for someone who is encountering\n3D graphics for the first time.  Some additional support is needed\u2014if not OpenGL 1.1\nthen some similar framework.  Since OpenGL 1.1 is\nstill supported, at least by all desktop implementations of OpenGL,\nit's a reasonable place to start.", "This chapter concentrates on the geometric aspects of 3D graphics, such as defining\nand transforming objects and projecting 3D scenes into 2D images.  The images that\nwe produce will look very unrealistic.  In the next chapter, we will see how to add\nsome realism by simulating the effects of lighting and of the material properties of surfaces."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.0}, {"section_title": "null", "chapter_id": "Chapter 1", "section_id": "Section 1.0", "content": ["The term \"computer graphics\" refers to anything involved in the creation or\nmanipulation of images on computer, including animated images.  It is a very\nbroad field, and one in which changes and advances seem to come at a dizzying pace.\nIt can be difficult for a beginner to know where to start.  However, there is\na core of fundamental ideas that are part of the foundation of most applications\nof computer graphics.  This book attempts to cover those foundational ideas, or\nat least as many of them as will fit into a one-semester college-level course.\nWhile it is not possible to cover the entire field in a first course\u2014or even a large\npart of it\u2014this should be a good place to start.", "This short chapter provides an overview and introduction to the material\nthat will be covered in the rest of the book, without going into a lot of detail."], "chapter_title": "Introduction", "id": 1.0}, {"section_title": "Transforms", "chapter_id": "Chapter 2", "section_id": "Section 2.3", "content": ["In ", ", we discussed ", "\nand how it is possible to transform coordinates from one coordinate system to another.  In this section,\nwe'll look at that idea a little more closely, and also look at how \n", " can\nbe used to place graphics objects into a coordinate system.", "In a typical application, we have a rectangle made of pixels, with its natural pixel coordinates, \nwhere an image will be displayed.  This rectangle will be called the ", ".\nWe also have a set of geometric objects that are defined in a possibly different coordinate system,\ngenerally one that uses real-number coordinates rather than integers.  These objects make up the\n\"scene\" or \"world\" that we want to view, and the coordinates that we use to define the scene\nare called ", ".", "For 2D graphics, the world\nlies in a plane.  It's not possible to show a picture of the entire infinite plane.  We need to pick some rectangular\narea in the plane to display in the image.  Let's call that rectangular area the ", ",\nor view window.  A coordinate transform is used to map the window to the viewport.", "\n", "In this illustration, ", " represents the coordinate transformation.  ", "\u00a0is a function that\ntakes world coordinates (", ",", ") in some window and maps them to pixel coordinates ", "(", ",", ")\nin the viewport.  (I've drawn the viewport and window with different sizes to emphasize that they\nare not the same thing, even though they show the same objects, but in fact they don't even exist in\nthe same space, so it doesn't really make sense to compare their sizes.) In this example, as you\ncan check,", "Look at the rectangle with corners at (-1,2) and (3,-1) in the window. When this rectangle\nis displayed in the viewport, it is displayed as the\nrectangle with corners ", "(-1,2) and ", "(3,-1). In this example,\n", "(-1,2)\u00a0=\u00a0(300,100) and ", "(3,-1)\u00a0=\u00a0(700,400).", "We use coordinate transformations in this way because it allows us to choose a world\ncoordinate system that is natural for describing the scene that we want to display, and it\nis easier to do that than to work directly with viewport coordinates.\nAlong the same lines, suppose that we want to define some complex object, and suppose that there will be several\ncopies of that object in our scene.  Or maybe we are making an animation, and we would like the\nobject to have different positions in different frames.  We would like to choose\nsome convenient coordinate system and use it to define the object once and for all.\nThe coordinates that we use to define an object are called ", "\nfor the object.  When we want to place the object into a scene, we need to transform\nthe object coordinates that we used to define the object into the world coordinate system\nthat we are using for the scene.  The transformation that we need is called a\n", ".  This picture illustrates an object defined\nin its own object coordinate system and then mapped by three different modeling transformations\ninto the world coordinate system:", "\n", "Remember that in order to view the scene, there will be another transformation that maps the object\nfrom a view window in world coordinates into the viewport.", "Now, keep in mind\nthat the choice of a view window tells which part of the scene is shown in the image.  Moving,\nresizing, or even rotating the window will give a different view of the scene.  Suppose we make\nseveral images of the same car:", "\n", "What happened between making the top image in this illustration and making the image on the bottom left?\nIn fact, there are two possibilities:  Either the car was moved to the ", ", or the view window that\ndefines the scene was moved to the ", ".  This is important, so be sure you understand it.\n(Try it with your cell phone camera. Aim it at some objects, take a step to the left, and notice\nwhat happens to the objects in the camera's viewfinder: They move to the right  in the picture!)\nSimilarly, what happens between the top picture and the middle picture on the bottom?  Either\nthe car rotated ", ", or the window was rotated ", ".  (Again, try it with a\ncamera\u2014you might want to take two actual photos so that you can compare them.)  Finally,\nthe change from the top picture to the one on the bottom right could happen because the car got\n", " or because the window got ", ".  (On your camera, a bigger window means that\nyou are seeing a larger field of view, and you can get that by applying a zoom to the camera or by\nbacking up away from the objects that you are viewing.)", "There is an important general idea here.  When we modify the view window, we change the\ncoordinate system that is applied to the viewport.  But in fact, this is the same as leaving\nthat coordinate system in place and moving the objects in the scene instead.  \nExcept that to get the same effect\nin the final image, you  have to apply the opposite transformation to the objects (for example,\nmoving the window to \nthe ", " is equivalent to moving the objects to the ", ").  So, there is\nno essential distinction between transforming the window and transforming the object.  Mathematically,\nyou specify a ", " by giving coordinates in some natural coordinate system, \nand the computer applies a sequence of transformations to those coordinates to produce, in the end,\nthe coordinates that are used to actually draw the primitive in the image.  You will think of some of\nthose transformations as modeling transforms and some as coordinate transforms, but to the computer,\nit's all the same.", "\nHere is a live demo that can help you to understand the equivalence between modeling transformations\nand viewport transformations.  The sliders control objects applied to the objects in the picture.\nIn the lower section of the demo, you see a larger view in which the viewport for the upper\nimage is represented as a translucent black rectangle.\n\nRead the help text in the demo for more information.\n", "\n", "\n", "We will return to this idea several times later in the book, but in any case, you can see that\n", " are a central concept in computer \ngraphics.  Let's look at some basic types of transformation in more detail.  The transforms\nwe will use in 2D graphics can be written in the form", "where (", ",", ") represents the coordinates of some point before the transformation\nis applied, and (", ",", ") are the transformed coordinates.  The transform is\ndefined by the six constants ", ", ", ", ", ", ", ", ", ", and ", ".  Note\nthat this can be written as a function ", ", where", "A transformation of this form is called an ", ".  An affine\ntransform has the property that, when it is applied to two parallel lines, the transformed \nlines will also be parallel.  Also, if you follow one affine transform by another affine\ntransform, the result is again an affine transform.", "A ", " transform simply moves every point by a certain amount \nhorizontally and a certain amount vertically.  If (", ",", ") is the \noriginal point and (", ",", ") is the transformed point, then the formula for a translation is", "where ", " is the number of units by which the point is moved horizontally and ", " \nis the amount by which it is moved vertically.\n(Thus for a translation, ", " = ", " = 1, and ", " = ", " = 0 in the\ngeneral formula for an affine transform.)\nA 2D graphics system will typically have a function such as", "to apply a translate transformation.  The translation would apply to everything that is\ndrawn ", " the command is given.  That is,  for all\nsubsequent drawing operations, ", " would be added to the x-coordinate and ", " would\nbe added to the y-coordinate.  Let's look at an example.  Suppose that you draw\nan \"F\" using coordinates in which the \"F\" is centered at (0,0).  \nIf you say ", "(4,2) ", " drawing the \"F\", then every point of the \"F\" \nwill be moved horizontally by 4 units and vertically by 2 units before the coordinates are\nactually used, so that after the translation, the \"F\" will be centered at (4,2):\n", "\n", "The light gray \"F\" in this picture shows what would be drawn without the translation; the\ndark red \"F\" shows the same \"F\" drawn after applying a translation by (4,2).  The top arrow shows\nthat the upper left corner of the \"F\" has been moved over 4 units and up 2 units.  Every point\nin the \"F\" is subjected to the same displacement.  Note that in my examples, I am assuming that\nthe y-coordinate increases from bottom to top.  That is, the y-axis points up.", "Remember that when you give the command ", "(", ",", "), the translation applies\nto ", " the drawing that you do after that, not just to the next shape that you draw.\nIf you apply another transformation after the translation, the second transform will not\nreplace the translation.  It will be combined with the translation, so that subsequent drawing\nwill be affected by the combined transformation.  For example, if you combine\n", "(4,2) with ", "(-1,5), the result is the same as a single\ntranslation, ", "(3,7).  This is an important point, and there will be a lot more to say about it later.", "Also remember that you don't compute coordinate transformations yourself.  You just specify the\noriginal coordinates for the object (that is, the object coordinates), and you specify the\ntransform or transforms that are to be applied. The computer takes care of applying the\ntransformation to the coordinates.  You don't even need to know the equations that are used\nfor the transformation; you just need to understand what it does geometrically.", "A ", " transform, for our purposes here, rotates each point about the origin, (0,0).\nEvery point is rotated through the same\nangle, called the angle of rotation.  For this purpose, angles can be measured either\nin degrees or in radians.  (The 2D graphics ", " that we will look at\nlater in this chapter use radians, but OpenGL uses degrees.)\nA rotation with a positive angle rotates objects in the direction from the positive x-axis towards the positive y-axis.  \nThis is counterclockwise in a coordinate system where the y-axis points up, \nas it does in my examples here, but it is\nclockwise in the usual pixel coordinates, where the y-axis points down rather than up.\nAlthough it is not obvious, when rotation through\nan angle of ", " radians about the origin is applied to the point (", ",", "),\nthen the resulting point (", ",", ") is given by\n", "That is, in the general formula for an affine transform, ", " = ", " = 0,\n", " = ", " = cos(", "), ", " = -sin(", "), and ", " = sin(", ").\nHere is a picture that illustrates a rotation about the origin by the angle  negative 135 degrees:\n", "\n", "Again, the light gray \"F\" is the original shape, and the dark red \"F\" is the shape that\nresults if you apply the rotation.  The arrow shows how the upper left corner of the original \"F\"\nhas been moved.", "A 2D graphics API would typically have a command ", "(", ") to apply a rotation.\nThe command is used ", " drawing the objects to which the rotation applies.", "We are now in a position to see what can happen when you\ncombine two transformations.  Suppose that before drawing some object, you say\n", "Assume that angles are measured in degrees.  \nThe translation will then apply to all subsequent drawing.  But, because of the rotation command,\nthe things that you draw after the translation are ", " objects.  That is, the translation\napplies to objects that have ", " been rotated.  \nAn example is shown on the left in the illustration below, where the light gray \"F\" is the original shape, and\nred \"F\" shows the result of applying the two transforms to the original.  The original \"F\"\nwas first rotated through a 90 degree angle, and then moved 4 units to the right.", "\n", "Note that transforms are\napplied to objects in the reverse of the order in which they are given in the code (because the\nfirst transform in the code is applied to an object that has already been affected by the second\ntransform).  And note that the order in which the transforms are applied is important.  If we reverse\nthe order in which the two transforms are applied in this example, by saying", "then the result is as shown on the right in the above illustration. In that picture,\nthe original \"F\" is first moved 4 units to the right and the resulting shape\nis then rotated through an angle of 90 degrees about the origin to give the shape that actually appears\non the screen.", "For another example of applying several transformations, suppose that we want to rotate a shape through\nan angle ", " about a point (", ",", ") instead of about the point (0,0).  We can do this by\nfirst moving the point (", ",", ") to the origin, using ", "(", ",", ").\nThen we can do a standard rotation about the origin by calling ", "(", "). Finally,\nwe can move the origin back to the point (", ",", ") by applying ", "(", ",", ").\nKeeping in mind that we have to write the code for the transformations in the reverse order, we need to say\n", "before drawing the shape.  (In fact, some graphics APIs let us accomplish this transform with a\nsingle command such as ", "(", ",", ",", ").  This would apply a rotation\nthrough the angle ", " about the point (", ",", ").)", "A ", " transform can be used to make objects bigger or smaller. Mathematically,\na scaling transform simply multiplies each x-coordinate by a given amount and each y-coordinate by\na given amount. That is, if a point (", ",", ") is scaled by a factor of ", " in the\nx direction and by a factor of ", " in the y direction, then the resulting point (", ",", ")\nis given by", "If you apply this transform to a shape that is centered at the origin, it will stretch the shape\nby a factor of ", " horizontally and ", " vertically.  Here is an example, in which the\noriginal light gray \"F\" is scaled by a factor of 3 horizontally and 2 vertically to give the\nfinal dark red \"F\":", "\n", "The common case where the horizontal and vertical scaling factors are the same is\ncalled ", ".  Uniform scaling stretches or shrinks a shape without\ndistorting it.", "When scaling is applied to a shape that is not centered at (0,0), then in addition to being\nstretched or shrunk, the shape will be moved away from 0 or towards 0.  In fact, the true description\nof a scaling operation is that it pushes every point away from (0,0) or pulls every point towards (0,0).\nIf you want to scale about a point other than (0,0), you can use a sequence of three transforms,\nsimilar to what was done in the case of rotation.", "A 2D graphics API can provide a function ", "(", ",", ") for\napplying scaling transformations.  As usual, the transform applies to all ", " and ", "\ncoordinates in subsequent drawing operations. Note that negative scaling factors are allowed and will result in reflecting the\nshape as well as possibly stretching or shrinking it.  For example, ", "(1,-1) will\nreflect objects vertically, through the ", "-axis.", "It is a fact that every affine transform can be created by combining translations, rotations\nabout the origin, and scalings about the origin.  I won't try to prove that, but \nhere is an\ninteractive demo that will let you experiment with translations, rotations, and scalings, and with the\ntransformations that can be made by combining them.", "\n", "\n", "I also note that a transform that is made from translations and rotations, with no scaling, will preserve\nlength and angles in the objects to which it is applied.  It will also preserve\n", " of rectangles.  Transforms with this property\nare called \"", ".\"   If you also allow ", " \nscaling, the resulting transformation will preserve angles and aspect ratio, but not lengths.", "We will look at one more type of basic transform, a ", ".\nAlthough shears can in fact be built up out of rotations and scalings if necessary, it is not\nreally obvious how to do so.  A shear will \"tilt\" objects.  A horizontal shear will tilt things \ntowards the left (for negative shear) or right (for positive shear).  A vertical shear tilts them\nup or down.  Here is an example of horizontal shear: ", "\n", "A horizontal shear does not move the x-axis.  Every other horizontal line is moved to the left\nor to the right by an amount that is proportional to the y-value along that line.  When a horizontal\nshear is applied to a point (", ",", "), the resulting point (", ",", ") is\ngiven by", "for some constant shearing factor ", ".  Similarly, a vertical shear with shearing factor ", "\nis given by the equations", "Shear is occasionally called \"skew.\"", "The last transformation that is applied to an object before it is displayed in an\nimage is the window-to-viewport transformation, which maps the rectangular ", "\nin the xy-plane that contains the scene to the rectangular grid of pixels where the \nimage will be displayed.  I'll assume here that the view window is not rotated; that it, its\nsides are parallel to the x- and y-axes.  In that case, the window-to-viewport transformation\ncan be expressed in terms of translation and scaling transforms.  Let's look at the\ntypical case where the viewport has pixel coordinates ranging from 0 on the left to \n", " on the right and from 0 at the top to ", " at the bottom.\nAnd assume that the limits on the view window are ", ", ", ",\n", ", and ", ".  In that case, the window-to-viewport transformation\ncan be programmed as:", "These should be the last transforms that are applied to a point.  Since transforms\nare applied to points in the reverse of the order in which they are specified in the\nprogram, they should be the first transforms that are specified in the program. To see how this works,\nconsider a point (", ",", ") in the view window.  (This point comes from some object in\nthe scene.  Several modeling transforms might have already been applied to the\nobject to produce the point (", ",", "), and that point is now ready for its final transformation\ninto viewport coordinates.)  The coordinates (", ",", ") are first translated by (", ",", ")\nto give (", ",", ").  These coordinates are then multiplied by the\nscaling factors shown above, giving the final coordinates", "Note that the point (", ",", ") is mapped to (0,0), while the\npoint (", ",", ") is mapped to (", ",", "),\nwhich is just what we want.", "There is still the question of ", ".  As noted in\n", ", if we want to force the aspect ratio of the\nwindow to match the aspect ratio of the viewport, it might be necessary\nto adjust the limits on the window.   Here is pseudocode for a subroutine\nthat will do that, again assuming that the top-left corner of the viewport\nhas pixel coordinates (0,0):", "The transforms that are used in computer graphics can be represented as\nmatrices, and the points on which they operate are represented as\nvectors.  Recall that a ", ", from the point of view of a\ncomputer scientist, is a two-dimensional array of numbers, while a\n", " is one-dimensional.  Matrices and vectors are\nstudied in the field of mathematics called ", ".\nLinear algebra is fundamental to computer graphics.  In fact,\nmatrix and vector math is built into ", ".\nYou won't need to know a great deal about linear algebra for this textbook,\nbut a few basic ideas are essential.", "The vectors that we need are lists of two, three, or four numbers.  They\nare often written as (", ",", "), (", ",", ",", "), and (", ",", ",", ",", ").  A matrix with N rows\nand M columns is called an \"N-by-M matrix.\"  For the most part,\nthe matrices that we need are N-by-N matrices, where N is 2, 3, or 4.\nThat is, they have 2, 3, or 4 rows and columns, and the number of \nrows is equal to the number of columns.", "If ", " and ", " are two N-by-N matrices, then they can be multiplied to\ngive a product matrix ", "\u00a0=\u00a0", ".  If ", "\u00a0is an N-by-N\nmatrix, and ", " is a vector of length N, then ", " can be multiplied\nby ", " to give another vector ", "\u00a0=\u00a0", ".  The function\nthat takes ", " to ", " is a transformation; it transforms any given\nvector of length N into another vector of length\u00a0N.  A\u00a0transformation of this\nform is called a ", ".", "Now, suppose that ", " and ", " are N-by-N matrices and ", " is \na vector of length N.  Then, we can form two different products:\n", "(", ") and (", ".  It is a central fact that these\ntwo operations have the same effect.  That is, we can multiply ", " by ", "\nand then multiply the result by ", ", or we can multiply the matrices\n", " and ", " to get the matrix product ", " and then multiply\n", " by ", ".  The result is the same.", "Rotation and scaling, as it turns out, are linear transformations.  That\nis, the operation of rotating (", ",", ") through an angle\u00a0", "\nabout the origin can be done by multiplying (", ",", ") by a 2-by-2 matrix.\nLet's call that matrix ", ".  Similarly, scaling by a factor\n", " in the horizontal direction and ", " in the vertical direction\ncan be given as a matrix ", ".  If we want to apply\na scaling followed by a rotation to the point ", " = (", ",", "), we can\ncompute ", "  ", "(", ") ", "\n(", ")", ".", "So what?  Well, suppose that we want to apply the same two operations, scale then rotate, \nto thousands of points,  as we typically do when transforming objects for computer graphics.  The point \nis that we could compute the product matrix ", " once \nand for all, and then apply the combined transform to each point with a single multiplication.\nThis means that if a program says", "the computer doens't have to keep track of two separate operations.  It combines the\noperations into a single matrix and just keeps track of that. Even if you apply, say, 50 \ntransformations to the object, the computer can just combine them all into one matrix.\nBy using matrix algebra, multiple transformations can be handled as efficiently as a\nsingle transformation!", "This is really nice, but there is a gaping problem: ", "\nTo bring translation into this framework, we do something that looks a little strange at first:\nInstead of representing a point in 2D as a pair of numbers (", ",", "), we represent\nit as the triple of numbers (", ",", ",1).  That is, we add a one as the third coordinate.\nIt then turns out that we can then represent rotation, scaling, and translation\u2014and hence\nany affine transformation\u2014on 2D space as multiplication by a 3-by-3 matrix.  The matrices \nthat we need have a bottom row containing (0,0,1).   Multiplying (", ",", ",1)\nby such a matrix gives a new vector (", ",", ",1).  We ignore the extra coordinate and consider this\nto be a transformation of (", ",", ") into (", ",", "). For the record, the 3-by-3\nmatrices for translation (", "), scaling (", "),\nand rotation (", ") in 2D are", "\n", "You can compare multiplication by these matrices to the formulas given above for translation, scaling,\nand rotation.  However, you won't need to do the multiplication yourself.  For now,\nthe important idea that you should take away from this discussion is that a sequence of transformations\ncan be combined into a single transformation.  The computer only needs to keep track of a single matrix, which we\ncan call the \"current matrix\" or \"current transformation.\"  To implement transform commands such as ", "(a,b)\nor ", "(d), the computer simply multiplies the current matrix by the matrix that represents the\ntransform."], "chapter_title": "Two-Dimensional Graphics", "id": 2.3}, {"section_title": "Shapes", "chapter_id": "Chapter 2", "section_id": "Section 2.2", "content": ["We have been talking about low-level graphics concepts like ", " \nand ", ", but\nfortunately we don't usually have to work on the lowest levels.  Most graphics systems let\nyou work with higher-level shapes, such as triangles and circles, rather than individual\npixels.  And a lot of the hard work with coordinates is done using \n", " rather than by working with coordinates\ndirectly.  In this section and the next, we will look at some of the higher-level capabilities\nthat are typically provided by 2D graphics APIs.", "In a graphics ", ", there will be certain basic shapes that can be drawn with one command, whereas\nmore complex shapes will require multiple commands.  Exactly what qualifies as a basic shape varies\nfrom one API to another.  In the JavaScript API for drawing on an ", ", for example,\nthe only basic shapes are lines and rectangles.  In this subsection, I consider lines, rectangles, and ovals\nto be basic.", "By \"line,\" I really mean line segment, that is a straight line segment connecting two given\npoints in the plane.  A simple one-pixel-wide line segment, without ", ", is \nthe most basic shape.  It can be drawn by coloring pixels that lie along the infinitely thin\ngeometric line segment.  An algorithm for drawing the line has to decide exactly which pixels\nto color.  One of the first computer graphics algorithms, \n", " for line drawing, implements\na very efficient procedure for doing so.  I won't discuss such low-level details here, but it's\nworth looking them up if you want to start learning about what graphics hardware actually has to do.\nIn any case, lines are typically more complicated. Antialiasing is one complication.  Line width is\nanother.  A wide line might actually be drawn as a rectangle.", "Lines can have other ", ", or properties, that affect\ntheir appearance. One question is, what should happen at the end of a wide line?  Appearance might\nbe improved by adding a rounded \"cap\" on the ends of the line.  A square cap\u2014that is, extending the\nline by half of the line width\u2014might also make sense.  Another question is, when two lines meet\nas part of a larger shape, how should the lines be joined?  And many graphics systems support lines \nthat are patterns of dashes and dots.  This illustration shows some of the possibilities:", "\n", "On the left are three wide lines with no cap, a round cap, and a square cap.  The geometric line\nsegment is shown as a dotted line.  (The no-cap style is called \"butt.\")  To the right are four lines\nwith different patters of dots and dashes.  In the middle are three different styles of line joins:\nmitered, rounded, and beveled.", "The basic rectangular shape has sides that are vertical and horizontal.  (A tilted rectangle generally\nhas to be made by applying a ", ".) Such a rectangle can be specified \nwith two points, (x1,y1) and (x2,y2), that give the endpoints of one of the diagonals of the rectangle.\nAlternatively, the width and the height can be given, along with a single base point, (x,y).  In that\ncase, the width and height have to be positive, or the rectangle is empty.  The base point (x,y) will\nbe the upper left corner of the rectangle if y increases from top to bottom, and it will be the\nlower left corner of the rectangle if y increases from bottom to top.", "\n", "Suppose that you are given points (x1,y1) and (x2,y2), and that you want to draw the rectangle\nthat they determine.  And suppose that the only rectangle-drawing command that you have available\nis one that requires a point (x,y), a width, and a height. For that command, x must be the\nsmaller of x1 and x2, and the width can be computed as the absolute value of x1 minus x2. And\nsimilarly for y and the height.  In pseudocode,\n", "A common variation on rectangles is to allow rounded corners.  For a \"round rect,\" the corners\nare replaced by elliptical arcs.  The degree of rounding can be specified by giving the horizontal radius\nand vertical radius of the ellipse.  Here are some examples of round rects.  For the shape at the\nright, the two radii of the ellipse are shown:", "\n", "My final basic shape is the oval.  (An oval is also called an ellipse.)  An oval is a closed\ncurve that has two radii.  For a basic oval, we assume that the radii are vertical and horizontal.\nAn oval with this property can be specified by giving the rectangle that just contains it.\nOr it can be specified by giving its center point and the lengths of its vertical radius and\nits horizontal radius.  In this illustration, the oval on the left is shown with its\ncontaining rectangle and with its center point and radii:", "\n", "The oval on the right is a circle.  A circle is just an oval in which the two radii have\nthe same length.  ", "If ovals are not available as basic shapes, they can be approximated by drawing a large\nnumber of line segments.  The number of lines that is needed for a good approximation depends on\nthe size of the oval.  It's useful to know how to do this. Suppose that an oval has center point (x,y), \nhorizontal radius r1, and vertical radius r2.  Mathematically, the points on the oval are given by", "where ", " takes on values from 0 to 360 if angles are measured in degrees or\nfrom 0 to 2\u03c0 if they are measured in radians.  Here ", " and ", " are the\nstandard sine and cosine functions.  To get an approximation for an oval, we can use this\nformula to generate some number of points and then connect those points with line segments.\nIn pseudocode, assuming that angles are measured in radians and that ", " represents\nthe mathematical constant\u00a0\u03c0,", "For a circle, of course, you would just have r1 = r2.  This is the first time\nwe have used the sine and cosine functions, but it won't be the last.  These\nfunctions play an important role in computer graphics because of their\nassociation with circles, circular motion, and rotation.  We will meet them\nagain when we talk about transforms in the ", ".", "Here's a little demo\nthat you can use to experiment with using line segements to approximate ovals:", "\n", "\n", "There are two ways to make a shape visible in a drawing.  You can ", " it.\nOr, if it is a closed shape such as a rectangle or an oval, you can ", " it.\nStroking a line is like dragging a pen along the line.  Stroking a rectangle or oval is like\ndragging a pen along its boundary.  Filling a shape means coloring all the points that are contained\ninside that shape.  It's possible to both stroke and fill the same shape; in that case, the\ninterior of the shape and the outline of the shape can have a different appearance.", "When a shape intersects itself, like the two shapes in the illustration below, it's not\nentirely clear what should count as the interior of the shape.  In fact, there are at least\ntwo different rules for filling such a shape.  Both are based on something called the\n", ".  The winding number of a shape about a point is, roughly,\nhow many times the shape winds around the point in the positive direction, which I take here\nto be counterclockwise.\nWinding number can be negative when the winding is in the opposite direction.  \nIn the illustration, the shapes\non the left are traced in the direction shown, and the winding number about each region is \nshown as a number inside the region.", "\n", "The shapes are also shown filled using the two fill rules.  For the shapes in the center,\nthe fill rule is to color any region that has a non-zero winding number.  For the shapes\nshown on the right, the rule is to color any region whose winding number is odd; regions with\neven winding number are not filled.", "There is still the question of what a shape should be filled ", ".  Of course, it\ncan be filled with a color, but other types of fill are possible, including \n", " and ", ". \nA pattern is an image, usually a small image.  When used to fill a shape, a pattern can be\nrepeated horizontally and vertically as necessary to cover the entire shape.\nA gradient is similar in that it is a way for color to vary from point to point, but \ninstead of taking the colors from an image, they are computed.  There are a lot of variations\nto the basic idea, but there is always a line segment along which the color varies.\nThe color is specified at the endpoints of the line segment, and possibly at additional\npoints; between those points, the color is ", ".\nFor other points on the line that contains the line segment, the pattern on the line segment\ncan be repeated, or the color of the endpoint can simply be extended.  For a\n", ", the color is constant along lines perpendicular to the basic\nline segment, so you get lines of solid color going in that direction.\nIn a ", ", the color is constant along circles centered at one\nof the endpoints of the line segment.  And that doesn't exhaust the possibilities.\nTo give you an idea what patterns and gradients can look like,\nhere is a shape, filled with two gradients and two patterns:\n", "\n", "The first shape is filled with a simple linear gradient defined by just two colors,\nwhile the second shape uses a radial gradient.", "Patterns and gradients are not necessarily restricted to filling shapes.  Stroking a shape is,\nafter all, the same as filling a band of pixels along the boundary of the shape,\nand that can be done with a gradient or a pattern, instead of with  a solid color.", "Finally, I will mention that a string of text can be considered to be a shape for the purpose of\ndrawing it.  The boundary of the shape is the outline of the characters.\nThe text is drawn by filling that shape.  In some graphics systems, it is also possible to\nstroke the outline of the shape that defines the text.  \nIn the following illustration, the string \"Graphics\" is shown, on top, filled with a pattern and,\nbelow that, filled with a gradient and stroked with solid black:", "\n", "It is impossible for a graphics API to include every possible shape as a basic shape, but there\nis usually some way to create more complex shapes.   For example, consider\n", ".  A polygon is a closed shape consisting of a\nsequence of line segments.  Each line segment is joined to the next at its endpoint, and the\nlast line segment connects back to the first.  The endpoints are called the vertices of the\npolygon, and a polygon can be defined by listing its vertices.", "In a ", ", all the sides are the same length and all the\nangles between sides are equal.  Squares and equilateral triangles are examples of regular\npolygons.  A ", " has the property that whenever two points are inside\nor on the polygon, then the entire line segment between those points is also inside or on the polygon.\nIntuitively, a convex polygon has no \"indentations\" along its boundary.  (Concavity can be a property\nof any shape, not just of polygons.)", "\n", "Sometimes, polygons are required to be \"simple,\" meaning that the polygon has no self-intersections.\nThat is, all the vertices are different, and a side can only intersect another side at its\nendpoints. And polygons are usually required to be \"planar,\" meaning that all the\nvertices lie in the same plane.  (Of course, in 2D graphics,\n", " lies in the same plane, so this is not an issue.  However, it does become\nan issue in 3D.)", "How then should we draw polygons?  That is, what capabilities would we like to have in a \ngraphics API for drawing them.  One possibility is to have commands for stroking and for\nfilling polygons, where the vertices of the polygon are given as an array of points or as an array\nof x-coordinates plus an array of y-coordinates.  In fact, that is sometimes done; for example,\nthe Java graphics API includes such commands.  Another, more flexible, approach is to introduce\nthe idea of a \"path.\"  Java, SVG, and the HTML canvas API all\nsupport this idea.  A path is a general shape that can include both line\nsegments and curved segments.  Segments can, but don't have to be, connected to other segments\nat their endpoints.  A path is created by giving a series of commands that tell, essentially,\nhow a pen would be moved to draw the path.  While a path is being created, there is a point\nthat represents the pen's current location.  There will be a command for moving the pen without\ndrawing, and commands for drawing various kinds of segments.  For drawing polygons, we\nneed commands such as", "(For ", ", I need to define \"starting point.\"  A path can be made up\nof \"subpaths\"  A subpath consists of a series of connected segments.  A ", "\nalways starts a new subpath.  A ", " ends the current segment and implicitly\nstarts a new one.  So \"starting point\" means the position of the pen after the most recent\n", " or ", ".)", "Suppose that we want a path that represents the triangle with vertices at (100,100), (300,100),\nand (200, 200).  We can do that with the commands", "The ", " command at the end could be replaced by ", ",\nto move the pen back to the first vertex.", "A path represents an abstract geometric object.  Creating\none does not make it visible on the screen.  Once we have a path, to make it visible we need additional\ncommands for stroking and filling the path.", "Earlier in this section, we saw how to approximate an oval by drawing, in effect, a regular\npolygon with a large number of sides.  In that example, I drew each side as a separate line segment,\nso we really had a bunch of separate lines rather than a polygon.  There is no way to fill such\na thing.  It would be better to approximate the oval with a polygonal path.  For an oval with\ncenter (x,y) and radii r1 and r2:", "Using this path, we could draw a filled oval as well as stroke it.  \nEven if we just want to draw the outline of a polygon,\nit's still better to create the polygon as a path rather than to draw the line segments as\nseparate sides.  With a path, the computer knows that the sides are part of single shape.\nThis makes it possible to control the appearance of the \"join\" between consecutive sides, as noted\nearlier in this section.", "I noted above that a path can contain other kinds of segments besides lines.  For example,\nit might be possible to include an arc of a circle as a segment.  Another type of curve\nis a ", ".  Bezier curves can be used to create very general \ncurved shapes.  They are fairly intuitive, so that they are often used in programs that\nallow users to design curves interactively.  Mathematically, Bezier curves are defined\nby parametric polynomial equations, but you don't need to understand what that means to\nuse them.  There are two kinds of Bezier curve in common use, cubic Bezier curves and\nquadratic Bezier curves; they are defined by cubic and quadratic polynomials respectively.\nWhen the general term \"Bezier curve\" is used, it usually refers to cubic Bezier curves.", "A cubic Bezier curve segment is defined by the two endpoints of the segment together\nwith two ", ".  To understand how it works,\nit's best to think about how a pen would draw the curve segment.  The pen starts at the\nfirst endpoint, headed in the direction of the first control point.  The distance of the\ncontrol point from the endpoint controls the speed of the pen as it starts drawing the\ncurve.  The second control point controls the direction and speed of the pen as it gets\nto the second endpoint of the curve.  There is a unique cubic curve that satisfies\nthese conditions.", "\n", "The illustration above shows three cubic Bezier\ncurve segments.  The two curve segments on the right are connected at an endpoint to form a longer\ncurve.  The curves are drawn as thick black lines.  The endpoints are shown as black dots\nand the control points as blue squares, with a thin red line connecting each control point\nto the corresponding endpoint. (Ordinarily, only the curve would be drawn, except in an\ninterface that lets the user edit the curve by hand.)  Note that at an endpoint, the\ncurve segment is tangent to the line that connects the endpoint to the control point.\nNote also that there can be a sharp point or corner where two curve segments meet.  However,\none segment will merge smoothly into the next if control points are properly chosen.\n", "This will all be easier to understand\nwith some hands-on experience. \nThis interative demo lets you edit cubic Bezier curve segments by dragging their endpoints \nand control points:\n", "\n", "\n", "When a cubic Bezier curve segment is added to a path, the path's current pen location acts\nas the first endpoint of the segment.  The command for adding the segment to the path must specify\nthe two control points and the second endpoint.  A typical command might look like", "This would add a curve from the current location to point (x,y), using (cx1,cy1) and (cx2,cy2) as the\ncontrol points.  That is, the pen leaves the current location heading towards (cx1,cy1), and it \nends at the point (x,y), arriving there from the direction of (cx2,cy2).  ", "Quadratic Bezier curve segments are similar to the cubic version, but in the quadratic\ncase, there is only one control point for the segment.  The curve leaves the first endpoint\nheading in the direction of the control point, and it arrives at the second endpoint coming\nfrom the direction of the control point.  The curve in this case will be an arc of a\nparabola.", "Again, this is easier to understand this with some hands-on experience.  Try this interactive demo:", "\n", "\n"], "chapter_title": "Two-Dimensional Graphics", "id": 2.2}, {"section_title": "SVG: A Scene Description Language", "chapter_id": "Chapter 2", "section_id": "Section 2.7", "content": ["We finish this chapter with a look at one more 2D graphics system:\n", ", or Scalable Vector Graphics.  So far, we have\nbeen considering graphics programming APIs.  SVG, on the other\nhand is a ", " rather\nthan a programming language.  Where a programming language creates\na scene by generating its contents procedurally, a scene description\nlanguage specifies a scene \"declaratively,\" by listing its content.\nSince SVG is a ", " language, the content of\nof a scene includes shapes, attributes such as color and line width,\nand geometric transforms.  Most of this should be familiar to you,\nbut it should be interesting to see it in a new context.", "SVG is an ", " language, which means it has a very strict\nand somewhat verbose syntax.  This can make it a little annoying to write,\nbut on the other hand, it makes it possible to read and understand\nSVG documents even if you are not familiar with the syntax.  It's possible\nthat SVG originally stood for \"Simple\" Vector Graphics, but it is by\nno means a simple language at this point.  I will cover only a part of it\nhere, and there are many parts of the language and many options that I will\nnot mention.  My goal is to introduce the idea of a scene description language\nand to show how such a language can use the same basic ideas that are\nused in the rest of this chapter.", "SVG can be used as a file format for storing vector graphics\nimages, in much the same way that PNG and JPEG are file formats for\nstoring pixel-based images.  That means that you can open an SVG\nfile with a web browser to view the image.  (This is true, at least,\nfor modern web browsers.)  An SVG image can be included in a web page\nby using it as the source of an ", " element.  That's how the\nSVG examples on this page are displayed.  Since SVG documents are written in plain text,\nyou can create SVG images using a regular text editor, and you can read the\nsource for an SVG image by opening it in a text editor or by viewing the\nsource of the image when it is displayed in a web browser.", "An SVG file, like any XML document, starts with some standard code that almost\nno one memorizes.  It should just be copied into a new document.  Here\nis some code that can be copied as a starting point for SVG \ndocuments of the type discussed in this section (which, remember use \nonly a subset of the full SVG specification):", "The first three lines say that this is an XML SVG document.  The rest of\nthe document is an ", " element that acts as a container for the entire\nscene description.  You'll need to know a little about XML syntax.\nFirst, an XML \"element\" in its general form looks like this:\n", "The element starts with a \"start tag,\" which begins with a \"<\" followed by an identifier\nthat is the name of the tag, and ending with a\u00a0\">\".  The start tag can include\n\"attributes,\" which have the form ", ".  The ", " is an identifier;\nthe ", " is a string.  The value must be enclosed in single or double quotation marks.\nThe element ends with an \"end tag,\" which has an element name that matches the element name\nin the start tag and has the form </", ">.  Element names and attribute names\nare case-sensitive.  Between the start and end tags\ncomes the \"content\" of the element.  The content can consist of text and nested elements.\nIf an element has no content, you can replace the \">\" at the end of the start tag with\n\"/>\", and leave out the end tag.  This is called a \"self-closing tag.\" For example,\n", "This is an actual SVG element that specifies a circle.  It's easy to forget the \"/\"\nat the end of a self-closing tag, but it has to be there to have a legal XML document.", "Looking back at the SVG document, the five lines starting with <svg are just a long\nstart tag.  You can use the tag as shown, and customize the values of the ", ",\n", ", ", ", and ", " attributes.  The next line\nis a comment; comments in XML start with \"", "\" and end with \"", "\".", "The ", " and ", " attributes of the ", " tag specify a\nnatural or preferred size for the image.  It can be forced into a different size, for\nexample if it is used in an ", " element on a web page that specifies a different\nwidth and height.  The size can be specified using units of measure such as ", " for\ninches, ", " for centimeters, and ", ", for pixels, with 90 pixels to the inch.\nIf no unit of measure is specified, pixels are used.  There cannot be any space between\nthe number and the unit of measure.", "The ", " attribute sets up the ", " that will be used for \ndrawing the image.  It is what I called the ", " in ", ".\nThe value for viewBox is a list of four numbers,\ngiving the minimum ", "value, the minimum ", ", the width, and the height\nof the view window.  The width and the height must be positive, so ", " increases from\nleft-to-right, and ", " increases from top-to-bottom.  The four numbers in the list\ncan be separated either by spaces or by commas; this is typical for lists of numbers in SVG.", "Finally, the ", " attribute tells what happens when the\n", " of the viewBox does not match the aspect ratio of the rectangle\nin which the image is displayed.  The default value, \"xMidYMid\", will extend the limts\non the viewBox either horizontally or vertically to preserve the aspect ratio, and the\nviewBox will appear in the center of the display rectangle.  If you would like your \nimage to stretch to fill the display rectangle, ignoring the aspect ratio, set the\nvalue of ", " to \"none\".  (The aspect ratio issue was\ndiscussed in ", ".)", "Let's look at a complete SVG document that draws a few simple shapes.  Here's the\ndocument.  You could probably figure out what it draws even without knowing any more\nabout SVG:", "and here's the image that is produced by this example:", "\n", "In the drawing coordinate system for this example, ", " ranges from 0 to 3, and\n", " ranges from 0 to 2.  All values used for drawing, including stroke width\nand font size, are given in terms of this coordinate system.  Remember that you can\nuse any coordinate system that you find convenient!  Note, by the way, that parts\nof the image that are not covered by the shapes that are drawn will be transparent.", "Here's another example, with a larger variety of shapes.  The source code for this\nexample has a lot of comments. It uses features that we will discuss in the remainer of\nthis section.", "\n", "You can take a look at the source code, ", ".\n(For example, open it in a text editor, or open it in a web browser and use the\nbrowser's \"view source\" command.)", "In SVG, a basic shape is specified by an element in which the tag name gives the\nshape, and attributes give the properties of the shape.  There are attributes to specify\nthe geometry, such as the endpoints of a line or the radius of a circle.\nOther attributes specify style properties, such as fill color and line width.\n(The style properties are what I call ", " elsewhere\nin this book; in this section, I am using the term \"attribute\" in its XML sense.)\nAnd there is a ", " attribute that can be used to apply a\n", " to the shape.", "For a detailed example, consider the ", " element, which specifies a rectangle.  \nThe geometry of the rectangle is given by attributes named ", ", ", ", ", "\nand ", " in the usual way.  The default value for ", " and ", " is zero;\nthat is, they are optional, and leaving them out is the same as setting their value to zero.\nThe ", " and the ", " are required attributes.  Their values must be\nnon-negative.  For example, the element", "specifies a rectangle with corner at (0,0), width 3, and height 2, while", "gives a rectangle with corner at (100,200), width 640, and height 480.  (Note, by\nthe way, that the attributes in an XML element can be given in any order.)  The ", "\nelement also has optional attributes ", " and ", " that can be used to make\n\"roundRects,\" with their corners replaced by elliptical arcs.  The values of ", "\nand ", " give the horizontal and vertical radii of the elliptical arcs.", "Style attributes can be added to say how the shape should be stroked and filled.\nThe default is to use a black fill and no stroke.  (More precisely, as we will see later,\nthe default for is for a shape to inherit the values of style attributes from its \nenvironment.  Black fill and no stroke is the initial environment.)  Here are some\ncommon style attributes:", "As an example that uses many of these options, let's make a square is rounded rather than pointed \nat the corners, with size 1, centered\nat the origin, and using a translucent red fill and a gray stroke:", "and a simple outline of a rectangle with no fill:", "The ", " attribute can be used to apply a transform or a series of\ntransforms to a shape.  As an example, we can make a rectangle tilted 30 degrees from\nthe horizontal:", "The value \"rotate(30)\" represents a rotation of 30 degrees (not radians!) about the \norigin, (0,0). The positive direction of rotation, as usual, rotates the positive x-axis in the\ndirection of the positive y-axis.  You can specify a different center of rotation by\nadding arguments to ", ".  For example, to rotate the same rectangle about its\ncenter", "Translation and scaling work as you probably expect, with transform values of\nthe form \"translate(", ")\" and \"scale(", ")\".  There are also\n", " transforms, but they go by the\nnames ", " and ", ", and the argument is a skew angle rather\nthan a shear amount.  For example, the transform \"skewX(45)\" tilts the y-axis\nby 45 degrees and is equivalent to an x-shear with shear factor\u00a01.\n(The function that tilts the y-axis is called ", " because it modifies,\nor skews, the x-coordinates of points while leaving their y-coordinates unchanged.)\nFor example, we can use ", " to tilt a rectangle and make it into a\nparallelogram:", "I used an angle of -30 degrees to make the rectangle tilt to the right\nin the usual pixel coordinate system.", "The value of the ", " attribute can be a list of transforms,\nseparated by spaces or commas.  The transforms are applied to the object, as\nusual, in the opposite of the order in which they are listed. So,", "would first skew the rectangle into a parallelogram, then rotate the parallelogram\nby 45 degrees about the origin, then translate it by 50 units in the y-direction.", "In addition to rectangles, SVG has lines, circles, ellipses, and text as basic\nshapes.  Here are some details.  A ", " element represents a line segement and\nhas geometric attributes ", ", ", ", ", ", and ", " to specify the \ncoordinates of the endpoints of the line segment.  These four attributes have\nzero as default value, which makes it easier to specify horizontal and vertical lines.\nFor example,", "Without the ", " attribute, you wouldn't see the line, since the default\nvalue for ", " is \"none\".", "For a ", " element, the geometric attributes are ", ", ", ", and ", "\ngiving the coordinates of the center of the circle and the radius.  The center coordinates\nhave default values equal to zero.  For an ", " element, the attributes are\n", ", ", ", ", ", and ", ", where ", " and ", " give\nthe radii of the ellipse in the x- and y-directions.", "A ", " element is a little different.  It has attributes ", " and ", ",\nwith default values zero, to specify the location of the basepoint of the text.  However,\nthe text itself is given as the content of the element rather than as an attribute.  That is,\nthe element is divided into a start tag and an end tag, and the text that will appear in\nthe drawing comes between the start and end tags.  For example,", "The usual stroke and fill attributes apply to text, but text has additional style\nattributes.  The ", " attribute specifies the font itself.  Its value\ncan be one of the generic font names \"serif\", \"sans-serif\", \"monospace\", or the name of\na specific font that is available on the system.  The ", " can be a number\ngiving the (approximate) height of the characters in the coordinate system.  (Font size\nis subject to coordinate and modeling transforms like any other length.)  You can get\nbold and italic text by setting ", " equal to \"bold\" and\n", " equal to \"italic\".  Here is an example that uses all of these options,\nand applies some additional styles and a transform for good measure:", "SVG has some nice features for making more complex shapes.  The ", " element\nmakes it easy to create a polygon from a list of coordinate pairs.  For example,", "creates a five-sided polygon with vertices at (0,0), (100,0), (100,75), (50,100), and\n(0,75).  Every pair of numbers in the ", " attribute specifies a vertex.  The numbers\ncan be separated by either spaces or commas.  I've used a mixture of spaces and commas here to\nmake it clear how the numbers pair up.   Of course, you can add the usual style attributes\nfor stroke and fill to the polygon element.  A ", " is similar to a ", ",\nexcept that it leaves out the last line from the final vertex back to the starting vertex.\nThe difference only shows up when a polyline is stroked; a polyline is filled as if the\nmissing side were added.", "The ", " element is much more interesting. In fact, all of the other basic shapes,\nexcept text, could be made using path elements.  A path can consist of line segments,\n", ", and elliptical arcs (although I won't\ndiscuss elliptical arcs here).  The syntax for\nspecifying a path is very succinct, and it has some features that we have not seen before.\nA path element has an attribute named ", " that contains the data for the path.  The\ndata consists of one or more commands, where each command consists of a single letter followed\nby any data necessary for the command.  The moveTo, lineTo, cubic Bezier, and quadratic\nBezier commands that you are already familiar with are coded by the letters M, L, C, and Q.\nThe command for closing a path segment is Z, and it requires no data.\nFor example the path data \"M\u00a010\u00a020\u00a0L\u00a0100\u00a0200\" would draw a line segment\nfrom the point (10,20) to the point (100,200).  You can combine several connected line segments\ninto one L command.  For example, the ", " example given above could be created\nusing the ", " element", "The Z at the end of the data closes the path by adding the final side to the polygon.\n(Note that, as usual, you can use either commas or spaces in the data.)", "The C command takes six numbers as data, to specify the two control points and the final\nendpoint of the cubic Bezier curve segment.  You can also give a multiple of six values to get\na connected sequence of curve segements.  Similarly, the Q command uses four data values to\nspecify the control point and final endpoint of the quadratic Bezier curve segment.\nThe large, curvy, yellow shape shown in the picture earlier in this section was created\nas a path with two line segments and two Bezier curve segments:", "SVG paths add flexibility by defining \"relative\" versions of the path commands,\nwhere the data for the command is given relative to the current position.\nA relative move command, for example, instead of telling ", " to move,\ntells ", " to move from the current position.  The names of the \nrelative versions of the path commands are lower case letters instead of upper case.\n\"M\u00a010,20\" means to move to the point with coordinates (10,20), while\n\"m\u00a010,20\" means to move 10 units horizontally and 20 units vertically\nfrom the current position.  Similarly, if the current position is (", "), then\nthe command \"l\u00a03,5\", where the first character is a lower case L, draws a line from (", ") to\n(", "+3,", ").", "SVG would not be a very interesting language if it could only work with\nindividual simple shapes.  For complex scenes, we want to be able to do\n", ", where objects can be constructed from\nsub-objects, and a transform can be applied to an entire complex object.\nWe need a way to group objects so that they can be treated as a unit.\nFor that, SVG has the ", " element.  The content of a ", "\nelement is a list of shape elements, which can be simple shapes or\nnested ", " elements.", "You can add style and ", " attributes to a ", " element.\nThe main point of grouping is that a group can be treated as a single\nobject.  A ", " attribute in a ", " will transform the\nentire group as a whole.  A style attribute, such as ", " or\n", ", on a ", " element will set a default value \nfor the group, replacing the current default.  Here is an example:", "The nested shapes use fill=\"none\" stroke=\"black\" stroke-width=\"2\" for the\ndefault values of the attributes.  The default can be overridden by specifying\na different value for the element, as is done for the stroke-width of the\n", " element in this example.  Setting transform=\"scale(1,\u22121)\"\nfor the group flips the entire image vertically.  I do this only because\nI am more comfortable working in a coordinate system in which y increases\nfrom bottom-to-top rather than top-to-bottom.  Here is the simple line\ndrawing of a face that is produced by this group:", "\n", "Now, suppose that we want to include multiple copies of an object in\na scene.  It shouldn't be necessary to repeat the code for drawing the object.\nIt would be nice to have something like reusable subroutines.  In fact,\nSVG has something very similar: You can define reusable objects inside a\n", " element.  An object that is defined inside ", " is\nnot added to the scene, but copies of the object can be added to the scene\nwith a single command.  For this to work, the object must have an ", " attribute\nto identify it.  For example, we could define an object that looks like a plus sign:", "A ", " element can then be used to add a copy of the plus sign\nobject to the scene.  The syntax is", "The value of the ", " attribute must be the ", " of the object,\nwith a \"#\" character added at the beginning. (Don't forget the\u00a0#.  If you leave it out,\nthe ", " element will simply be ignored.)  You can add a ", " attribute\nto the ", " element to apply a transformation to the copy of the object.  You can also apply\nstyle attributes, which will be used as default values for the attributes in the copy.  For\nexample, we can draw several plus signs with different transforms and stroke widths:", "Note that we can't change the color of the plus sign, since it already specifies\nits own stroke color.", "An object that has been defined in the ", " section can also be used\nas a sub-object in other object definitions.  This makes it possible to create\na hierarchy with multiple levels.  Here is an example from ", "\nthat defines a \"wheel\" object, then uses two copies of the wheel as sub-objects in a \n\"cart\" object:", "The SVG file goes on to add one copy of the wheel and four copies of the\ncart to the image.  The four carts have different colors and transforms.\nHere is the image:", "\n", "SVG has a number of advanced features that I won't discuss here, but I do want to\nmention one: ", ".  It is possible to animate almost any property\nof an SVG object, including geometry, style, and transforms.  The syntax for animation\nis itself fairly complex, and I will only do a few examples.  But I will tell you enough\nto produce a fairly complex hierarchical animation like the \"cart-and-windmills\"\nexample that was discussed and used as a demo in ", ".\nAn SVG version of that animation can be found in ", ".\nHere is what it looks like, although some web browsers might show it as a static\nimage instead of an animation:", "\n", "Many attributes of a shape element can be animated by adding an ", "\nelement to the content of the shape element.   Here is an example that makes a rectangle\nmove across the image from left to right:", "Note that the ", " is nested inside the ", ".\nThe ", " attribute tells which attribute of the ", "\nis being animated, in this case,\u00a0", ".  The ", " and ", " attributes\nsay that ", " will take on values from 0 to 430.  The ", " attribute is the\n\"duration\", that is, how long the animation lasts; the value \"7s\" means \"7 seconds.\"\nThe attribute ", "=\"indefinite\" means that after the animation completes,\nit will start over, and it will repeat indefinitely, that is, as long as the image is\ndisplayed.  If the ", " attribute is omitted, then after the animation\nruns once, the rectangle will jump back to its original position and remain there.\nIf ", " is replaced by ", "=\"freeze\", then after the animation runs,\nthe rectangle will br frozen in its final position, instead of jumping back to the starting\nposition.  The animation begins when the image first loads.  If you want the animation to\nstart at a later time, you can add a ", " attribute whose value gives the time\nwhen the animation should start, as a number of seconds after the image loads.", "What if we want the rectangle to move back and forth between its initial and final\nposition?  For that, we need something called ", ",\nwhich is an important idea in its own right.  The ", " and ", " attributes\nallow you to specify values only for the beginning and end of the animation.  In a keyframe\nanimation, values are specified at additional times in the middle of the animation.\nFor a keyframe animation in SVG, the ", " and ", " attributes are replaced\nby ", " and ", ".  Here is our moving rectangle example,\nmodified to use keyframes:", "The ", " attribute is a list of numbers, separated by semicolons.\nThe numbers are in the range 0 to 1, and should be in increasing order.  The first number\nshould be 0 and the last number should be 1.  A number specifies a time during the animation,\nas a fraction of the complete animation.  For example, 0.5 is a point half-way through the\nanimation, and 0.75 is three-quarters of the way.  The ", " attribute is a list\nof values, with one value for each key time.  In this case, the value for ", " is\n0 at the start of the animation, 430 half-way through the animation, and 0 again at the\nend of the animation.  Between the key times, the value for ", " is obtained by interpolating\nbetween the values specified for the key times.  The result in this case is that the rectangle\nmoves from left to right during the first half of the animation and then back from right to\nleft in the second half.", "Transforms can also be animated, but you need to use the ", "\ntag instead of ", ", and you need to add a ", " attribute to specify\nwhich transform you are animating, such as \"rotate\" or \"translate\".  Here, for example,\nis a transform animation applied to a group:", "The animation shows a growing \"tree\" made from a green triangle and a brown rectangle.\nIn the animation, the transform goes from ", "(0,0) to ", "(0.4,0.7).\nThe animation starts 3 seconds after the image loads and lasts 15 seconds.  At the end\nof the animation, the tree freezes at its final scale.  The ", " attribute\non the ", " element specifies the scale that is in effect until the animation\nstarts.  (A scale factor of 0 collapses the object to size zero, so that it is invisible.)\nYou can find this example, along with a moving rectangle and a keyframe animation, in \nthe sample file ", ". Here is the\nanimation itself.  To see the growing trees, you might have to reload this page or view\nthe image in a separate window:", "\n", "You can create animated objects in the ", " section of an SVG file,\nand you can apply animation to ", " elements.  This makes it possible\nto create hierarchical animations.  Here is a simple example:", "\n", "The example shows a rotating hexagon with a rotating square at each vertex of the\nhexagon.  The hexagon is constructed from six copies of one object, with a different rotation\napplied to each copy.  (A copy of the basic object is shown in the image to the right of the\nhexagon.)  The square is defined as an animated object with its own rotation.  It is used\nas a sub-object in the hexagon.  The rotation that is applied to the hexagon applies to the\nsquare, on top of its own built-in rotation.  That's what makes this an example of\nhierarchical animation.", "If you look back at the ", " \nexample now, you can probably see how to do the animation.  Don't forget to check out the source code,\nwhich is surprisingly short!"], "chapter_title": "Two-Dimensional Graphics", "id": 2.7}, {"section_title": "Pixels, Coordinates, and Colors", "chapter_id": "Chapter 2", "section_id": "Section 2.1", "content": ["To create a two-dimensional image, each point in the image is\nassigned a color.  A point in 2D can be identified by a pair of numerical\ncoordinates.  Colors can also\nbe specified numerically.  However, the assignment of numbers to points\nor colors is somewhat arbitrary.  So we need to spend some time\nstudying ", ", which associate\nnumbers to points, and ", ", which\nassociate numbers to colors.", "A digital image is made up of rows and columns of ", ".\nA pixel in such an image can be specified by saying which column and which row contains\nit.  In terms of coordinates, a pixel can be identified by a pair of integers giving\nthe column number and the row number.  For example, the pixel with coordinates (3,5)\nwould lie in column number 3 and row number 5.  Conventionally, columns are numbered from left\nto right, starting with zero.  Most graphics systems, including the ones we will study\nin this chapter, number rows from top to bottom, starting from zero.  Some, including\nOpenGL, number the rows from bottom to top instead.\n", "\n", "Note in particular that the pixel that is identified by a pair of\ncoordinates (", ",", ") depends on the choice of coordinate system.\nYou always need to know what coordinate system is in use before you know what\npoint you are talking about.", "Row and column numbers identify a pixel, not a point.  A pixel contains many points;\nmathematically, it contains an infinite number of points.  The goal of computer graphics is not\nreally to color pixels\u2014it is to create and manipulate images.  In some ideal\nsense, an image should be defined by specifying a color for each point, not just for\neach pixel.  Pixels are an approximation.  If we imagine that there is a true, ideal\nimage that we want to display, then any image that we display by coloring pixels is\nan approximation.  This has many implications.", "Suppose, for example, that we want to draw a line segment.  A mathematical line\nhas no thickness and would be invisible.  So we really want to draw a thick line\nsegment, with some specified width.  Let's say that the line should be \none pixel wide.  The problem is that, unless the line is horizontal or vertical,\nwe can't actually draw the line by coloring pixels.  A diagonal geometric line will cover some\npixels only partially. It is not possible to make part of a pixel black and part of it white.\nWhen you try to draw a line with black and white pixels only, the result is a jagged\nstaircase effect.  This effect is an example of something called \"aliasing.\"  Aliasing can also be seen\nin the outlines of characters drawn on the screen and in diagonal or curved boundaries between\nany two regions of different color.  (The term aliasing likely comes from the fact that\nideal images are naturally described in real-number coordinates.  When you try to represent\nthe image using pixels, many real-number coordinates will map to the same integer\npixel coordinates; they can all be considered as different names or \"aliases\" for the\nsame pixel.)", "\n", "  is a term for techniques that are designed to\nmitigate the effects of aliasing.  The idea is that when a pixel is only partially\ncovered by a shape, the color of the pixel should be a mixture of the color of the\nshape and the color of the background.  When drawing a black line on a white background,\nthe color of a partially covered pixel would be gray, with the shade of gray depending\non the fraction of the pixel that is covered by the line.  (In practice, calculating this\narea exactly for each pixel would be too difficult, so some approximate method is used.)\nHere, for example, is a geometric line, shown on the left, along with two approximations\nof that line made by coloring pixels.  The lines are greately magnified so that you can see the\nindividual pixels.  The line on the right is drawn using antialiasing, while the one in the \nmiddle is not:", "\n", "Note that antialiasing does not give a perfect image, but it can reduce the \"jaggies\" that \nare caused by aliasing (at least when it is viewed on a normal scale).", "There are other issues involved in mapping real-number coordinates to pixels.\nFor example, which point in a pixel should correspond to integer-valued coordinates\nsuch as (3,5)?  The center of the pixel?  One of the corners of the pixel?\nIn general, we think of the numbers as referring to the top-left corner of the pixel.\nAnother way of thinking about this is to say that integer coordinates refer to the\nlines between pixels, rather than to the pixels themselves.  But that still\ndoesn't determine exactly which pixels are affected when a geometric shape is drawn.\nFor example, here are two lines drawn using HTML canvas graphics,\nshown greatly magnified.  The lines were specified to be colored black with a\none-pixel line width:", "\n", "The top line was drawn from the point (100,100) to the point (120,100).  In\ncanvas graphics, integer coordinates corresponding to the lines between pixels, \nbut when a one-pixel line is drawn, it\nextends one-half pixel on either side of the infinitely thin geometric line.  So for the top line,\nthe line as it is drawn lies half\nin one row of pixels and half in another row.  The graphics system, which uses\nantialiasing, ", " the line by coloring both\nrows of pixels gray.  The bottom line was drawn from the point (100.5,100.5) to\n(120.5,120.5).  In this case, the line lies exactly along one line of pixels,\nwhich gets colored black.  The gray pixels at the ends of the bottom line have to do with\nthe fact that the line only extends halfway into the pixels at its endpoints.\nOther graphics systems might render the same lines differently.", "The following interactive demo lets you experiment with\npixels and antialiasing.\n\n(Note that in any of the interactive demos that accompany this book, you can click\nthe question mark icon in the upper left for more information about how to use it.)", "\n", "\n", "All this is complicated further by the fact that pixels aren't what they used to\nbe.   Pixels today are smaller!  The resolution of a display device can be measured\nin terms of the number of pixels per inch on the display, a quantity referred to\nas PPI (pixels per inch) or sometimes DPI (dots per inch).  Early screens tended to have\nresolutions of somewhere close to 72 PPI.  At that resolution, and at a typical viewing\ndistance, individual pixels are clearly visible.  For a while, it seemed like most\ndisplays had about 100 pixels per inch, but high resolution displays today can have\n200, 300 or even 400 pixels per inch.  At the highest resolutions, individual\npixels can no longer be distinguished.", "The fact that pixels come in such a range of sizes is a problem if we use\ncoordinate systems based on pixels.  An image created assuming that there are\n100 pixels per inch will look tiny on a 400 PPI display.  A one-pixel-wide line\nlooks good at 100 PPI, but at 400 PPI, a one-pixel-wide line is probably\ntoo thin.", "In fact, in many graphics systems, \"pixel\" doesn't really refer to the \nsize of a physical pixel.  Instead, it is just another unit of measure, which is\nset by the system to be something appropriate.  (On a desktop system, a pixel\nis usually about one one-hundredth of an inch.  On a smart phone, which is\nusually viewed from a closer distance, the value might be closer to 1/160 inch.\nFurthermore, the meaning of a pixel as a unit of measure can change when,\nfor example, the user applies a magnification to a web page.)", "Pixels cause problems that have not been completely solved.  Fortunately, they\nare less of a problem for ", ", which is mostly what we\nwill use in this book.  For vector graphics, pixels only become an issue during\n", ", the step in which a vector image is converted\ninto pixels for display.  The vector image itself can be created using any\nconvenient coordinate system.  It represents an idealized, resolution-independent\nimage.  A rasterized image is an approximation of that ideal image, but how to\ndo the approximation can be left to the display hardware.", "When doing 2D graphics, you are given a rectangle in which you want to\ndraw some ", ".\nPrimitives are specified using some coordinate system on the rectangle.\nIt should be possible to select a coordinate system that is appropriate\nfor the application.  For example, if the rectangle represents a floor\nplan for a 15 foot by 12 foot room, then you might want to use a\ncoordinate system in which the unit of measure is one foot and the\ncoordinates range from 0 to 15 in the horizontal direction and 0 to\n12 in the vertical direction.  The unit of measure in this case is feet\nrather than pixels, and one foot can correspond to many pixels in the\nimage.  The coordinates for a pixel will, in general, be real numbers\nrather than integers.  In fact, it's better to forget about pixels\nand just think about points in the image.  A point will have a pair\nof coordinates given by real numbers.", "To specify the coordinate system on a rectangle, you just have\nto specify the horizontal coordinates for the left and right\nedges of the rectangle and the vertical coordinates for the\ntop and bottom.  Let's call these values ", ",\n", ", ", ", and ", ".  Often, they are\nthought of as ", ", ", ", ", ", and ", ",\nbut there is no reason to assume that, for example, ", "\nis less than ", ".  We might want a coordinate system in\nwhich the vertical coordinate increases from bottom to top instead\nof from top to bottom.  In that case, ", " will correspond to\nthe maximum ", "-value instead of the minimum value.", "To allow programmers to specify the coordinates system that\nthey would like to use, it would be good to have a subroutine such as", "The graphics system would then be responsible for automatically\n ", " the  \ncoordinates from the specfiied coordinate system into pixel coordinates.\nSuch a subroutine might not be available, so it's useful to see how the transformation\nis done by hand.  Let's consider the general case.  Given coordinates for a point in \none coordinate system, we want to find the coordinates for the same point in a second \ncoordinate system.  (Remember that a coordinate system is just a way of assigning numbers\nto points.  It's the points that are real!)  Suppose that the horizontal and vertical\nlimits are ", ", ", ", ", ", and ", " for\nthe first coordinate system, and are ", ", ", ", ", ", \nand ", " for the second.  Suppose that a point has coordinates (", ")\nin the first coordinate system.  We want to find the coordinates (", ")\nof the point in the second coordinate system", "\n", "Formulas for ", " and ", " are then given by", "The logic here is that ", " is located at a certain fraction of the distance from ", " to\n", ".  That fraction is given by", "The formula for ", " just says that ", " should lie at the same fraction of the distance\nfrom ", " to ", ".  You can also check the formulas by testing that\nthey work when ", " is equal to ", " or to ", ", and when\n", " is equal to ", " or to ", ".", "As an example, suppose that we want to transform some real-number coordinate system\nwith limits ", ", ", ", ", ", and ", " into pixel\ncoordinates that range from 0 at left to 800 at the right and from 0 at the top\n600 at the bottom.  In that case, ", " and ", " are zero, and \nthe formulas become simply\n", "Of course, this gives ", " and ", " as real numbers, and they will have\nto be rounded or truncated to integer values if we need integer coordinates for pixels.\nThe reverse transformation\u2014going from pixel coordinates to real number coordinates\u2014is\nalso useful.  For example, if the image is displayed on a computer screen, and you want to\nreact to mouse clicks on the image, you will probably get the mouse coordinates in terms\nof integer pixel coordinates, but you will want to transform those pixel coordinates into \nyour own chosen coordinate system.", "In practice, though, you won't usually have to do the transformations yourself, since most\ngraphics APIs provide some higher level way to specify transforms.  We will talk more about\nthis in ", ".", "The ", " of a rectangle is the ratio of its width to its height.\nFor example an aspect ratio of 2:1 means that a rectangle is twice as wide as it is tall,\nand an aspect ratio of 4:3 means that the width is 4/3 times the height.  Although aspect ratios\nare often written in the form ", ":", ", I will use the term to refer to the\nfraction ", ".  A square has aspect ratio equal to\u00a01.  A rectangle with aspect\nratio 5/4 and height 600 has a width equal to 600*(5/4), or 750.", "A coordinate system also has an aspect ratio.  If the horizontal and vertical limits for\nthe coordinate system are ", ", ", ", ", ", and ", ", as \nabove, then the aspect ratio is the absolute value of", "If the coordinate system is used on a rectangle with the same aspect ratio, then when viewed in\nthat rectangle, one unit in the horizontal direction will have the same apparent length as a unit in the\nvertical direction.  If the aspect ratios don't match, then there will be some distortion.\nFor example, the shape defined by the equation ", "\u00a0+", "\u00a0=\u00a09\nshould be a circle, but that will only be true if the aspect ratio of the (", ",", ")\ncoordinate system matches the aspect ratio of the drawing area.", "\n", "It is not always a bad thing to use different units of length in the vertical and horizontal \ndirections.  However, suppose that you want to use coordinates with limits ", ", ", ", \n", ", and ", ", and that you do want to preserve the aspect ratio.  In that case,\ndepending on the shape of the display rectangle, you might have to adjust the values either of\n", " and ", " or of ", " and ", " to make the aspect ratios match:\n", "\n", "We will look more deeply into geometric transforms later in the chapter, and at that time,\nwe'll see some program code for setting up coordinate systems.", "We are talking about the most basic foundations of computer graphics.  One of those is\ncoordinate systems.  The other is color.  Color is actually a surprisingly complex topic.\nWe will look at some parts of the topic that are most relevant to computer graphics\napplications.", "The colors on a computer screen are produced as combinations of red, green, and blue light.\nDifferent colors are produced by varying the intensity of each type of light.  A color can be\nspecified by three numbers giving the intensity of red, green, and blue in the color.\nIntensity can be specified as a number in the range zero, for minimum intensity, to one, for\nmaximum intensity.  This method of specifying color is called the ", ",\nwhere RGB stands for Red/Green/Blue.  For example, in the RGB color model, the number triple \n(1,\u00a00.5,\u00a00.5) represents the color obtained by setting red to full intensity, while \ngreen and blue are set to half intensity. The red, green, and blue values for a color\nare called the ", " of that color \nin the RGB color model.", "Light is made up of waves with a variety of wavelengths. \nA pure color is one for which all the light has the same wavelength,\nbut in general, a color can contain many wavelengths\u2014mathematically,\nan infinite number.  How then can we represent all colors by combining just red, green, and\nblue light?  In fact, we can't quite do that.", "You might have heard that combinations of the three basic, or \"primary,\" colors are sufficient\nto represent all colors, because the human eye has three kinds of color sensors that detect red,\ngreen, and blue light.  However, that is only an approximation.  The eye does contain three\nkinds of color sensor.  The sensors are called \"cone cells.\"\nHowever, cone cells do not respond exclusively to red, green, and blue light.  Each kind\nof cone cell responds, to a varying degree, to wavelengths of light in a wide range.  A given\nmix of wavelengths will stimulate each type of cell to a certain degree, and the intensity of\nstimulation determines the color that we see.  A different mixture of wavelengths that stimulates\neach type of cone cell to the same extent will be perceived as the same color.  So a perceived\ncolor can, in fact, be specified by three numbers giving the intensity of stimulation of\nthe three types of cone cell. However, it is not possible to produce all possible patterns of\nstimulation by combining just three basic colors, no matter how those colors are chosen.  \nThis is just a fact about the way our eyes actually work; it might have been different.\nThree basic colors can produce a reasonably large fraction of the set of perceivable colors,\nbut there are colors that you can see in the world that you will never see on your computer\nscreen.  (This whole discussion only applies to people who actually have three kinds of\ncone cell.  Color blindness, where someone is missing one or more kinds of cone cell, is\nsurprisingly common.)", "The range of colors that can be produced by a device such as a computer screen is called\nthe ", " of that device.  Different computer screens can have different\ncolor gamuts, and the same RGB values can produce somewhat different colors on different screens.\nThe color gamut of a color printer is noticeably different\u2014and probably\nsmaller\u2014than the color gamut of a screen, which explain why a printed image probably\ndoesn't look exactly the same as it did on the screen.  (Printers, by the way, make colors\ndifferently from the way a screen does it.  Whereas a screen combines light to make a color, \na printer combines inks or dyes.  Because of this difference, colors meant for printers are often\nexpressed using a different set of basic colors.  A common color model for printer colors\nis CMYK, using the colors cyan, magenta, yellow, and black.)", "In any case, the most common color model for computer graphics is RGB.  RGB colors are most\noften represented using 8 bits per color component, a total of 24 bits to represent a color.\nThis representation is sometimes called \"24-bit color.\"\nAn 8-bit number can represent 2", ", or 256, different values, which we can take to\nbe the positive integers from 0 to 255. A\u00a0color is then specified as a triple of integers\n(r,g,b) in that range.", "This representation works well because 256 shades of red, green, and\nblue are about as many as the eye can distinguish.  In applications where images are processed\nby computing with color components, it is common to use additional bits per color component,\nto avoid visual effects that might occur due to rounding errors in the computations.\nSuch applications might use a 16-bit integer or even a 32-bit floating point value for\neach color component.  On the other hand, sometimes fewer bits are used.  For example, one\ncommon color scheme uses 5 bits for the red and blue components and 6 bits for the green\ncomponent, for a total of 16 bits for a color.  (Green gets an addition bit because\nthe eye is more sensitive to green light than to red or blue.)  This \"16-bit color\" saves memory\ncompared to 24-bit color and was more common when memory was more expensive.", "There are many other color models besides RGB.  RGB is sometimes criticized as being unintuitive.\nFor example, it's not obvious to most people that yellow is made of a combination of red and green.\nThe closely related color models ", " \nand ", " describe the same set of colors as RGB, but attempt\nto do it in a more intuitive way.  (HSV is sometimes called HSB, with the \"B\"\nstanding for \"brightness.\"  HSV and HSB are exactly the same model.)", "The \"H\" in these models stands for \"hue,\" a basic spectral color.\nAs H increases, the color changes from red to yellow to green to cyan to blue to magenta, and then\nback to red.  The value of H is often taken to range from 0 to 360, since the colors can be thought\nof as arranged around a circle with red at both 0 and 360 degrees.", "The \"S\" in HSV and HSL stands for \"saturation,\"\nand is taken to range from 0 to 1.  A saturation of 0 gives a shade of gray (the shade depending on\nthe value of V or L). A saturation of 1 gives a \"pure color,\" and decreasing the saturation is\nlike adding more gray to the color.  \"V\"\u00a0stands for \"value,\" and \"L\" stands for \"lightness.\"\nThey determine how bright or dark the color is.  The main difference is that in the HSV model, the\npure spectral colors occur for V=1, while in HSL, they occur for L=0.5.", "Let's look at some colors in the HSV color model.   The illustration below shows\ncolors with a full range of H-values, for S and V equal to 1 and to 0.5.  Note that for S=V=1, you\nget bright, pure colors.  S=0.5 gives paler, less saturated colors.  V=0.5 gives darker colors.", "\n", "It's probably easier to understand color models by looking at some actual colors\nand how they are represented. Here is an interactive demo that \nlet's you do that for the RGB and HSV color models:\n", "\n", "\n", "Often, a fourth component is added to color models.  The fourth component is called\n", ", and color models that use it are\nreferred to by names such as RGBA and HSLA.  Alpha is not a color as such.  It is usually used\nto represent transparency.  A color with maximal alpha value is fully opaque; that is, it is\nnot at all transparent.  A color with alpha equal to zero is completely transparent and therefore\ninvisible.  Intermediate values give translucent, or partly transparent, colors.\nTransparency determines what happens when you draw with one color (the foreground color) \non top of another color (the background color).  If the foreground color is fully opaque, it \nsimply replaces the background color.  If the foreground color is partly transparent, then\nthen it is blended with the background color.  Assuming that the alpha component ranges from\n0\u00a0to\u00a01, the color that you get can be computed as", "This computation is done separately for the red, blue, and green color components.\nThis is called ", ".\nThe effect is like viewing the background through colored glass; the color of the glass\nadds a tint to the background color.  This type of blending is not the only possible use\nof the alpha component, but it is the most common.", "An RGBA color model with 8 bits per component uses a total of 32 bits to represent a color.\nThis is a convenient number because integer values are often represented using 32-bit values.\nA 32-bit integer value can be interpreted as a 32-bit RGBA color.  How the color components are\narranged within a 32-bit integer is somewhat arbitrary.  The most common layout is to store\nthe alpha component in the eight high-order bits, followed by red, green, and blue.  (This should\nprobably be called ARGB color.)  However, other layouts are also in use."], "chapter_title": "Two-Dimensional Graphics", "id": 2.1}, {"section_title": "HTML Canvas Graphics", "chapter_id": "Chapter 2", "section_id": "Section 2.6", "content": ["Most modern web browsers support a 2D graphics ", " that can be used\nto create images on a web page.  The API is implemented using ", ", the client-side programming \nlanguage for the web.  I won't cover the JavaScript language in this section.  To understand the \nmaterial presented here, you don't need to know much about it.  Even if you\nknow nothing about it at all, you can learn something about its 2D graphics API and\nsee how it is similar to, and how it differs from, the Java API presented in the\n", ".  (For a short review\nof JavaScript, see ", " in ", ".)", "The visible content of a web page is made up of \"elements\" such\nas headlines and paragraphs.  The content is specified using the ", " language.\nA \"canvas\" is an HTML element. It appears on the page as a blank rectangular area which can\nbe used as a drawing surface by what I am calling the \"", "\" graphics API.\nIn the source code of a web page, a canvas element is created with code of the form", "The ", " and ", " give the size of the drawing area, in pixels.  The\n", " is an identifier that can be used to refer to the canvas in JavaScript.", "To draw on a canvas, you need a graphics context.  A graphics context is an object that\ncontains functions for drawing shapes.  It also contains variables that record the current graphics \nstate, including  things like the current drawing color, transform, and font.  Here, I will \ngenerally use ", " as the name of the variable that refers to the graphics context, \nbut the variable name is, of course, up to the programmer.  This graphics context plays the same role in \nthe canvas API that a variable of type ", " plays in Java.\nA typical starting point is\n", "The first line gets a reference to the canvas element on the web page, using its ", ".\nThe second line creates the graphics context for that canvas element.  (This code will\nproduce an error in a web browser that doesn't support canvas, so you might add some error\nchecking such as putting these commands inside a ", " statement.)", "Typically, you will store the canvas graphics context in a global variable\nand use the same graphics context throughout your program.  This is in contrast\nto Java, where you typically get a new ", " context\neach time the ", "() method is called, and that new context\nis in its initial state with default color and stroke\nproperties and with no applied transform.  When a graphics context \ncontext is global, changes made to the state in one function\ncall will carry over to subsequent function calls, unless you do something to limit\ntheir effect.  This can actually lead to a fairly common type of bug: For example, \nif you apply a 30-degree rotation in a function, those rotations will ", "\neach time the function is called, unless you do something to undo the previous rotation\nbefore applying the next rotation.", "The rest of this section will be mostly concerned with describing what you can do with\na canvas graphics context.  But here, for the record, is the complete source code for\na very minimal web page that uses canvas graphics:\n", "For a more complete, though still minimal, example, look at the sample page\n", ".  (You should look at the page\nin a browser, but you should also read the source code.)  This example shows\nhow to draw some basic shapes using canvas graphics, and you can use it as\na basis for your own experimentation.  There are also three more advanced\n\"starter\" examples:  ", " adds\nsome utility functions for drawing shapes and setting up a coordinate system;\n", " adds animation and includes\na simple ", " example; and\n", " shows how to respond to keyboard\nand mouse events.", "The default coordinate system on a canvas is the usual: The unit of measure is one pixel;\n(0,0) is at the upper left corner; the ", "-coordinate increases to the right;\nand the ", "-coordinate increases downward.   The range of ", " and ", "\nvalues are given by the ", " and ", " properties of the ", "\nelement.  The term \"pixel\" here for the unit of measure is not really correct.  \nProbably, I should say something like \"one nominal pixel.\"\nThe unit of measure is one pixel at typical desktop resolution with no magnification.\nIf you apply a magnification to a browser window, the unit of measure gets stretched.\nAnd on a high-resolution screen, one unit in the default coordinate system might \ncorrespond to several actual pixels on the display device.", "The canvas API supports only a very limited set of basic shapes. In fact, the\nonly basic shapes are rectangles and text.  Other shapes must be created as paths.\nShapes can be ", " and \n", ".  That includes text: When you stroke a string of\ntext, a pen is dragged along the outlines of the characters; when you fill a string,\nthe insides of the characters are filled.  It only really makes sense to stroke text\nwhen the characters are rather large.  Here are the functions for drawing\nrectangles and text, where ", " refers to the object that represents\nthe graphics context:", "A path can be created using functions in the graphics context.  The context keeps track of\na \"current path.\"  In the current version of the API, paths are not represented by objects,\nand there is no way to work with more than one path at a time or to keep a copy of a path\nfor later reuse.  Paths can contain lines, ", ", and circular arcs.\nHere are the most common functions for working with paths:", "Creating a curve with these commands does not draw anything.  To get something visible\nto appear in the image, you must fill or stroke the path.", "The commands ", "() and ", "() are used to fill\nand to stroke the current path.  If you fill a path that has not been closed, the fill\nalgorithm acts as though a final line segment had been added to close the path.\nWhen you stroke a shape, it's the center of the virtual pen that moves along the path.\nSo, for high-precision canvas drawing, it's common to\nuse paths that pass through the centers of pixels rather than through their corners.\nFor example, to draw a line that extends from the pixel with coordinates (100,200) to\nthe pixel with coordinates (300,200), you would actually stroke the geometric line\nwith endpoints (100.5,200.5) and (100.5,300.5).  We should look at some examples.\nIt takes four steps to draw a line:", "Remember that the line remains as part of the current path until the\nnext time you call ", "().  Here's how to draw a filled,\nregular octagon centered at (200,400) and with radius 100:", "The function ", "() can be used to draw a circle, with a start \nangle of 0 and an end angle of ", ".  Here's a filled circle with radius \n100, centered at 200,300:", "To draw just the outline of the circle, use ", "()\nin place of ", "().  You can apply both operations to the same path.\nIf you look at the details of ", "(),\nyou can see how to draw a wedge of a circle:", "There is no way to draw an oval that is not a circle, except by using \ntransforms.  We will cover that later in this section.  But JavaScript has\nthe interesting property that it is possible to add new functions and \nproperties to an existing object.  The sample program\n", " shows how to\nadd functions to a graphics context for drawing lines, ovals, and\nother shapes that are not built into the API.", "Attributes such as line width that affect the visual appearance of\nstrokes and fills are stored as properties of the graphics context.\nFor example, the value of ", " is a number that\nrepresents the width that will be used for strokes.  (The width is \ngiven in pixels for the default coordinate system, but it is subject \nto transforms.)  You can change the line width by assigning a value\nto this property:", "The change affects subsequent strokes.  You can also read the current\nvalue:", "The property ", " controls the appearance of\nthe endpoints of a stroke.  It can be set to\n\"round\", \"square\", or \"butt\".  The quotation marks are part of the value.  For example,", "Similarly, ", " controls\nthe appearance of the point where one segment of a stroke joins another\nsegment; its possible values are \"round\", \"bevel\", or \"miter\".  (Line\nendpoints and joins were discussed in ", ".)", "Note that the values for ", " and ", "\nare strings.  This is a somewhat unusual aspect of the API.  Several other properties\nof the graphics context take values that are strings, including the properties that\ncontrol the colors used for drawing and the font that is used for drawing\ntext.", "Color is controlled by the values of the properties ", "\nand ", ".  The graphics context maintains separate styles\nfor filling and for stroking.  A\u00a0solid color for stroking or filling is specified\nas a string.  Valid color strings are ones that can be used in ", ", the language\nthat is used to specify colors and other style properties of elements on web pages.\nMany solid colors can be specified by their names, such as \"red\", \"black\", and\n\"beige\".  An ", " can be specified as a string of the\nform \"rgb(r,g,b)\", where the parentheses contain three numbers in the range\n0 to 255 giving the red, green, and blue components of the color.  Hexadecimal color\ncodes are also supported, in the form \"#XXYYZZ\" where XX, YY, and ZZ are two-digit\nhexadecimal numbers giving the RGB color components.  For example,", "The style can actually be more complicated\nthan a simple solid color:  ", " and \n", " are also supported.  As an \nexample, a gradient can be created with a series of steps such as", "The first line creates a linear gradient that will vary in color along\nthe line segment from the point (420,420) to the point (550,200).\nColors for the gradient are specified by the ", " function:\nthe first parameter gives the fraction of the distance from the initial\npoint to the final point where that color is applied, and the second is a string\nthat specifies the color itself.  A color stop at 0 specifies\nthe color at the initial point; a color stop at 1 specifies the color\nat the final point.  Once a gradient has been created, it can be used both\nas a fill style and as a stroke style in the graphics context.", "Finally, I note that the font that is used for drawing text is the\nvalue of the property ", ".  The value is a string\nthat could be used to specify a font in ", ".\nAs such, it can be fairly complicated, but the simplest versions\ninclude a font-size (such as ", " or ", ") and a font-family\n(such as ", ", ", ", ", ", or the name of any font\nthat is accessible to the web page).\nYou can add ", " or ", " or both to the front of the string.\nSome examples:\n", "The default is \"10px sans-serif,\" which is usually too small.  Note that text, like all drawing, is\nsubject to coordinate transforms.  Applying a scaling operation changes the\nsize of the text, and a negative scaling factor can produce mirror-image text.", "A graphics context has three basic functions for modifying the current transform\nby scaling, rotation, and translation.  There are also functions that will compose\nthe current transform with an arbitrary transform and for completely\nreplacing the current transform:", "Note that there is no ", ", but you can apply a shear as\na general transform.  For example, for a horizontal shear with shear factor\u00a00.5, use", "To implement hierarchical modeling, as discussed in ", ", \nyou need to be able to save\nthe current transformation so that you can restore it later.  Unfortunately,\nno way is provided to read the current transformation from a canvas graphics\ncontext.  However, the graphics context itself keeps a stack of transformations\nand provides methods for pushing and popping the current transformation.  In fact,\nthese methods do more than save and restore the current transformation.  They actually\nsave and restore almost the entire state of the graphics context, including properties\nsuch as current colors, line width, and font (but not the current path):", "Using these methods, the basic setup for drawing an object with a modeling\ntransform becomes:", "Note that if drawing the object includes any changes to attributes\nsuch as drawing color, those changes will be also undone by the call to ", "().\nIn hierarchical graphics, this is usually what you want, and it eliminates the\nneed to have extra statements for saving and restoring things like color.", "To draw a hierarchical model, you need to traverse a ", ", either\nprocedurally or as a data structure.  It's pretty much the same as in Java.\nIn fact, you should see that the basic concepts that you learned about\ntransformations and modeling carry over to the canvas graphics API.  Those\nconcepts apply very widely and even carry over to 3D graphics APIs, with\njust a little added complexity.  The demo program\n", " from ", "\nimplements hierarchical modeling using the 2D canvas API.", "Now that we know how to do transformations, we can see how to draw an oval\nusing the canvas API.  Suppose that we want an oval with center at (", "),\nwith horizontal radius ", " and with vertical radius ", ".\nThe idea is to draw a circle of radius 1 with center at (0,0), then transform\nit.  The circle needs to be scaled by a factor of ", " horizontally and\n", " vertically.  It should then be translated to move its center from\n(0,0) to (", ").  We can use ", "() and ", "()\nto make sure that the transformations only affect the circle.  Recalling that\nthe order of transforms in the code is the opposite of the order in which they\nare applied to objects, this becomes:", "Note that the current path is ", " affected by the\ncalls to ", "() and ", "().  So,\nin the example, the oval-shaped path is not discarded when\n", "() is called. When ", "() is\ncalled at the end, it is the oval-shaped path that is stroked.  On the\nother hand, the line width that is used for the stroke is not affected\nby the scale transform that was applied to the oval.  Note that if\nthe order of the last two commands were reversed, then the line width\nwould be subject to the scaling.", "There is an interesting point here about transforms and paths.  In the HTML canvas\nAPI, the points that are used to create a path are transformed by the\ncurrent transformation before they are saved.  That is, they are saved\nin pixel coordinates.  Later, when the path is stroked or filled, the current\ntransform has no effect on the path (although it can affect,\nfor example, the line width when the path is stroked).  In particular,\nyou can't make a path and then apply different transformations.  For example,\nyou can't make an oval-shaped path, and then use it to draw several ovals\nin different positions.  Every time you draw the oval, it will be in the\nsame place, even if different translation transforms are applied to\nthe graphics context.", "The situation is different in Java, where the coordinates that are stored\nin the path are the actual numbers that are used to specify the path,\nthat is, the ", ".  When the path is stroked or\nfilled, the transformation that is in effect at that time is applied\nto the path.  The path can be reused many times\nto draw copies with different transformations.  This comment is offered as an\nexample of how APIs that look very similar can have subtle differences.", "In ", ", we looked at the sample program\n", ", which uses a\n", " both to implement an\n", " and to allow direct manipulation of\nthe colors of individual pixels.  The same ideas can be applied\nin HTML canvas graphics, although the way it's done is a little different.\nThe sample web application ", "\ndoes pretty much the same thing as the Java program (except for the\nimage filters).", "Here\nis a live demo \nversion of the program that has the same functionality.\nYou can try it out to see how the various drawing tools work.  Don't\nforget to try the \"Smudge\" tool! (It has to be applied to shapes that\nyou have already drawn.)", "\n", "\n", "For JavaScript, a web page is represented as a data structure, defined\nby a standard called the ", ", or Document Object model.\nFor an off-screen canvas, we can use a ", " that is not part of\nthat data structure and therefore is not part of the page.\nIn JavaScript, a ", "\ncan be created with the function call ", "(\"canvas\").\nThere is a way to add this kind of dynamically created canvas to the\nDOM for the web page, but it can be used as an off-screen canvas without doing so.\nTo use it, you have to set its width and height properties, and you\nneed a graphics context for drawing on it.  Here, for example, is\nsome code that creates a 640-by-480 canvas, gets a graphics\ncontext for the canvas, and fills the whole canvas with white:", "The sample program lets the user drag the mouse on the canvas to\ndraw some shapes. The off-screen canvas holds the official copy of\nthe picture, but it is not seen by the user.  There is also an\non-screen canvas that the user sees.  The off-screen canvas is copied \nto the on-screen canvas whenever the picture is modified.  \nWhile the user is dragging the mouse to\ndraw a line, oval, or rectangle, the new shape is actually\ndrawn on-screen, over the contents of the off-screen canvas. It is\nonly added to the off-screen canvas when the user finishes the\ndrag operation. For the other tools, changes are made directly\nto the off-screen canvas, and the result is then copied to the\nscreen.  This is an exact imitation of the Java program.", "(The demo version shown above \nactually uses a somewhat different technique to accomplish the\nsame thing.  It uses two on-screen canvases, one located exactly\non top of the other.  The lower canvas holds the actual image.\nThe upper canvas is completely transparent, except when the\nuser is drawing a line, oval, or rectangle.  While the user\nis dragging the mouse to draw such a shape, the new shape\nis drawn on the upper canvas, where it hides the part of\nthe lower canvas that is beneath the shape.  When the user\nreleases the mouse, the shape is added to the lower canvas\nand the upper canvas is cleared to make it completely transparent\nagain.  Again, the other tools operate directly on the lower\ncanvas.)", "The \"Smudge\" tool in the ", " \nand demo is implemented by computing with the color component values of pixels in the image.\nThe implementation requires some way to read the colors of pixels in a canvas.  That can be \ndone with the function ", "(", "), where ", " is a 2D graphics\ncontext for the canvas.  The function reads the colors of a rectangle of pixels, where (", ") is\nthe upper left corner of the rectangle, ", " is its width, and ", " is its height.  The\nparameters are always expressed in pixel coordinates.  Consider, for example", "This returns the color data for a 20-by-10 rectangle in the upper left corner of the canvas.\nThe return value, ", ", is an object with properties ", ",\n", ", and ", ".  The ", " and ", " give the number of\nrows and columns of pixels in the returned data.  (According to the documentation, on a high-resolution \nscreen, they might not be the same as the width and height in the function call.  The data can be \nfor real, physical pixels on the display device, not the \"nominal\" pixels that are used in the pixel \ncoordinate system on the canvas. There might be several device pixels for each nominal pixel.\nI'm not sure whether this can really happen.)", "The value of ", " is an array, with four array elements for each pixel.  The four\nelements contain the red, blue, green, and alpha color components of the pixel, given as integers\nin the range 0 to 255.  For a pixel that lies outside the canvas, the four component values will\nall be zero.  The array is a value of type ", " whose elements are\n8-bit unsigned integers limited to the range 0 to 255.  This is one of JavaScript's\n", " datatypes, which can only hold values of a specific numerical type.\nAs an example, suppose that you just want to read the RGB color of one pixel, at coordinates (", ").\nYou can set", "Then the RGB color components for the pixel are R = ", "[0],\nG = ", "[1], and B = ", "[2].", "The function ", "(", ",", ",", ") is\nused to copy the colors from an image data object into a canvas, placing it into\na rectangle in the canvas with upper left corner at (", ").  The ", "\nobject can be one that was returned by a call to ", ", possibly\nwith its color data modified.  Or you can create a blank image data object by\ncalling ", "(", ") and fill it with data.", "Let's consider the \"Smudge\" tool in the sample program. When the user clicks the\nmouse with this tool, I use ", " to get the color data from a \n9-by-9 square of pixels surrounding the mouse location.  ", " is the graphics\ncontext for the canvas that contains the image.  Since I want to do real-number\narithmetic with color values, I copy the color components into another typed array,\none of type ", ", which can hold 32-bit floating point numbers.\nHere is the function that I call to do this:", "The floating point array, ", ", will be used for computing new\ncolor values for the image as the mouse moves.  The color values\nfrom this array will be copied into the image data object, ", ", \nwhich will them be used to put the color values into the image.  This is done in\nanother function, which is called for each point that is visited as the user\ndrags the Smudge tool over the canvas:", "In this function, a new color is computed for each pixel in a 9-by-9 square of\npixels around the mouse location.  The color is replaced by a weighted average\nof the current color of the pixel and the color of the corresponding pixel in the\n", ".  At the same time, the color in ", "\nis replaced by a similar weighted average.", "It would be worthwhile to try to understand this example to see how pixel-by-pixel\nprocessing of color data can be done.  See the \n", "\nof the example for more details.", "For another example of pixel manipulation, we can look at \nimage filters that modify an image by replacing the color of each pixel with\na weighted average of the color of that pixel and the 8 pixels that\nsurround it. Depending on the weighting factors that are used, the\nresult can be as simple as a slightly blurred version of the image, or\nit can something more interesting.", "Here is \nan an\ninteractive demo that lets you apply several different image filters to\na variety of images:", "\n", "\n", "The filtering operation in the demo uses the image data functions\n", ", ", ", and ", "\nthat were discussed above.  Color data from the entire image is obtained\nwith a call to ", ".  The results of the averaging \ncomputation are placed in a new image data object, and the resulting\nimage data is copied back to the image using ", ".", "The remaining question is, where do the original images come\nfrom, and how do they get onto the canvas in the first place?\nAn image on a web page is specified by an element in the web page\nsource such as", "The ", " attribute specifies the URL from which the image is\nloaded.  The optional ", " can be used to reference the image\nin JavaScript.  In the script,", "gets a reference to the object that represents the image in the\ndocument structure.  Once you have such an object, you can use it to\ndraw the image on a canvas.  If ", " is a graphics context\nfor the canvas, then", "draws the image with its upper left corner at (", ").  Both\nthe point (", ") and the image itself are transformed by any\ntransformation in effect in the graphics context.  This will draw\nthe image using its natural width and height (scaled by the transformation,\nif any).  You can also specify the width and height of the rectangle\nin which the image is drawn:", "With this version of ", ", the image is scaled to fit\nthe specified rectangle.", "Now, suppose that the image you want to draw onto the canvas is not\npart of the web page?  In that case, it is possible to load the image dynamically.\nThis is much like making an off-screen canvas, but you are making\nan \"off-screen image.\"  Use the ", " object to create an\n", " element:", "An ", " element needs a ", " attribute that\nspecifies the URL from which it is to be loaded.  For example,", "As soon as you assign a value to the ", " attribute, the browser\nstarts loading the image.  The loading is done asynchronously; that is,\nthe computer continues to execute the script without waiting for the\nload to complete.  This means that you can't simply draw the image on\nthe line after the above assignment statement:  The image is very likely\nnot done loading at that time.  You want to draw the image after it has\nfinished loading.  For that to happen, you need to assign a function to the image's ", "\nproperty before setting the ", ". That function will\nbe called when the image has been fully loaded.  Putting this together, here is a simple \nJavaScript function for loading an image from a specified URL and drawing it on a canvas\nafter it has loaded:", "A similar technique is used to load the images in the filter demo.", "There is one last mystery to clear up.  When discussing the use of an off-screen\ncanvas in the ", " example earlier in this section, I noted that\nthe contents of the off-screen canvas have to be copied to the main canvas,\nbut I didn't say how that can be done.  In fact, it is done using ", ".\nIn addition to drawing an image onto a canvas, ", " can be used to draw the contents of\none canvas into another canvas.  In the sample program, the command", "is used to draw the off-screen canvas to the main canvas.  Here, ", "\nis a graphics context for drawing on the main canvas, and ", " is the object\nthat represents the off-screen canvas."], "chapter_title": "Two-Dimensional Graphics", "id": 2.6}, {"section_title": "Java Graphics2D", "chapter_id": "Chapter 2", "section_id": "Section 2.5", "content": ["In the rest of this chapter, we look at specific implementations\nof two-dimensional graphics.  There are a few new ideas here,\nbut mostly you will see how the general concepts that we have covered are used in several\nreal graphics systems.", "In this section, our focus is on the Java programming language.\nJava remains one of the most popular programming languages.  Its\nstandard desktop version includes a sophisticated 2D graphics API,\nwhich is our topic here.  Before reading this section, you should\nalready know the basics of Java programming. But even if you don't,\nyou should be able to follow most of the discussion of the\ngraphics API itself.  (See ", " in ", " for a very basic\nintroduction to Java.)", "The original version of Java had a much smaller graphics API.  It was\ntightly focused on pixels, and it used only integer coordinates.  The API\nhad subroutines for stroking and filling a variety of basic shapes,\nincluding lines, rectangles, ovals, and polygons (although Java uses the\nterm ", " instead of ", ").  Its specification\nof the meaning of drawing operations was very precise on the pixel\nlevel.  Integer coordinates are defined to refer to the lines\nbetween pixels.  For example, a 12-by-8 pixel grid has\n", "-coordinates from 0 to 12 and ", "-coordinates from\n0 to 8, as shown below.  The lines between pixels are numbered,\nnot the pixels.", "\n", "The command ", "(3,2,5,3)\nfills the rectangle with upper left corner at (3,2), with\nwidth 5, and with height 3, as shown on the left above.  The command\n", "(3,2,5,3) conceptually drags a \"pen\" around the outline\nof this rectangle.  However, the pen is a 1-pixel square, and it is the\nupper left corner of the pen that moves along the outline.  As the pen\nmoves along the right edge of the rectangle, the pixels to the ", "\nof that edge are colored; as the pen moves along the bottom edge, the\npixels below the edge are colored.  The result is as shown on the right above.\nMy point here is not to belabor the details, but to point out that having\na precise specification of the meaning of graphical operations gives you very\nfine control over what happens on the pixel level.", "Java's original graphics did not support things like real-number coordinates, \n", ", ", ", or \n", ".  Just a few years after Java was first introduced,\na new graphics API was added that does support all of these.  It is that\nmore advanced API that we will look at here.", "Java is an object-oriented language.  Its API is defined as a large set\nof classes,   The actual drawing operations in the original graphics API\nwere mostly contained in the class named ", ".  \nIn the newer API, drawing operations are methods in a class named ", ",\nwhich is a subclass of ", ", so that all the original drawing operations\nare still available.  (A class in Java is contained in a collection of classes known as a \"package.\"\n", " and ", ", for example, are in the\npackage named ", ".  Classes that define shapes and transforms are in a package\nnamed ", ".  However, in the rest of this section, I will talk about classes\nwithout mentioning their packages.)", "A graphics system needs a place to draw.  In Java, the drawing surface is often an object\nof the class ", ", which represents a rectangular area on the\nscreen.  The ", " class has a method named ", "()\nto draw its content.  To create a drawing surface, you can create a subclass of\n", " and provide a definition for its ", "()\nmethod.  All drawing should be done inside ", "(); when it is necessary\nto change the contents of the drawing, you can call the panel's ", "() method\nto trigger a call to ", "().  The ", "() method has\na parameter of type ", ", but the parameter that is passed\nto the method is actually an object of type ", ", and\nit can be type-cast to ", " to obtain access to the\nmore advanced graphics capabilities.  So, the definition of the ", "()\nmethod usually looks something like this:", "In the rest of this section, I will assume that ", " is a variable of\ntype ", ", and I will discuss some of the things\nthat you can do with it.  As a first example, I note that ", "\nsupports ", ", but it is not turned on by default.  It\ncan be enabled in a graphics context ", " with the rather intimidating\ncommand", "For simple examples of graphics in complete Java programs,\nyou can look at the sample programs ", "\nand ", ".  They provide very minimal\nframeworks for drawing static and animated images, respectively, using ", ".\nThe program ", " is a similar framework for\nworking with mouse and key events in a graphics program.\nYou can use these programs as the basis for some experimentation if you want to explore\nJava graphics.", "Drawing with the original ", " class is done using integer coordinates, \nwith the measurement given in pixels. This works well in the standard coordinate system, but is\nnot appropriate when real-number coordinates are used, since the unit of measure in such a \ncoordinate system will not be equal to a pixel.  We need to be able to specify shapes using\nreal numbers.  The Java package ", " provides support for shapes defined using real number \ncoordinates.  For example, the class ", " in that package represents line segments\nwhose endpoints are given as pairs of real numbers.", "Now, Java has two real number types: ", " and ", ".\nThe ", " type can represent a larger range of numbers than ", ", with a greater\nnumber of significant digits, and ", " is the\nmore commonly used type.  In fact, ", " are simply easier to use in Java.\nHowever, ", "  values generally have enough accuracy\nfor graphics applications, and they have the advantage of taking up less space in memory.\nFurthermore, computer graphics hardware often uses float values internally.", "So, given\nthese considerations, the ", " package actually provides\ntwo versions of each shape, one using coordinates of type ", " and\none using coordinates of type ", ".  This is done in a rather strange way.\nTaking ", " as an example, the class ", "\nitself is an ", ".  It has two subclasses, one that represents lines using\n", " coordinates and one using ", " coordinates.\nThe strangest part is that these subclasses are defined as nested classes\ninside ", ": ", " and\n", ".  This means that you can declare a variable of\ntype ", ", but to create an object, you need to use\n", " or ", ":\n", "Note that when using constants of type ", " in Java,\nyou have to add \"F\" as a suffix to the value.  this is one reason why ", "\nare easier in Java.  For simplicity, you might want to stick to using\n", ".  However, ", " might give\nslightly better performance.", "Let's take a look at some of the other classes from ", ".\nThe abstract class ", "\u2014with its concrete subclasses \n", " and ", "\u2014represents\na point in two dimensions, specified by two real number coordinates.  A point is not a\nshape; you can't fill or stroke it.  A point can be\nconstructed from two real numbers (\"", "\").  If ", "\nis a variable of type ", ", you can use ", "() and\n", "() to retrieve its coordinates, and you can use ", "(", "),\n", "(", "), or ", "(", ",", ") to set its coordinates.\nIf ", " is a variable of type ", ", you can also refer\ndirectly to the coordinates as ", " and ", "\n(and similarly for ", ").\nOther classes in ", " offer a similar variety of ways\nto manipulate their properties, and I won't try to list them all here.\n", " The package ", "\ncontains a variety of classes that represent geometric shapes, including  ", ",\n", ", ", ",\n", ", ", ",\nand ", ".\nAll of these are abstract classes, and each of them contains a pair of subclasses such as\n", " and ", ".\nSome shapes, such as rectangles, have\ninteriors that can be filled; such shapes also have\noutlines that can be stroked.  Some shapes, such as lines, are purely one-dimensional\nand can only be stroked.", "Aside from lines, rectangles are probably the simplest shapes.  A ", "\nhas a corner point (", ",", "), a ", ", and a ", ", and\ncan be constructed from that data (\"", "\").  The corner point\n(", ",", ") specifies the minimum ", "- and ", "-values in the rectangle.\nFor the usual pixel coordinate\nsystem, (", ",", ") is the upper left corner.  However, in a coordinate system in which the\nminimum value of ", " is at the bottom, (", ",", ") would be the lower left corner.\nThe sides of the rectangle are parallel to the coordinate axes.  A variable\n", " of type ", "\nhas public instance variables ", ", ", ", ", ", and ", ".\nIf the width or the height is less than or equal to zero, nothing will be drawn when the rectangle\nis filled or stroked.   A common task is to define a rectangle from two corner points (", ",", ")\nand (", ",", ").  This can be accomplished by creating a rectangle with height and width\nequal to zero and then ", " the second point to the rectangle.\nAdding a point to a rectangle causes the rectangle to grow just enough to include that point:\n", "The classes ", ",\n", ", ", "\nand ", " create other basic shapes and work similarly\nto ", ".  You can check the Java API documentation for details.", "The ", " class is more interesting. It represents general\npaths made up of segments that can be lines and ", ".\nPaths are created using methods similar to the ", " and ", "\nsubroutines that were discussed in ", ".  To create a\npath, you start by constructing an object of type ", "\n(or ", "):", "The path ", " is empty when it is first created.\nYou construct the path by moving an imaginary \"pen\" along the path that you want to create.\nThe method ", "(", ",", ") moves the pen to the point (", ",", ") without\ndrawing anything.  It is used to specify the initial point of the path or\nthe starting point of a new piece of the path.\nThe method ", "(", ",", ") draws a line\nfrom the current pen position to (", ",", "), leaving the pen at (", ",", ").\nThe method ", "() can be used to close the path (or the current piece of the path) \nby drawing a line back to its starting point.\nFor example, the following code creates a triangle with vertices at (0,5), (2,-3), and (-4,1):\n", "You can also add Bezier curve segments to a ", ".  \nBezier curves were discussed in ", ".   You can\nadd a cubic Bezier curve to a ", " ", " with the method\n", "This adds a curve segment that starts at the current pen position and ends at\n(", ",", "), using (", ",", ") and (", ",", ") as\nthe two ", " for the curve.  The\nmethod for adding a quadratic Bezier curve segment to a path is ", ".\nIt requires only a single control point:", "When a path intersects itself, its interior is determined by looking at\nthe ", ", as discussed in ", ".\nThere are two possible rules for determining whether a point is interior:\nasking whether the winding number of the curve about that point is non-zero, \nor asking whether it is even.  You can set the winding rule used by a ", "\n", " with", "The default is ", ".", "Finally, I will note that it is possible to draw a copy of an image into a graphics context.\nThe image could be loaded from a file or created by the program.  I discuss the second possibility\nlater in this section.  An image is represented by an object of type ", ".\nIn fact, I will assume here that the object is of type ", ",\nwhich is a subclass of ", ".  If ", " is such an object, then", "will draw the image with its upper left corner at the point (", ",", ").  (The fourth\nparameter is hard to explain, but it should be specified as ", " for ", ".)  \nThis draws the image\nat its natural width and height, but a different width and height can be specified in the method:", "There is also a method for drawing a string of text.  The method specifies the\nstring and the basepoint of the string.  (The basepoint is the lower left corner of\nthe string, ignoring \"descenders\" like the tail on the letter \"g\".)  For example,", "Images and strings are subject to transforms in the same way as other shapes. Transforms\nare the only way to get rotated text and images.  As an example, here is what can happen\nwhen you apply a rotation to some text and an image:", "\n", "Once you have an object that represents a shape, you can fill the shape or stroke it.\nThe ", " class defines methods for doing this.\nThe method for stroking a shape is called ", ":", "Here, ", " is of type ", ", and\nshape can be of type ", ", ", ",\n", " or any of the other shape classes.  These are\noften used on a newly created object, when that object represents a shape that\nwill only be drawn once.  For example", "Of course, it is also possible to create shape objects and reuse them many\ntimes.", "The \"pen\" that is used for stroking a shape is usually represented by an\nobject of type ", ".  The default stroke has line\nwidth equal to\u00a01.  That's one unit in the current coordinate system, not\none pixel.  To get a line with a different width, you can install a new stroke\nwith", "The ", " in the constructor is of type ", ".  It is possible to\nadd parameters to the constructor to control the shape of a stroke at its endpoints and\nwhere two segments meet.  (See ", ".)  For example,", "It is also possible to make strokes out of dashes and dots, but I won't discuss how\nto do it here.", "Stroking or filling a shape means setting the colors of certain pixels.  In Java, the rule\nthat is used for coloring those pixels is called a \"paint.\"  Paints can be solid colors,\n", ", or ", ".\nLike most things in Java, paints are represented by objects.  If ", " is\nsuch an object, then", "will set ", " to be used in the graphics context ", " for subsequent\ndrawing operations, until the next time the paint is changed.  (There is also an\nolder method, ", "(", "), that works only for colors and is\nequivalent to calling ", "(", ").)", "Solid colors are represented by objects of type ", ".  A color\nis represented internally as an ", ". An opaque color, with maximal\nalpha component, can be created using the constructor", "where ", ", ", ", and ", " are integers in the range 0 to 255 that give\nthe red, green, and blue components of the color.  To get a translucent color, you can\nadd an alpha component, also in the range 0 to 255:", "There is also a function, ", "(", "), that creates a color\nfrom values in the HSB color model (which is another name for ", ").\nIn this case, the hue, saturation, and brightness color components must be given as values of\ntype ", ".  And there are constants to represent about a dozen common\ncolors, such as ", ", ", ", and ", ".\nFor example, here is how I might draw a square with a black outline and a light\nblue interior:", "Beyond solid colors, Java has the class ", ", to represent\nsimple ", ", and ", "\nto represent to represent ", ".  (Image patterns\nused in a similar way in 3D graphics are called ", ".)\nGradients and patterns were discussed in ", ".\nFor these paints, the color that is applied to a pixel depends on the coordinates of the\npixel.", "To create a ", ", you need a ", "\nobject to specify the image that it will use as a pattern.  You also\nhave to say how coordinates in the image will map\nto drawing coordinates in the display.  You do this by specifying a rectangle in\nthat will hold one copy of the image.  So the constructor takes the form:", "where ", " is the ", " and ", " \nis a ", ".  Outside that specified rectangle, the image is\nrepeated horizontally and vertically. The constructor for a ", " \ntakes the form", "Here, ", ", ", ", ", ", and ", " are values of type ", ";\n", " and ", " are of type ", "; and\n", " is ", ".  The gradient color will vary along the line\nsegment from the point (", ",", ") to the point (", ",", ").\nThe color is ", " at the first endpoint and is ", " at the second\nendpoint.  Color is constant along lines perpendicular to that line segment.  The\nboolean parameter ", " says whether or not the color pattern repeats.\nAs an example, here is a command that will install a ", "\ninto a graphics context:", "You should, by the way, note that the current paint is used for strokes as well\nas for fills.", "The sample Java program ", " displays a polygon\nfilled with a ", " or a ", " \nand lets you adjust their properties.  The image files ", "\nand ", " are part of that program, and they must be\nin the same location as the compiled class files that make up that program when it is run.", "Java implements ", " as\nmethods in the ", " class. For example, if ", " is a\n", ", then calling ", "(1,3) will apply\na translation by (1,3) to objects that are drawn after the method is called. The methods\nthat are available correspond to the transform functions discussed in ", ":", "A transform in Java is represented as an object of the class ", ".\nYou can create a general ", " with the contstructor\n", "The transform ", " will transform a point (", ") to the point (", ") given by", "You can apply the transform ", " to a graphics context ", " by calling\n", "(", ").", "The graphics context ", " includes the current affine transform, which is the\ncomposition of all the transforms that have been applied.  Commands such as\n", " and ", " modify the current transform.  You can\nget a copy of the current transform by calling ", "(), which returns\nan ", " object.\nYou can set the current transform using ", "(", ").\nThis replaces the current transform in ", " with the ", "\n", ".  (Note that ", "(", ") is different from ", "(", ");\nthe first command ", " the current transform in ", ", while the second\n", " the current transform by composing it with ", ".)", "The ", " and ", " methods can be used to implement\n", ".  The idea, as discussed in ", ",\nis that before drawing an object, you should save the current transform.  \nAfter drawing the object, restore the saved transform.  Any additional modeling\ntransformations that are applied while drawing the object and its sub-objects will have\nno effect outside the object.  In Java, this looks like", "For hierarchical graphics, we really need a ", " of transforms.  However, if the hierarchy is implemented\nusing subroutines, then the above code would be part of a subroutine, and the value of the local\nvariable ", " would be stored on the subroutine call stack.  Effectively, we\nwould be using the subroutine call stack to implement the stack of saved transforms.", "In addition to modeling transformations, transforms are used to set up the\n", "-to-", " transformation that\nestablishes the ", " that will be used for drawing.\nThis is usually done in Java just after the graphics context has been created,\nbefore any drawing operations.  It can be done with a Java version of the\n", " \nfunction from ", ".  See the sample program\n", " for an example.", "I will mention one more use for ", " objects:  Sometimes,\nyou do need to explicitly transform coordinates.  For example, given ", "\n(", ",", "), I might need to know where they will actually end up on the screen, in \npixel coordinates.  That is, I would like to transform (", ",", ") by the current transform\nto get the corresponding pixel coordinates.  The ", " class\nhas a method for applying the affine transform to a point.  It works with objects of type\n", ".  Here is an example:", "One way I have used this is when working with strings.  Often when displaying a string in a\ntransformed coordinate system, I want to transform the basepoint of a string, but not\nthe string itself.  That is, I want the transformation to affect the location of the string\nbut not its size or rotation.  To accomplish this, I use the above technique to obtain\nthe pixel coordinates for the transformed basepoint, and then draw the string at\nthose coordinates, using an original, untransformed graphics context.", "The reverse operation is also sometimes necessary.  That is, given pixel coordinates\n(", ",", "), find the point (", ",", ") that is transformed to (", ",", ")\nby a given affine transform.  For example, when implementing mouse interaction, you will\ngenerally know the pixel coordinates of the mouse, but you will want to find the corresponding\npoint in your own chosen coordinate system.  For that, you need an ", ".\nThe inverse of an affine transform ", " is another transform that performs the opposite transformation.\nThat is, if ", "(", ",", ") = (", ",", "), \nand if ", " is the inverse transform, then ", "(", ",", ")\n= (", ",", "). In Java, the inverse transform of an ", "\n", " can be obtained with", "(A final note: The older drawing\nmethods from ", ", such as ", ", use integer coordinates.\nIt's important to note that any shapes drawn using these older methods are subject to the same transformation\nas shapes such as ", " that are specified with real\nnumber coordinates. For example, drawing a line with ", "(1,2,5,7)\nwill have the same effect as drawing a ", " that\nhas endpoints (1.0,2.0) and (5.0,7.0).  In fact, all drawing is affected by\nthe transformation of coordinates.)", "In some graphics applications, it is useful to be able to work with images that\nare not visible on the screen.  That is, you need what I call an ", ".\nYou also need a way to quickly copy the off-screen canvas onto the screen.\nFor example, it can be useful to store a copy of the on-screen image in an off-screen canvas.\nThe canvas is the official copy of the image.  Changes to the image are made to the canvas,\nthen copied to the screen.  One reason to do this is that you can then draw extra stuff on\ntop of the screen image without changing the official copy.  For example, you might draw\na box around a selected region in the on-screen image.  You can do this without damaging the\nofficial copy in the off-screen canvas.  To remove the box from the screen, you just have\nto copy the canvas image onto the screen.", "In Java, an off-screen image can be implemented as an object of type ", ".\nA ", " represents a region in memory where you can draw, in exactly the\nsame way that you can draw to the screen.  That is, you can obtain a graphics context\n", " of type ", " that you can use for drawing on the image.\nA ", " is an ", ", and you can draw\nit onto the screen\u2014or into any other graphics context\u2014like any other ", ",\nthat is, by using the ", " method of the graphics context where you want to display the\nimage.  In a typical setup, there are variables", "The objects are created using, for example,", "The constructor for ", " specifies the\nwidth and height of the image along with its type.  The type tells what\ncolors can be represented in the image and how they are stored.  Here,\nthe type is ", ", which means the image uses\nregular ", " with 8 bits for each\ncolor component.  The three color components for a pixel are packed\ninto a single integer value.", "In a program that uses a ", " to store a copy of\nthe on-screen image, the ", " method generally has the form", "A sample program that uses this technique is ", ".\nIn that program, the user can draw lines, rectangles, and ovals by dragging the mouse.\nAs the mouse moves, the shape is drawn between the starting point of the mouse and its\ncurrent location.  As the mouse moves, parts of the existing image can be repeatedly covered \nand uncovered, without changing the existing image.  In fact, the image is in an off-screen \ncanvas, and the shape that the user is drawing is actually drawn by ", "\nover the contents of the canvas.  The shape is not drawn to the official image in the canvas\nuntil the user releases the mouse and ends the drag operation.", "But my main reason for writing the program was to illustrate pixel manipulation, that is,\ncomputing with the color components of individual pixels.  The ", "\nclass has methods for reading and setting the color of individual pixels.  An image consists of\nrows and columns of pixels.  If ", " is a ", ", then", "gets the integer that represents the color of the pixel in column number ", " and row\nnumber ", ".  Each color component is stored in an 8-bit field in the integer ", "\nvalue.  The individual color components can be extracted for processing using Java's bit\nmanipulation operators:", "Similarly, given red, green, and blue color component values in the range 0 to 255,\nwe can combine those component values into a single integer and use it to set the\ncolor of a pixel in the image:", "There are also methods for reading and setting the colors of an entire rectangular\nregion of pixels.", "Pixel operations are used to implement two features of the sample program.  First, there is a\n\"Smudge\" tool.  When the user drags with this tool, it's like smearing wet paint.  When\nthe user first clicks the mouse, the color components from a small square of pixels surrounding\nthe mouse position are copied into arrays.  As the user moves the mouse, color from the\narrays is blended with the color of the pixels near the mouse position.  Here is a\nsmall rectangle that has been \"smudged\":", "\n", "The second use of pixel manipulation is in implementing \"filters.\"  A filter, in this\nprogram, is an operation that modifies an image by replacing the color of each \npixel with a weighted average of the colors of a 3-by-3 square of pixels.\nA \"Blur\" filter for example, uses equal weights for all pixels in the average, \nso the color of a pixel is changed to the simple average of the colors of that \npixel and its neighbors.  Using different weights for each pixel can produce some \nstriking effects.", "The pixel manipulation in the sample program produces effects that can't be achieved\nwith pure ", ".  I encourage you to learn more by looking at\nthe ", ".\nYou might also take a look at the live demos in the  ", ",\nwhich implement the same effects using ", " graphics."], "chapter_title": "Two-Dimensional Graphics", "id": 2.5}, {"section_title": "Some Linear Algebra", "chapter_id": "Chapter 3", "section_id": "Section 3.5", "content": ["Linear algebra is a branch of mathematics that is fundamental to\ncomputer graphics.  It studies ", ",\n", ",\nand ", ".  We have already encountered\nthese topics in ", " in a two-dimensional\ncontext.  In this section, we look at them more closely and extend\nthe discussion to three dimensions.", "It is not essential that you know the mathematical details that\nare covered in this section, since they can be handled internally in\nOpenGL or by software libraries.  However, you will need to be familiar\nwith the concepts and the terminology. This is especially true for\nmodern OpenGL, which leaves many of the details up to your programs.\nEven when you have a software library to handle the details, you still\nneed to know enough to use the library.  You might want to skim\nthis section and use it later for reference.", "A vector is a quantity that has a length and a direction.  A vector can be visualized\nas an arrow, as long as you remember that it is the length and direction of the\narrow that are relevant, and that its specific location is irrelevant.\nVectors are often used in computer graphics to represent directions, such as\nthe direction from an object to a light source or the direction in which a surface\nfaces.  In those cases, we are more interested in the direction of a vector than\nin its length.", "If we visualize a 3D vector ", " as an arrow starting at the origin, (0,0,0), and ending\nat a point ", ", then we can, to a certain extent, identify ", "\nwith ", "\u2014at least as long as we remember that an arrow starting\nat any other point could also be used to represent ", ".\nIf ", " has coordinates (", "), we can use the same coordinates\nfor ", ".  When we think of (", ") as a vector, ", " represents\nthe ", " in the ", "-coordinate between the starting point of the arrow and\nits ending point, ", " is the change in the ", "-coordinate, and ", " is\nthe change in the ", "-coordinate.  For\nexample, the 3D point (", ") = (3,4,5) has the\nsame coordinates as the vector (", ") = (3,4,5).\nFor the point, the coordinates (3,4,5) specify a position in space\nin the ", " coordinate system.  For the vector, the coordinates (3,4,5)\nspecify the change in the ", ", ", ", and ", " coordinates along\nthe vector.  If we represent the vector with an arrow that starts\nat the origin (0,0,0), then the head of the arrow will be at (3,4,5).\nBut we could just as well visualize the vector as an arrow that starts at\nthe point (1,1,1), and in that case the head of the arrow would be at\nthe point (4,5,6).", "The distinction between a point and a vector is subtle.  For some\npurposes, the distinction can be ignored; for other purposes, it is important.\nOften, all that we have is a sequence of numbers, which we can treat \nas the coordinates of either a vector or a point, whichever is more appropriate in the context.", "One of the basic properties of a vector is its ", ".\nIn terms of its coordinates, the length of a 3D vector (", ")\nis given by ", "(", "+", "+", ").\n(This is just the Pythagorean theorem in three dimensions.)  If ", " is\na vector, its length is denoted by\u00a0", ".\nThe length of a vector is also called its ", ".\n(We are considering 3D vectors here, but concepts and formulas are similar for other dimensions.)\n", "Vectors of length 1 are particularly important.  They are called\n", ".  If ", " = (", ")\nis any vector other than (0,0,0), then there is exactly one unit vector\nthat points in the same direction as ", ".  That vector is given by", "where ", " is the length of ", ".  Dividing a vector by its\nlength is said to ", " the vector: The result\nis a unit vector that points in the same direction as the original\nvector.", "Two vectors can be added.  Given two vectors ", " = (", ") and\n", " = (", "), their sum is defined as", "The sum has a geometric meaning:", "\n", "Multiplication is more complicated.  The obvious definition of the product of two vectors,\nsimilar to the definition of the sum, does not have geometric meaning and is rarely used.\nHowever, there are three kinds of vector multiplication that are used: the scalar\nproduct, the dot produt, and the cross product.", "If ", " = (", ") is a vector and ", " is a number, then the ", "\nof ", " and ", " is defined as", "Assuming that ", " is positive and ", " is not zero, ", " is a vector that points in the same\ndirection as ", ", whose length is ", " times the length of ", ".  If ", " is negative,\n", " points in the opposite direction from ", ", and its length is ", "\ntimes the length of ", ".  This type of product is called a scalar product because a number like\n", " is also referred to as a \"scalar,\" perhaps because multiplication by ", " scales ", "\nto a new length.", "Given two vectors ", " = (", ") and\n", " = (", "), the ", "\nof ", " and ", " is denoted by ", "\u00b7", " and is defined\nby", "Note that the dot product is a number, not a vector.\nThe dot product has several very important geometric meanings.  First of\nall, note that the length of a vector ", " is just the square root of\n", "\u00b7", ".  Furthermore, the dot product of two non-zero\nvectors ", " and ", " has the property that\n", "where ", " is the measure of the angle between ", " and ", ".  In\nparticular, in the case of two unit vectors, whose lengths are 1, the dot product of\ntwo unit vectors is simply the cosine of the angle between them.  Furthermore,\nsince the cosine of a 90-degree angle is zero, two non-zero vectors are perpendicular\nif and only if their dot product is zero.  Because of these properties,\nthe dot product is particularly important in lighting calculations, where the\neffect of light shining on a surface depends on the angle that the light makes \nwith the surface.", "The scalar product and dot product are defined in any dimension.  For vectors in 3D, there is\nanother type of product called the ", ", which also\nhas an important geometric meaning. For vectors ", " = (", ") and\n", " = (", "), the cross product of ", "\nand ", " is denoted ", "\u00d7", " and is the vector defined by\n", "If ", " and ", " are non-zero vectors, then ", "\u00d7", "\nis zero if and only if ", " and ", " point in the same direction or in\nexactly opposite directions.  Assuming ", "\u00d7", " is non-zero, then\nit is perpendicular both to ", " and to ", "; furthermore, \nthe vectors ", ", ", ", ", "\u00d7", " follow the\n", "; that is, if you curl the fingers of your right hand from \n", " to ", ", then your thumb points in the direction of ", "\u00d7", ". If\n", " and ", " are perpendicular unit vectors, then the cross product\n", "\u00d7", " is also a unit vector, which is perpendicular both\nto ", " and to ", ".", "Finally, I will note that given two points ", " = (", ") and\n", " = (", "), the difference ", "\nis defined by", "This difference is a vector that can be visualized as an arrow that starts at ", "\nand ends at ", ".", "Now, suppose that ", ", ", ", and ", "\nare vertices of a polygon.  Then the vectors ", " and\n", " lie in the plane of the polygon, and so the cross product", "is a vector that is perpendicular to the polygon.", "\n", "This vector is said\nto be a ", " for the polygon.  A normal vector of length one\nis called a ", ".  Unit normals will be important in lighting\ncalculations, and it will be useful to be able to calculate a unit normal for a polygon\nfrom its vertices.", "A matrix is just a two-dimensional array of numbers.  A matrix with ", " rows and\n", " columns is said to be an ", "-by-", " matrix.  If ", " and ", "\nare matrices, and if the number of columns in ", " is equal to the number of\nrows in ", ", then ", " and ", " can be multiplied to give the matrix\nproduct ", ".  If ", " is an ", "-by-", " matrix and ", " is\nan ", "-by-", " matrix, then ", " is an ", "-by-", " matrix.\nIn particular, two ", "-by-", " matrices can be multiplied to give another\n", "-by-", " matrix.", "An ", "-dimensional vector can be thought of an ", "-by-", " matrix.  If\n", " is an ", "-by-", " matrix and ", " is a vector in ", " dimensions,\nthought of as an ", "-by-", " matrix, then the product ", " is again an\n", "-dimensional vector (though in this case thought of as a ", "-by-", " matrix).\nThe product of a 3-by-3 matrix ", " and a 3D vector ", " = (", ")\nis often displayed like this:", "\n", "Note that the ", "-th coordinate in the product ", " is simply the dot product of the\n", "-th row of the matrix ", " and the vector\u00a0", ".", "Using this definition of the multiplication of a vector by a matrix, a matrix defines a\ntransformation that can be applied to one vector to yield another vector.\nTransformations that are defined in this way are ", ",\nand they are the main object of study in linear algebra.  A linear transformation ", " has\nthe properties that for two vectors ", " and ", ", ", ",\nand for a number ", ", ", ".", "Rotation and scaling are linear transformations, but translation is ", "\na linear transformaton.\nTo include translations, we have to widen our view of transformation to include\n", ".\nAn affine transformation can be defined, roughly, as a linear transformation followed by a \ntranslation.  Geometrically, an affine transformation is a transformation that preserves\nparallel lines; that is, if two lines are parallel, then their images under an affine\ntransformation will also be parallel.\nFor computer graphics, we are interested in affine transformations in\nthree dimensions.  However\u2014by what seems at first to be a very odd trick\u2014we\ncan narrow our view back to the linear by moving into the fourth dimension.\n", "Note first of all that an affine transformation in three dimensions transforms a vector\n(", ") into a vector (", ") given by\nformulas", "These formulas express a linear transformation given by multiplication by the 3-by-3 matrix", "\n", "followed by translation by ", " in the ", " direction, ", " in the ", "\ndirection and ", " in the ", " direction.  The trick is to replace each three-dimensional\nvector (", ") with the four-dimensional vector\n(", "1), adding a \"1\" as the fourth coordinate.   And instead of \nthe 3-by-3 matrix, we use the 4-by-4 matrix\n", "\n", "If the vector (", ",1) is multiplied by this 4-by-4 matrix,\nthe result is  precisely the vector (", "1).  That is,\ninstead of applying an ", " transformation to the 3D vector (", "),\nwe can apply a ", " transformation to the 4D vector (", ",1).", "This might seem pointless to you, but nevertheless, that is what is done in OpenGL and\nother 3D computer graphics systems:  An\naffine transformation is represented as a 4-by-4 matrix in which the bottom row is\n(0,0,0,1), and a three-dimensional vector is changed into a four dimensional vector\nby adding a 1 as the final coordinate.  The result is that all the affine transformations\nthat are so important in computer graphics can be implemented as multiplication of\nvectors by matrices.", "The identity transformation, which leaves vectors unchanged, corresponds to multiplication\nby the ", ", which has ones along its descending diagonal and\nzeros elsewhere. The OpenGL function ", "() sets the current matrix to\nbe the 4-by-4 identity matrix.  An OpenGL transformation function, such as ", "(", "),\nhas the effect of multiplying the current matrix  by the 4-by-4 matrix that\nrepresents the transformation.  Multiplication is on the right; that is, if ", " is\nthe current matrix and ", " is the matrix that represents the transformation, then\nthe current matrix will be set to the product matrix\u00a0", ". For the record,\nthe following illustration shows the identity matrix and the matrices\ncorresponding to various OpenGL transformation functions:", "\n", "It is even possible to use an arbitrary transformation matrix in OpenGL, using the\nfunction ", "(", ") or ", "(", "). The parameter, ", ",\nis an array of numbers of type ", " or ", ",\nrepresenting a transformation matrix.  The array is a one-dimensional array of length 16.\nThe items in the array are the numbers from the transformation matrix, stored in ", ",\nthat is, the numbers in the fist column, followed by the numbers in the second\ncolumn, and so on.  These function multiply the current matrix by the matrix ", ",\non the right.  You could use them, for example, to implement a ", ",\nwhich is not easy to represent as a sequence of scales, rotations, and translations.", "We finish this section with a bit of mathematical detail about the implementation of transformations.\nThere is one common transformation in computer graphics that is not an affine transformation:\nIn the case of a perspective projection, the projection transformation is not affine.\nIn a perspective projection, an object will appear to get smaller as it moves farther away\nfrom the viewer, and that is a property that no affine transformation can express, since\naffine transforms preserve parallel lines and parallel lines will seem to converge in the\ndistance in a perspective projection.\n", "Surprisingly, we can still represent a perspective projection as a 4-by-4 matrix, provided\nwe are willing to stretch our use of coordinates even further than we have already.  We\nhave already represented 3D vectors by 4D vectors in which the fourth coordinate is 1.\nWe now allow the fourth coordinate to be anything at all.  When the fourth coordinate, ", ",\nis non-zero, we consider the coordinates (", ") to\nrepresent the three-dimensional vector (", ").  Note that this\nis consistent with our previous usage, since it considers (", ")\nto represent (", "), as before.   When the fourth coordinate is zero,\nthere is no corresponding 3D vector, but it is possible to think of (", ",0)\nas representing a 3D \"point at infinity\" in the direction of (", "),\nas long as at least one of ", ", ", ", and ", " is non-zero. \n", "Coordinates (", ") used in this way are referred to\nas ", ".  If we use homogeneous coordinates, then any\n4-by-4 matrix can be used to transform three-dimensional vectors, including matrices whose\nbottom row is not (0,0,0,1). Among the transformations\nthat can be represented in this way is the projection transformation for a perspective\nprojection.  And in fact, this is what OpenGL does internally.  It represents all three-dimensional\npoints and vectors using homogeneous coordinates, and it represents all transformations as\n4-by-4 matrices.  You can even specify vertices using homogeneous coordinates.  For example, the\ncommand\n", "with a non-zero value for ", ", generates the 3D point (", ").  Fortunately, you will almost never\nhave to deal with homogeneous coordinates directly.  The only real exception to this is\nthat homogeneous coordinates are used, surprisingly, when configuring OpenGL lighting, as\nwe'll see in the ", "."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.5}, {"section_title": "Hierarchical Modeling", "chapter_id": "Chapter 2", "section_id": "Section 2.4", "content": ["In this section, we look at how complex scenes can be built\nfrom very simple shapes.  The key is hierarchical\nstructure.  That is, a complex object can be made up of\nsimpler objects, which can in turn be made up of even\nsimpler objects, and so on until it bottoms out with\nsimple ", "\nthat can be drawn directly.  This is called\n", ".  We will see\nthat the ", "\nthat were studied in the ", "\nplay an important role in hierarchical modeling.", "Hierarchical structure is the key to dealing with complexity\nin many areas of computer science (and in the rest of reality),\nso it be no surprise that it plays an important role in\ncomputer graphics.", "A major motivation for introducing a new coordinate system is that it should be\npossible to use the coordinate system that is most natural to the scene that you want to\ndraw.  We can extend this idea to individual objects in a scene:  When drawing an object,\nuse the coordinate system that is most natural for the object.\n", "Usually, we want an object in its natural coordinates to be centered at the origin, (0,0),\nor at least to use the origin as a convenient reference point.  Then, to place it in\nthe scene, we can use a ", " transform, followed by a ", ", \nfollowed by a ", " to set its size, orientation, and position in the scene.  \nRecall that transformations used in this way are called \n", ".\nThe transforms are often applied in the order scale, then rotate, then translate,\nbecause scaling and rotation leave the reference point, (0,0), fixed.  Once the object\nhas been scaled and rotated, it's easy\nto use a translation to move the reference point to any desired point in the scene.\n(Of course, in a particular case, you might not need all three operations.) Remember that in the code,\nthe transformations are specified in the opposite order from the order in which they are\napplied to the object and that the transformations are specified before drawing the\nobject.  So in the code, the translation would come first, followed by the rotation and\nthen the scaling.  Modeling transforms are not always composed in this order, but\nit is the most common usage.", "The modeling transformations that are used to place an object in the scene should not\naffect other objects in the scene.  To limit their application to just the one object,\nwe can save the current transformation before starting work on the object and restore it\nafterwards.  How this is done differs from one graphics ", " to another, \nbut let's suppose here that there are subroutines ", "() and\n", "() for performing those tasks.  That is, ", "\nwill make a copy of the modeling transformation that is currently in effect and store\nthat copy.  It does not change the current transformation; it merely saves a copy.\nLater, when ", " is called, it will retrieve that copy and will\nreplace the current modeling transform with the retrieved transform.  Typical code\nfor drawing an object will then have the form:", "Note that we don't know and don't need to know what the saved transform does.\nPerhaps it is simply the so-called ", ", which\nis a transform that doesn't modify the coordinates to which it is applied.\nOr there might already be another transform in place, such as a coordinate transform that affects the scene as a whole.\nThe modeling transform for the object is effectively applied in addition to any other transform that\nwas specified previously.  The modeling transform moves the object from its natural coordinates into its\nproper place in the scene.  Then on top of that, a coordinate transform that is applied to the scene as a whole\nwould carry the object along with it.", "Now let's extend this idea.  Suppose that the object that we want to draw is itself a complex \npicture, made up of a number of smaller objects.  Think, for example, of a potted flower made up of\npot, stem, leaves, and bloom.  We would like to be able to draw the smaller component objects in their\nown natural coordinate systems, just as we do the main object.  For example, we would like to specify\nthe bloom in a coordinate system in which the center of the bloom is at (0,0).\nBut this is easy:  We draw each small component object, such as the bloom,\nin its own coordinate system, and use a modeling transformation to move the sub-object\ninto position ", ".  We are composing the complex object in its\nown natural coordinate system as if it were a complete scene.", "On top of that, we can apply ", " modeling\ntransformation to the complex object as a whole, to move it into the actual scene; \nthe sub-objects of the complex object are carried along with it.  That is,\nthe overall transformation that applies to a sub-object consists of a modeling transformation\nthat places the sub-object into the complex object, followed by the transformation that\nplaces the complex object into the scene.", "In fact, we can build objects that are made up of smaller objects which in turn\nare made up of even smaller objects, to any level. For example, we could draw the bloom's petals in\ntheir own coordinate systems, then apply modeling transformations to place the petals into the\nnatural coordinate system for the bloom.  There will be another\ntransformation that moves the bloom into position\non the stem, and yet another transformation that places the entire potted flower into the scene.\nThis is hierarchical modeling.", "Let's look at a little example.  Suppose that we want to draw a simple 2D image of a cart with\ntwo wheels.", "\n", "This cart is used as one part of a complex scene in an example below.\nThe body of the cart can be drawn as a pair of rectangles.  For the wheels, suppose that we\nhave written a subroutine", "that draws a wheel.  This subroutine draws the wheel in its own natural coordinate system. \nIn this coordinate system, the wheel is centered at (0,0) and has radius\u00a01.", "In the cart's coordinate system, I found it convenient to use the midpoint of the base of the\nlarge rectangle as the reference point.  I \nassume that the positive direction of the ", "-axis points upward, which is the common\nconvention in mathematics.  The rectangular body of the cart has\nwidth 6 and height 2, so the coordinates of the lower left corner of the rectangle are (-3,0),\nand we can draw it with a command such as ", "(-3,0,6,2).\nThe top of the cart is a smaller red rectangle, which can be drawn in a similar way.\nTo complete the cart, we need\nto add two wheels to the object.  To make the size of the wheels fit the cart, they need to be scaled.\nTo place them in the correct positions relative to body of the cart, one wheel must be translated\nto the left and the other wheel, to the right.  When I coded this example, I had to play\naround with the numbers to get the right sizes and positions for the wheels, and I found\nthat the wheels looked better if I also moved them down a bit.  Using the usual techniques of\nhierarchical modeling, we save the current transform before drawing each wheel, and we restore it after\ndrawing the wheel. This restricts the effect of the modeling transformation for the wheel\nto that wheel alone, so that it does not affect any other part of the cart.\nHere is pseudocode for a  subroutine that draws the cart in its own coordinate system:", "It's important to note that the same subroutine is used to draw both wheels.  The reason that\ntwo wheels appear in the picture in different positions is that different modeling transformations are in effect for the\ntwo subroutine calls.", "Once we have this cart-drawing subroutine, we can use it to add a cart to a scene.\nWhen we do this, we apply another modeling transformation to the cart as a whole.  Indeed, we could add several carts\nto the scene, if we wanted, by calling the ", " subroutine several times with different modeling transformations.\n", "You should notice the analogy here:  Building up a complex scene out of objects is similar to\nbuilding up a complex program out of subroutines.  In both cases, you can work on pieces of the\nproblem separately, you can compose a solution to a big problem from solutions to smaller problems,\nand once you have solved a problem, you can reuse that solution in several places.\n", "Here is a demo that uses\nthe cart in an animated scene:", "\n", "\n", "You can probably guess how hierarchical modeling is used to draw \nthe three windmills in this example.\nThere is a ", " method that draws a windmill in its own coordinate system.  Each of the\nwindmills in the scene is then produced by applying a different modeling transform to the standard\nwindmill.  Furthermore, the windmill is itself a complex object that is constructed from several\nsub-objects using various modeling transformations.", "It might not be so easy to see how different parts of the scene can be animated.  In fact, animation\nis just another aspect of modeling.  A computer ", " consists of a sequence of frames.  Each frame\nis a separate image, with small changes from one frame to the next.  From our point of view, each frame\nis a separate scene and has to be drawn separately.  The same object can appear in many frames.  To\nanimate the object, we can simply apply a different modeling transformation to the object in each\nframe.  The parameters used in the transformation can be computed from the current time or from the frame number.\nTo make a cart move from left to right, for example, we might apply a modeling transformation", "to the cart, where ", " is the frame number.\nIn each frame, the cart will be 0.1 units farther to the right than in the previous\nframe.  (In fact, in the actual program, the translation that is applied to the cart is\n", "which moves the reference point of the cart from -3 to 13 along the horizontal axis\nevery 300 frames.  In the coordinate system that is used for the scene, the x-coordinate\nranges from 0 to 7, so this puts the cart outside the scene for much of the loop.)", "The really neat thing is that this type of animation works with hierarchical modeling.  For example,\nthe ", " method doesn't just draw a windmill\u2014it draws an ", " windmill,\nwith turning vanes.  That just means that the rotation applied to the vanes depends on the frame \nnumber.  When a modeling transformation is applied to the windmill, the rotating vanes are scaled and\nmoved as part of the object as a whole.  This is an example of hierarchical modeling.\nThe vanes are sub-objects of the windmill.  The rotation of the vanes is part of the modeling\ntransformation that places the vanes into the windmill object.  Then a further modeling transformation\nis applied to the windmill object to place it in the scene.\n", "The file ", " contains\nthe complete source code for a Java version of this example.  The ", "\nof this book covers graphics programming in Java.  Once you are familiar with that, you should take\na look at the source code, especially the ", "() method, which draws the entire scene.", "Logically, the components of a complex scene form a structure.  In this structure,\neach object is associated with the sub-objects that it contains.  If the scene is hierarchical,\nthen the structure is hierarchical.  This structure is known as a\n", ".  A scene graph is a tree-like structure,\nwith the root representing the entire scene, the children of the root representing the\ntop-level objects in the scene, and so on.  We can visualize the scene graph for our\nsample scene:", "\n", "In this drawing, a single object can have several connections to one or\nmore parent objects.  Each connection represents one occurrence of the object in its\nparent object.  For example, the \"filled square\" object occurs as a sub-object\nin the cart and in the windmill.  It is used twice in the cart and once in the\nwindmill.  (The cart contains two red rectangles, which are created as squares\nwith a non-uniform scaling; the pole of the windmill is made as a scaled square.)\nThe \"filled circle\" is used in the sun and is used twice in the wheel.  The \"line\"\nis used 12 times in the sun and 12 times in the wheel; I've drawn one thick arrow, marked\nwith a 12, to represent 12 connections.  The wheel, in turn, is used twice in\nthe cart.  (My diagram leaves out, for lack of space, two occurrences of the filled\nsquare in the scene: It is used to make the road and the line down the middle of the road.)", "Each arrow in the picture can be associated with a modeling transformation\nthat places the sub-object into its parent object.  When an object contains several\ncopies of a sub-object, each arrow connecting the sub-object to the object will have\na different associated modeling transformation.  The object is the same for each copy;\nonly the transformation differs.", "Although the scene graph exists conceptually, in some applications it exists\nonly implicitly.  For example, the Java version of the program that was\nmentioned above draws the image \"procedurally,\" that is, by calling subroutines.\nThere is no data structure to represent the scene graph.\nInstead, the scene graph is implicit in\nthe sequence of subroutine calls that draw the scene.  Each node in the graph is a subroutine,\nand each arrow is a subroutine call.  The various objects are drawn using different\nmodeling transformations.  As discussed in ", ",\nthe computer only keeps track of a \"current transformation\" that represents all\nthe transforms that are applied to an object.  When an object is drawn by a subroutine,\nthe program saves the current transformation before calling the subroutine.\nAfter the subroutine returns, the saved transformation is restored.  \nInside the subroutine, the object is drawn in its own\ncoordinate system, possibly calling other subroutines to draw sub-objects with their\nown modeling transformations. Those extra transformations will have no effect outside of the subroutine,\nsince the transform that is in effect before the subroutine is called will be restored\nafter the subroutine returns.", "It is also possible for a scene graph to be represented by an actual data structure in the program.\nIn an object-oriented approach, the graphical objects in the scene are represented by\nprogram objects.  There are many ways to build an object-oriented scene graph ", ".\nFor a simple example implemented in Java, you can take a look at\n", ".  This program draws\nthe same animated scene as the previous example, but it represents the scene with\nan object-oriented data structure rather than procedurally.   The same scene graph API\nis implemented in ", " in the live demo shown\nearlier on this page, and you might take a look at that after you read about\n", " graphics in ", ".", "In the example\nprogram, both in Java and in JavaScript, a node in the scene graph is\nrepresented by an object belonging to a class named ", ".\n", " is an abstract class, and actual nodes in the scene graph are defined by \nsubclasses of that class.  For example, there is a subclass named\n", " to represent a complex graphical object that is\nmade up of sub-objects.  A variable, ", ", of type ", " includes \na method ", "(", ") for adding a sub-object to the compound object.", "When implementing a scene graph as data structure made up of objects,\na decision has to be made about how to handle transforms. \nOne option is to allow transformations to be associated with any node in the scene graph.  In this case, however,\nI decided to use special nodes to represent transforms as\nobjects of type  ", ".\nA ", " is a ", " that\ncontains a link to another ", " and also contains a\nmodeling transformation that is to be applied to that object.\nThe modeling transformation is given in terms of scaling, rotation, and translation amounts\nthat are instance variables in the object.  It is worth noting that these are always applied\nin the order scale, then rotate, then translate, no matter what order the instance variables\nare set in the code. If you want to do a translation followed by a rotation, you will need\ntwo ", " to implement it, since a translation plus a rotation\nin the same ", " would be applied in the order rotate-then-translate.\nIt is also worth noting that the setter methods for the scaling,\nrotation, and translation have a return value that is equal to the object.  This makes it possible\nto chain calls to the methods into a single statement such as", "and even say things like", "This type of chaining can make for\nmore compact code and can eliminate the need for a lot of extra temporary variables.", "Another decision has to be made about how to handle color.  One possibility would be\nto make a ", " class similar to\n", ".  However, in this case I just added\na ", "() method to the main ", "\nclass.  A color that is set on a compound object is inherited by any sub-objects,\nunless a different color is set on the sub-object.  In other words, a color on a compound object\nacts as a default color for its sub-objects, but color can be overridden on the sub-objects.", "In addition to compound objects and transformed objects, we need scene graph\nnodes to represent the basic graphical objects that occupy the bottom level of the scene graph.\nThese are the nodes that do the actual drawing in the end.", "For those who are familiar with data structures, I will note that\na scene graph is actually an example of a \"directed acyclic graph\" or \"dag.\"\nThe process of drawing the scene involves a traversal of this dag.  The\nterm \"acyclic\" means that there can't be cycles in the graph.  For a scene graph,\nthis is the obvious requirement that an object cannot be a sub-object, either\ndirectly or indirectly, of itself.", "Suppose that you write a subroutine to draw an object. At the beginning of the\nsubroutine, you use a routine such as ", "() to save a copy of the current transform.\nAt the end of the subroutine, you call ", "() to reset the current\ntransform back to the value that was saved.  Now, in order for this to work correctly\nfor hierarchical graphics, these routines must actually use a ", " of transforms.\n(Recall that a stack is simply a list where items can be added, or \"pushed,\" onto\none end of the list and removed, or \"popped,\" from the same end.)  The problem is that\nwhen drawing a complex object, one subroutine can call other subroutines.  This means that\nseveral drawing subroutines can be active at the same time,\neach with its own saved transform.  When a transform is saved after\nanother transform has already been saved, the system needs\nto remember both transforms.  When ", "() is called, it is the\nmost recently saved transform that should be restored.", "A stack has exactly the structure that is needed to implement these operations.\nBefore you start drawing an object, you would push the current transform onto the stack.  After drawing\nthe object, you would pop the transform from the stack.  Between those two operations, if the object is\nhierarchical, the transforms for its sub-objects will have been pushed onto and popped from the\nstack as needed.", "Some graphics APIs come with transform stacks already defined.\nFor example, the original OpenGL API includes the functions\n", "() and ", "() for using a stack of transformation matrices\nthat is built into OpenGL.  The Java 2D graphics API does not include a built-in\nstack of transforms, but it does have methods for getting and setting the current\ntransform, and the get and set methods can be used with an explicit stack data structure\nto implement the necessary operations.\nWhen we turn to the HTML canvas API for 2D graphics, we'll see that it includes\nfunctions named ", "() and ", "() that are actually ", "\nand ", " operations on a stack.  These functions are essential to implementing\nhierarchical graphics for an HTML canvas.", "Let's try to bring this all together by considering how it applies to a simple object\nin a complex scene: one of the filled circles that is part of the front wheel on the cart\nin our example scene.  Here, I have rearranged part of the scene graph for that scene, and I've added\nlabels to show the modeling transformations that are applied to each object:", "\n", "The rotation amount for the wheel and the translation amount for the cart are shown as\nvariables, since they are different in different frames of the animation.  When the computer\nstarts drawing the scene, the modeling transform that is in effect is the\n", ", that is, no transform at all.  As it prepares to draw\nthe cart, it saves a copy of the current transform (the identity) by pushing it onto the stack.\nIt then modifies the current transform by multiplying it by the modeling transforms for the cart,\n", "(0.3,0.3) and ", "(dx,0).  When it comes to drawing the wheel, it\nagain pushes the current transform (the modeling transform for the cart as a whole) onto the\nstack, and it modifies the current transform to take the wheel's modeling transforms into\naccount.  Similarly, when it comes to the filled circle, it saves the modeling transform\nfor the wheel, and then applies the modeling transform for the circle.", "When, finally, the\ncircle is actually drawn in the scene, it is transformed by the combined transform.\nThat transform places the circle directly into the scene, but it has been composed\nfrom the transform that places the circle into the wheel, the one that places the wheel \ninto the cart, and the one that places the cart into the scene.  After drawing the circle,\nthe computer replaces the current transform with one it pops from the stack.  That will be the \nmodeling transform for the wheel as a whole, and that transform will be used for any further parts of the\nwheel that have to be drawn.  When the wheel is done, the transform for the cart is popped.\nAnd when the cart is done, the original transform, the identity, is popped.  When the computer\ngoes onto the next object in the scene, it starts the whole process again, with the identity\ntransform as the starting point.", "This might sound complicated, but I should emphasize that it something that the computer\ndoes for you.  Your responsibility is simply to design the individual objects, in their\nown natural coordinate system. As part of that, you specify the modeling transformations that are applied\nto the sub-objects of that object.  You construct the scene as a whole in a similar way.\nThe computer will then put everything together for you, taking into account the many layers\nof hierarchical structure.  You only have to deal with one component of the structure at\na time.  That's the power of hierarchical design; that's how it helps you deal with complexity."], "chapter_title": "Two-Dimensional Graphics", "id": 2.4}, {"section_title": "Using GLUT and JOGL", "chapter_id": "Chapter 3", "section_id": "Section 3.6", "content": ["OpenGL is an ", " for graphics only, with no support for things like\nwindows or events.  OpenGL depends on external mechanisms to\ncreate the drawing surfaces on which it will draw.  Windowing APIs\nthat support OpenGL often do so as one library among many others that\nare used to produce a complete application.  We will look at\ntwo cross-platform APIs that make it possible to use OpenGL\nin applications, one for C/C++ and one for Java.", "For simple applications written in C or C++, one possible\nwindowing API is ", " (OpenGL Utility Toolkit).  GLUT is a very small\nAPI.  It is used to create windows that serve as\nsimple frames for OpenGL drawing surfaces.  It has support for\nhandling mouse and keyboard events, and it can do basic animation.\nIt does not support controls such as buttons or input fields,\nbut it does allow for a menu that pops up in response to\na mouse action.  You can find information about the GLUT API at", "\n", "\n", "If possible, you should use FreeGLUT, which is compatible with GLUT but has\na few extensions and a fully open source license.   See", "\n", "\n", "\n", " (Java OpenGL) is a collection of classes that make it\npossible to use OpenGL in Java applications.  JOGL is integrated\ninto Swing and AWT, the standard Java graphical user interface APIs.\nWith JOGL, you can create Java GUI components on which\nyou can draw using OpenGL.  These OpenGL components can be\nused in any Java application, in much the same way that you\nwould use a ", "\nor ", " as a drawing surface.\nLike many things Java, JOGL is immensely complicated.  We will use it\nonly in fairly simple applications.\nJOGL is not a standard part of Java.  It's home web site is", "\n", "\n", "This section contains information to get you started using GLUT and JOGL, assuming\nthat you already know how the basics of programming with C and Java.  It also briefly\ndiscusses ", ", a JavaScript library that I have written to simulate the subset\nof OpenGL 1.1 that is used in this book.", "To work with GLUT, you will need\na C compiler and copies of the OpenGL and GLUT (or FreeGLUT)\ndevelopment libraries.  I can't tell you exactly that means on\nyour own computer.  On my computer, which runs Linux Mint, for example,\nthe free C compiler gcc is already available.  To do OpenGL\ndevelopment, I installed several packages, including\n", " and ", ".\n(Mesa is a Linux implementation of OpenGL.)  If ", " contains\na complete C program that uses GLUT, I can compile it using a command such as\n", "The \"-o glutprog\" tells the compiler to use \"glutprog\" as the\nname of its output file, which can then be run as a normal executable file;\nwithout this option, the executable file would be named \"a.out\".\nThe \"-lglut\" and \"-lGL\" options tell the compiler to link the program with the GLUT and OpenGL libraries.\n(The character after the \"-\" is a lower case \"L\".)\nWithout these options, the compiler won't recognize any GLUT or OpenGL functions.  If the program\nalso uses the ", " library, compiling it would require the option \"-lGLU, and if it uses\nthe math library, it would need the option \"-lm\".  If a program requires additional .c files,\nthey should be included as well.  For example, the sample program\n", " depends on ", ", and it\ncan be compiled with the Linux gcc compiler using the command:", "The sample program ", " can be used as a starting\npoint for writing programs that use GLUT.  While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "The GLUT library makes it easy to write basic OpenGL applications in\u00a0C.  GLUT\nuses event-handling functions.  You write functions to handle events that occur\nwhen the display needs to be redrawn or when the user clicks the mouse or presses a key\non the keyboard.", "To use GLUT, you need to include the header file ", " (or ", ")\nat the start of any source code file that uses it, along with the general OpenGL header file,\n", ".  The header files should be installed in a standard location, in a folder named ", ".\nSo, the program usually begins with", "On my computer, saying ", " actually includes the subset\nof FreeGLUT that corresponds to GLUT.  To get access to all of FreeGLUT, I would\nsubstitute ", ".  Depending on the features that it uses,\na program might need other header files, such as ", " \nand ", ".", "The program's ", "() function must contain some code to initialize GLUT, to\ncreate and open a window, and to set up event handling by registering the functions that\nshould be called in response to various events.  After this setup, it must\ncall a function that runs the GLUT event-handling loop.  That function\nwaits for events and processes them by calling the functions that have been registered\nto handle them.  The event loop runs until the program ends, which happens when\nthe user closes the window or when the program calls the standard ", "() function.", "To set up the event-handling functions,\nGLUT uses the fact that in C, it is possible to pass a function name as a parameter\nto another function.  For example, if ", "() is the function that \nshould be called to draw the content of the window, then the\nprogram would use the command", "to install this function as an event handler for\ndisplay events. A display event occurs when the contents of the window need to be redrawn, including\nwhen the window is first opened.\nNote that ", " must have been previously defined, as a function with no parameters:", "Keep in mind that it's not the name of this function that makes it an OpenGL display\nfunction.  It has to be set as the display function by calling ", "(", ").\nAll of the GLUT event-handling functions work in a similar way (except many of them do need\nto have parameters).", "There are a lot of possible event-handling functions, and I will only cover some of\nthem here.  Let's jump right in and look at a possible ", "() routine for a GLUT\nprogram that uses most of the common event handlers:", "The first five lines do some necessary initialization, the next seven lines install event\nhandlers, and the call to ", "() runs the GLUT event loop.  I will discuss all of\nthe functions that are used here.  The first GLUT function call must be ", ",\nwith the parameters as shown.  (Note that ", " and ", "\nrepresent command-line arguments for the program.  Passing them to ", " allows\nit to process certain command-line arguments that are recognized by GLUT.  I won't discuss those\narguments here.)  The functions ", " and ", "\ndo the obvious things; size is given in pixels, and \nwindow position is given in terms of pixel coordinates on the computer\nscreen, with (0,0) at the upper left corner of the screen.  The function ", "\ncreates the window, but note that nothing can happen in that window until ", "\nis called.  Often, an additional, user-defined function is called in ", "() to do\nwhatever initialization of global variables and OpenGL state is required by the program.\nOpenGL initialization can be done after calling ", " and before\ncalling ", ".  Turning to the other functions used in ", "(),", "\n", " \u2014 Must be called to\ndefine some characteristics of the OpenGL drawing context.  The parameter specifies\nfeatures that you would like the OpenGL context to have.  The features are represented by\nconstants that are OR'ed together in the parameter.  ", " says that a depth buffer\nshould be created; without it, the depth test won't work.  If you are doing 2D graphics, you\nwouldn't include this option.  ", " asks for ", ", \nwhich means that drawing is actually done off-screen, and the\noff-screen copy has to copied to the screen to be seen.  The copying is done by\n", ", which ", " be called at the end of the display function.\n(You can use ", " instead of ", " to get ", "; \nin that case, you have to call ", "() at the end of the display function instead\nof ", "().  However, all of the examples in this book use ", ".)", "\n", " \u2014 The display function\nshould contain OpenGL drawing code that can completely redraw the scene.  This is\nsimilar to ", "() in Java.\nThe display function can have any name, but it must be declared as a void\nfunction with no parameters: ", "().", "\n", " \u2014 The reshape function\nis called when the user changes the size of the window.  Its parameters tell the\nnew width and height of the drawing area:", "For example, you might use this method to set up the projection transform, if the\nprojection depends only on the window size.  A reshape function is not required, but\nif one is provided, it should always set the\nOpenGL ", ", which is the part of the window that\nis used for drawing.  Do this by calling", "The viewport is set automatically if no reshape function is specified.", "\n", " \u2014 The keyboard function is\ncalled when the user types a character such as 'b' or 'A' or a space.  It is not called\nfor special keys such as arrow keys that do not produce characters when pressed.\nThe keyboard function has a parameter of type ", " which\nrepresents the character that was typed.  It also has two ", " parameters \nthat give the location of the mouse when the key was pressed, in pixel coordinates\nwith (0,0) at the upper left corner of the display area.  So, the definition of\nthe key function must have the form:", "Whenever you make any changes to the program's data that require the display to be redrawn,\nyou should call ", "().  This is similar to calling ", "() in\nJava.  It is better to call ", "()\nthan to call the display function directly.  (I also note that it's possible to\ncall OpenGL drawing commands directly in the event-handling functions, but it probably only makes\nsense if you are using single buffering; if you do this, call ", "()\nto make sure that the drawing appears on the screen.)", "\n", " \u2014 The \"special\"\nfunction is called when the user presses certain special keys, such as an arrow\nkey or the Home key.  The parameters are an integer code for the key that was pressed, plus the\nmouse position when the key was pressed:", "GLUT has constants to represent the possible key codes, including\n", ", ", ", ", ", and ", "\nfor the arrow keys and ", " for the Home key. For example,\nyou can check whether the user pressed the left arrow key by testing\n", "\u00a0", ".", "\n", " \u2014 The mouse function is\ncalled both when the user presses and when the user releases a button on the mouse, with a parameter to tell\nwhich of these occurred.  The function will generally look like this:", "The first parameter tells which mouse button was pressed or released; its\nvalue is the constant ", " for the left, ", " for the \nmiddle, and ", " for the right mouse button.  The other\ntwo parameters tell the position of the mouse.  The mouse position\nis given in pixel coordinates with (0,0) in the top left corner of the display area and\nwith y increasing from top to bottom.", "\n", " \u2014 The motion function\nis called when the user moves the mouse while dragging, that is, while a mouse button\nis pressed.  After the user presses the mouse in the OpenGL window, this function will\ncontinue to be called even if the mouse moves outside the window, and the mouse\nrelease event will also be sent to the same window.  The function has two parameters\nto specify the new mouse position:", "\n", " \u2014 The idle function is called by the\nGLUT event loop whenever there are no events waiting to be processed.  The\nidle function has no parameters.  It is called as often as possible, not at\nperiodic intervals.  GLUT also has a timer function, which schedules some function to be\ncalled once, after a specified delay.  To set a timer, call", "and define ", " as", "The parameter to ", " when it is called will be the same integer that was passed as\nthe third parameter to ", ".  If you want to use ", "\nfor animation, then ", " should end with another call to ", ".", "A GLUT window does not have a menu bar, but it is possible to add a hidden popup menu to the window.\nThe menu will appear in response to a mouse click on the display.  You can set whether it\nis triggered by the left, middle, or right mouse button.", "A menu is created using the function ", ",\nwhere the parameter is the name of a function that will be called when the user\nselects a command from the menu.  The function must be defined with a parameter of\ntype ", " that identifies the command that was selected:", "Once the menu has been created, commands are added to the menu by calling the function\n", "(", ").  The first parameter is the string that\nwill appear in the menu.  The second is an ", " that identifies the\ncommand; it is the integer that will be passed to the menu-handling function when\nthe user selects the command from the menu.", "Finally, the function ", "(", ") attaches the menu to the\nwindow.  The parameter specifies which mouse button will trigger the menu.  Possible\nvalues are ", ", ", ", and ", ".\nAs far as I can tell, if a mouse click is used to trigger the popup menu, than the same\nmouse click will ", " also produce a call to the mouse-handler function.", "Note that a call to ", " doesn't mention the menu, and a\ncall to ", " doesn't mention either the menu or the window.\nWhen you call ", ", the menu that is created becomes the \"current\nmenu\" in the GLUT state.  When ", " is called, it adds a command\nto the current menu.  When ", " is called, it attaches the current\nmenu to the current window, which was set by a call to ", ".\nAll this is consistent with the OpenGL \"state machine\" philosophy, where functions\nact by modifying the current state.", "As an example, suppose that we want to let the user set the background color for\nthe display.  We need a function to carry out commands that we will add to the menu.  For example,\nwe might define", "We might have another function to create the menu.  This function would be called\nin ", "(), after calling ", ":", "It's possible to have submenus in a menu.  I won't discuss the procedure here, but you can look\nat the sample program ", " for an example of using submenus.", "In addition to window and event handling, GLUT includes some functions for drawing basic 3D shapes\nsuch as spheres, cones, and ", ".  \nIt has two functions for each shape, a \"solid\" version that draws\nthe shape as a solid object, and a ", " version that draws \nsomething that looks like it's made of wire mesh.  (The wireframe is produced by drawing \njust the outlines of the polygons that make up the object.)  For example, the function", "draws a solid sphere with the given radius, centered at the origin.  Remember that this is\njust an approximation of a sphere, made up of polygons.  For the approximation, the sphere is divided by\nlines of longitude, like the slices of an orange, and by lines of latitude, like a stack of disks.\nThe parameters ", " and ", " tell how many subdivisions to use.  Typical values\nare 32 and 16, but the number that you need to get a good approximation for a sphere depends on the\nsize of the sphere on the screen.  The function ", " has the same parameters but\ndraws only the lines of latitude and longitude.  Functions for a cone, a cylinder, \nand a ", " (doughnut) are similar:", "For a torus, the ", " is the size of the doughnut hole.  The function", "draws a cube of a specified size.\nThere are functions for the other regular polyhedra that have no parameters and draw the \nobject at some fixed size:  ", "(), ", "(),\n", "(), and ", "().\nThere is also ", "(", ") that draws a famous object that is often used as an\nexample.  Here's what the teapot looks like:", "\n", "Wireframe versions of all of the shapes are also available.  For example,\n", "(", ") draws a wireframe teapot.  Note that \nGLUT shapes come with ", " that\nare required for lighting calculations.  However, except for the teapot, they do\nnot come with ", ", which are required for applying\ntextures to objects. ", "GLUT also includes some limited support for drawing text in an OpenGL drawing\ncontext.  I won't discuss that possibility here.  You can check the API\ndocumentation if you are interested, and you can find an example in the\nsample program ", ".", "JOGL is a framework for using OpenGL in Java programs.  It is a large and complex API that\nsupports all versions of OpenGL, but it is fairly easy to use for basic applications.\nIn my examples and discussion, I will be using JOGL\u00a02.3, the latest version\nas of March, 2015.  Note that version 2.3 is not fully compatible with earlier versions.\n(Hopefully, later versions will remain compatible with 2.3.)", "The sample program ", " can be used as a starting\npoint for writing OpenGL programs using JOGL. While it doesn't do anything except open a\nwindow, the program contains the framework needed to do OpenGL drawing, including doing\nanimation, responding to mouse and keyboard events, and setting up a menu.  The source\ncode contains comments that tell you how to use it.", "To use JOGL, you will need two .jar files containing the Java classes for JOGL:\n", " and ", ".  In addition, you will\nneed two native library files.  A native library is\na collection of routines that can be called from Java but are not written in Java.  Routines\nin a native library will work on only kind of computer; you need a different native library\nfor each type of computer on which your program is to be used.  The native libraries for\nJOGL are stored in additional .jar files, which are available in several versions for\ndifferent computers.  For example, for 64-bit Linux, you need\n", " and ", ".\nFor 32-bit Linux, the files are\n", " and ", ".\nIt is unfortunate that there are different versions for 64 and 32 bit operating systems, since\nmany people don't know which they are using.  However, if you are in doubt, you can get\nboth; JOGL will figure out which of the two to use.\nFor Mac\u00a0OS, you need\n", " and ", ".\nFor 64-bit Windows, the files are\n", " and ", ".", "You can get the jar files from the JOGL web site, ", ".\nI extracted them from the very large (54 megabyte) archive file", "\n\n", "\n\n", "I have also made the necessary files available on my own web site, at", "\n\n", "\n\n", "JOGL is open-source, and the files are freely redistributable, according to their\n", ".", "To do JOGL development, you should create a directory somewhere on your computer to hold the jar\nfiles.  Place the two JOGL jar files in that directory, along with the two native library jar files\nfor your platform.  (Having extra native library jar files doesn't hurt, as long as you have\nthe ones that you need.)", "It is possible to do JOGL development on the command line.  You have to tell the\n", " command where to find the two JOGL jar files. You do that in the\nclasspath (\"-cp\") option to the ", " command.  For example, if you are working\nin Linux or MacOS, and if the jar\nfiles happen to be in the same directory where you are working, you might say:", "It's similar for Windows, except that the classpath uses a \";\" instead of a \":\" to\nseparate the items in the list:", "There is an essential period at the end of the classpath, which makes it possible for Java to\nfind .java files in the current directory.\nIf the jar files are not in the current directory,\nyou can use full path names or relative path names to the files.  For example,", "Running a program with the ", " command is exactly similar. For example:", "Note that you don't have to explicitly reference the native library jar files.\nThey just have to be in the same directory with the JOGL jar files.", "I do most of my Java development using the Eclipse IDE (", ").\nTo do development with JOGL in Eclipse, you will have to configure Eclipse\nwith information about the jar files.  To do that, start up Eclipse.  You want to\ncreate a \"User Library\" to contain the jar files:\nOpen the Eclipse Preferences window, and select \"Java\" / \"Build\u00a0Path\" / \"User\u00a0Libraries\"\non the left.  Click the \"New\" button on the right.  Enter \"JOGL\" (or any name you like) as the\nname of the user library.  Make sure that the new user library is selected in the\nlist of libraries, then click the \"Add External Jars\" button.  In the file selection box,\nnavigate to the directory that contains the JOGL jar files, and select the two jar files that\nare needed for JOGL, ", " and ", ".  \n(Again, you do not need to add the native libraries; they just need to be in the same directory\nas the JOGL jar files.)  Click \"Open.\"  The selected\njars will be added to the user library. (You could also add them one at a time, if you don't\nknow how to select multiple files.)  It should\nlook something like this:", "\n", "Click \"OK.\"  The user library has been created. You will only have to do this\nonce, and then you can use it in all of your JOGL projects.", "Now, to use OpenGL in a project, create a new Java project as usual in Eclipse.\nRight-click the project in the Project Explorer view, and select \"Build\u00a0Path\" /\n\"Configure\u00a0Build\u00a0Path\" from the menu.  You will see the project Properties\ndialog, with \"Build Path\" selected on the left.  (You can also access this through the\n\"Properties\" command in the \"Project\" menu.)  Select \"Libraries\" at the top of the\nwindow, and then click the \"Add\u00a0Library\" button.  In the popup window, select \"User\u00a0Library\"\nand click \"Next.\"  In the next window, select your JOGL User Library and click \"Finish.\"\nFinally, click \"OK\" in the main Properties window.  Your project should now be set up\nto do JOGL development.  You should see the JOGL User Library listed as part of the\nproject in the Project Explorer.  Any time you want to start a new JOGL project, you can go through\nthe same setup to add the JOGL User Library to the build path in the project.", "With all that setup out of the way, it's time to talk about actually\nwriting OpenGL programs with Java.  With JOGL,\nwe don't have to talk about mouse and keyboard handling or animation, since that can be done\nin the same way as in any Java program.  You will only need to know about a few classes from\nthe JOGL API.", "First, you need a GUI component on which you can draw using OpenGL.  For that, you\ncan use ", ", which is a subclass of ", ".\n(", " is for use in programs based on the Swing API; an alternative\nis ", ", which is a subclass of the older AWT class\n", ".)  The class is defined in the package ", ".\nAll of the other classes that we will need for basic OpenGL programming \nare in the package ", ".", "JOGL uses Java's event framework to manage OpenGL drawing contexts, and it defines a\ncustom event listener interface, ", ", to manage\nOpenGL events.  To draw on a ", " with OpenGL, you need to\ncreate an object that implements the ", " interface, and\nregister that listener with your ", ".  The ", "\ninterface defines the following methods:\n", "The ", " parameter in these methods tells which OpenGL drawing surface\nis involved.  It will be a reference to the ", ".\n(", "  is an interface that is implemented by\n", " and other OpenGL drawing surfaces.)\nThe ", "() method is a place to do OpenGL initialization.  (According to the\ndocumentation, it can actually be called several times, if the OpenGL context\nneeds to be recreated for some reason. So ", "() should not be used to\ndo initialization that shouldn't be done more than once.) The \n", "() method will be called to give you a chance to\ndo any cleanup before the OpenGL drawing context is destroyed.\nThe ", "() method is called when the window first opens and\nwhenever the size of the ", " changes.\nOpenGL's ", "() function is called automatically before ", "()\nis called, so you won't need to do it yourself.  Usually, you won't need to write\nany code in ", "() or ", "(), but they have to be there to\nsatisfy the definition of the ", " interface.", "The ", "() method is where the actual drawing is done and where you\nwill do most of your work.  It should ordinarily clear the drawing area and completely redraw the scene.\nTake a minute to study an outline for a minimal JOGL program.  It creates a\n", " which also serves as the\n", ":", "At this point, the only other thing you need to know is how to use OpenGL\nfunctions in the program.  In JOGL, the OpenGL\u00a01.1 functions are collected into\nan object of type ", ".  (There are different classes\nfor different versions of OpenGL;  ", " contains\nOpenGL\u00a01.1 functionality, along with later versions that are compatible with 1.1.)\nAn object of type ", " is an OpenGL graphics context,\nin the same way that an object of type ", "\nis a graphics context for ordinary Java 2D drawing.  The statement\n", "in the above program obtains the drawing context for\nthe ", ", that is, for the\n", " in that program.  The name of the\nvariable could, of course, be anything, but ", " or ", " is conventional.", "For the most part, using OpenGL functions in JOGL is the same as in C,\nexcept that the functions are now methods in the object ", ".  For example,\na call to ", "(", ") becomes", "The redundant \"gl.gl\" is a little annoying, but you get used to it.  OpenGL constants\nsuch as ", " are static members of ", ", so that, for\nexample, ", " becomes ", " in JOGL.\nParameter lists for OpenGL functions\nare the same as in the C API in most cases.  One exception is for functions such as ", "()\nthat take an array/pointer parameter in C.  In JOGL, the parameter becomes an ordinary\nJava array, and an extra integer parameter is added to give the position of the data in\nthe array.  Here, for example, is how one might draw a triangle in JOGL, with all the\nvertex coordinates in one array:", "The biggest change in the JOGL API is the use of ", "\ninstead of arrays in functions such as ", ".  This is discussed\nin ", ".  We will see in ", " that texture images also get special\ntreatment in JOGL.", "The JOGL API includes a class named ", " that makes GLUT's\nshape-drawing functions available in Java.  (Since you don't need GLUT's window or event functions\nin Java, only the shape functions are included.)  Class ", "\nis defined in the package ", ".\nTo draw shapes using this class, you need\nto create an object of type GLUT.  It's only necessary to make one of these for use in a program:", "The methods in this object include all the shape-drawing functions from the GLUT C API,\nwith the same names and parameters.  For example:", "(I don't know why these are instance methods in an object rather than\nstatic methods in a class; logically, there is no need for the object.)", "The GLU library is available through the class ", ",\n and it works similarly to GLUT.   That is, you have to create an object of type\n ", ", and the GLU functions will be available as methods\n in that object.  We have encountered GLU only for the functions ", "\n and ", ", which are discussed in ", ".\n For example,", "The JavaScript library ", " was written to accompany and support this textbook.\nIt implements the subset of OpenGL 1.1 that is discussed in ", " and\n", ", except for display lists (", ").\nIt is used in the demos that appear in\nthose chapters.  Many of the sample programs that are discussed in those chapters are available\nin JavaScript versions that use glsim.js.", "If you would like to experiment with OpenGL 1.1, \nbut don't want to go through the trouble of setting up a C or Java environment that supports \nOpenGL programming, you can consider writing your programs as web pages using glsim.js.\nNote that glsim is meant for experimentation and practice only, not for serious applications.", "The OpenGL API that is implemented by glsim.js is essentially the same as the C API, although \nsome of the details of semantics are different.  Of course the techniques for creating a\ndrawing surface and an OpenGL drawing context are specific to JavaScript and differ from\nthose used in GLUT or JOGL.", "To use glsim.js, you need to create an ", " document with a <canvas> element\nto serve as the drawing surface.  The HTML file has to import the script; if glsim.js is in the\nsame directory as the HTML file, you can do that with", "To create the OpenGL drawing context, use the JavaScript command", "where ", " is either a string giving the ", " of the <canvas> element or\nis the JavaScript ", " object corresponding to the <canvas> element. Once you\nhave created the drawing context in this way, any OpenGL commands that you give will apply to\nthe canvas.  To run the program, you just need to open the HTML document in a web browser that\nsupports ", ".", "The easiest way to get started programming is to modify a program that already exists.\nThe sample program ", ", from ", "\nis a very minimal example of using glsim.js.\nThe sample web page ", " can be used as a starting\npoint for writing longer programs that use glsim.js.  It provides a framework for doing OpenGL drawing,\nwith support for animation and mouse and keyboard events.  The code contains comments that tell \nyou how to use it.  Some documentation for the glsim.js library can be found in\n", "."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.6}, {"section_title": "Shapes and Colors in OpenGL 1.1", "chapter_id": "Chapter 3", "section_id": "Section 3.1", "content": ["This section introduces some of the core features\nof OpenGL.  Much of the discussion in this section is limited\nto 2D.  For now, all you need to\nknow about 3D is that it adds a third direction to the\n", " and ", " directions that are used in 2D.\nBy convention, the third direction is called\u00a0", ".\nIn the default coordinate system, the positive direction of\nthe ", "-axis points in a direction perpendicular to\nthe image.", "In the default coordinate system for OpenGL, the image\nshows a region of 3D space in which ", ", ", ", and\n", " all range from minus one to one.  To show a \ndifferent region, you have to apply a \n", ".  For now,\nwe will just use coordinates that lie between -1 and\u00a01.", "A note about programming: OpenGL can be implemented in many different programming\nlanguages, but the API specification more or less assumes that\nthe language is\u00a0C.  For the most part, the C specification\ntranslates directly into other languages.  The main differences\nare due to the special characteristics of arrays in the C language.\nMy examples will follow the C syntax, with a few notes about how\nthings can be different in other languages.  Since I'm following\nthe C API, I will refer to \"functions\" rather than \"subroutines\"\nor \"methods.\"  ", " \nexplains in detail how to write OpenGL programs in C and in Java.\nYou will need to consult that section before you can do any\nactual programming.  The live OpenGL 1.1 demos for\nthis book are written using a ", " simulator that implements\na subset of OpenGL 1.1.  That simulator is also discussed in\n", ".", "OpenGL can draw only a few basic shapes, including points, lines, and\ntriangles.  There is no built-in support for curves or curved surfaces;\nthey must be approximated by simpler shapes.  The basic shapes are\nreferred to as ", ".\nA primitive in OpenGL is defined by its ", ".\nA vertex is simply a point in 3D, given by its ", ", ", ", and\n", " coordinates.  Let's jump right in and see how to draw a \ntriangle.  It takes a few steps:", "Each vertex of the triangle is specified by a call to the function\n", ".  Vertices must be specified between calls to ", "\nand ", ".  The parameter to ", " tells which type of primitive\nis being drawn.  The ", " primitive allows you to draw more than\none triangle: Just specify three vertices for each triangle that you want to draw.", "(I should note that these functions actually just send commands to the ", ". OpenGL\ncan save up batches of commands to transmit together, and the drawing won't actually\nbe done until the commands are transmitted.  To ensure that that happens, the\nfunction ", "() must be called.  In some cases, this function might be\ncalled automatically by an OpenGL API, but you might well run into times when\nyou have to call it yourself.)", "For OpenGL, vertices have three coordinates.  The function ", "\nspecifies the ", " and ", " coordinates of the vertex, and the ", " coordinate \nis set to zero.  There is also a function ", " that specifies\nall three coordinates.  The \"2\" or \"3\" in the name tells how many parameters are\npassed to the function.  The \"f\" at the end of the name indicates that the parameters\nare of type ", ".  In fact, there are other \"glVertex\" functions,\nincluding versions that take parameters of type ", " or ", ",\nwith named like ", " and ", ".  There are even versions that\ntake four parameters, although it won't be clear for a while why they should exist.\nAnd, as we will see later, there are versions that take an array of numbers instead of individual numbers\nas parameters.  The entire set of vertex functions is often referred to as \"glVertex*\", \nwith the \"*\" standing in for the parameter specification.  (The proliferation of\nnames is due to the fact that the C programming language doesn't support overloading\nof function names; that is, C distinguishes functions only by their names and not by\nthe number and type of parameters that are passed to the function.)", "OpenGL 1.1 has ten kinds of primitive.  Seven of them still exist in modern OpenGL; the\nother three have been dropped.  The simplest primitive is ", ",\nwhich simply ", " a point at each vertex of the primitive.\nBy default, a point is rendered as a single pixel.  The size of point primitives can be\nchanged by calling", "where the parameter, ", ", is of type ", " and specifies the\ndiameter of the rendered point, in pixels.  By default,\npoints are squares.  You can get circular points by calling", "The functions ", " and ", " change the OpenGL \"state.\"\nThe state includes all the settings that affect rendering.  We will encounter\nmany state-changing functions.  The functions ", " and ", "\ncan be used to turn many features on and off.  In general, the rule is that any\nrendering feature that requires extra computation is turned off by default.  If you want\nthat feature, you have to turn it on by calling ", " with the appropriate\nparameter.", "There are three primitives for drawing line segments:  ", ",\n", ", and ", ".  ", " draws\ndisconnected line segments; specify two vertices for each segment that you\nwant to draw.  The other two primitives draw connected sequences of line\nsegments.  The only difference is that ", " adds an extra\nline segment from the final vertex back to the first vertex.  Here is what\nyou get if use the same six vertices with the four primitives we have\nseen so far:", "\n", "The points A, B, C, D, E, and F were specified in that order.  In this illustration,\nall the points lie in the same plane, but keep in mind that in general, points can\nbe anywhere in 3D space.", "The width for line primitives can be set by calling ", "(", ").\nThe line width is always specified in pixels.  It is ", " subject to scaling by\ntransformations.", "Let's look at an example.  OpenGL does not have a circle primitive, but we can\napproximate a circle by drawing a polygon with a large number of sides.  To draw\nan outline of the polygon, we can use a ", " primitive:", "This draws an approximation for the circumference of a circle of radius 0.5 with\ncenter at (0,0). \nRemember that to learn how to use examples like this one in\na complete, running program, you will have to read ", ".\nAlso, you might have to make some changes to the code, depending on which OpenGL\nimplementation you are using.", "The next set of primitives is for drawing triangles.  There are three of them:\n", ", ", ", and ", ".", "\n", "The three triangles on the left make up one ", " primitive, with nine vertices.  \nWith that primitive, every set of three vertices makes a separate triangle.  For a\n", " primitive, the first three vertices produce a triangle.  After that,\nevery new vertex adds another triangle to the strip, connecting the new vertex to the\ntwo previous vertices.  Two ", " primitives are shown on the right.\nAgain for a ", ", the first three vertices make a triangle, and every vertex\nafter that adds anther triangle, but in this case, the new triangle is made by connecting\nthe new vertex to the previous vertex and to the very first vertex that was specified (vertex\n\"A\"\u00a0in the picture).  Note that ", " can be used for drawing\nfilled-in polygons.  In this picture, by the way, the dots and lines are not part of the\nprimitive; OpenGL would only draw the filled-in, green interiors of the figures.", "The three remaining primitives, which have been removed from modern OpenGL, are\n", ", ", ", and ", ".  The name \"quad\" is short\nfor quadrilateral, that is, a four-sided polygon.  \nA quad is determined by four vertices.  In order for a quad to be ", " correctly in\nOpenGL, all vertices of the quad must lie in the same plane.  The same is true for polygon\nprimitives.  Similarly, to be rendered correctly, quads and polygons must be\n", " (see ", ").  Since OpenGL doesn't check whether\nthese conditions are satisfied, the use of quads and polygons is error-prone. Since the same shapes can easily\nbe produced with the triangle primitives, they are not really necessary, but here for\nthe record are some examples:", "\n", "The vertices for these primitives are specified in the order A, B, C, ....\nNote how the order differs for the two quad primitives:  For ", ", the vertices for\neach individual quad should be specified in counterclockwise order around the quad;\nfor ", ", the vertices should alternate from one side of the strip\nto the other.", "OpenGL has a large collection of functions that can be used to specify colors for\nthe geometry that we draw.  These functions have names of the form ", ",\nwhere the \"*\" stands for a suffix that gives the number and type of the parameters.\nI should warn you now that for realistic 3D graphics, OpenGL has\na more complicated notion of color that uses a different set of functions.  You will\nlearn about that in the ", ", but \nfor now we will stick to ", ".", "For example, the function ", " has three parameters of\ntype ", ".  The parameters give the red, green, and blue components of the color as\nnumbers in the range 0.0 to\u00a01.0.  (In fact, values outside this range are allowed,\neven negative values.  When color values are used in computations, out-of-range values\nwill be used as given.  When a color actually appears on the screen, its component\nvalues are clamped to the range\u00a00\u00a0to\u00a01.  That is, values less than zero\nare changed to zero, and values greater than one are changed to one.)", "You can add a fourth component to the color by using ", "().  The fourth\ncomponent, known as ", ", is not used in the default \ndrawing mode, but it is possible to configure\nOpenGL to use it as the degree of transparency of the color, similarly to the use of the\nalpha component in the 2D graphics APIs that we have looked at.  You need two commands\nto turn on transparency:", "The first command enables use of the alpha component.  It can be disabled by\ncalling ", "(", "). When the ", " option is\ndisabled, alpha is simply ignored. \nThe second command tells how the alpha component of a\ncolor will be used.  The parameters shown here are the\nmost common; they implement transparency in the usual way.  I should note that \nwhile transparency works fine in 2D, it is much more difficult to use transparency correctly\nin 3D.", "If you would like to use integer color values in the range 0 to 255, you can use\n", "() or ", " to set the color.  In these function names,\n\"ub\" stands for \"unsigned byte.\"  ", " is an eight-bit\ndata type with values in the range 0 to 255.  Here are some examples of commands for setting \ndrawing colors in OpenGL:", "Using any of these functions sets the value of a \"current color,\" which \nis part of the OpenGL state.  When you\ngenerate a vertex with one of the ", " functions, the current color\nis saved along with the vertex coordinates, as an ", "\nof the vertex.  We will see that vertices can have other kinds of attribute as well\nas color.  One interesting point about OpenGL is that colors are associated with\nindividual vertices, not with complete shapes.  By changing the current color\nbetween calls to ", "() and ", "(), you can get a shape in which\ndifferent vertices have different color attributes.  When you do this, OpenGL\nwill compute the colors of pixels inside the shape by\n", " the colors of the vertices.  (Again, since\nOpenGL is extremely configurable, I have to note that interpolation of colors is just the\ndefault behavior.)  For example, here is a triangle in which the three vertices\nare assigned the colors red, green, and blue:", "\n", "This image is often used as a kind of \"Hello World\" example for\nOpenGL.  The triangle can be drawn with the commands", "Note that when drawing a primitive, \nyou do ", " need to explicitly set a color for each vertex, as was done here.\nIf you want a shape that is all one color, you just have to set the current color\nonce, before drawing the shape (or just after the call to ", "().  For\nexample, we can draw a solid yellow triangle with", "Also remember that the color for a vertex is specified ", " the call to\n", " that generates the vertex.", "Here\nis an interactive demo that draws the basic OpenGL triangle, with different\ncolored vertices.  You can control the colors of the vertices to see how the\ninterpolated colors in the interior of the triangle are affected. This is our\nfirst OpenGL example.  The demo actually uses WebGL, so you can use it as\na test to check whether your web browser supports WebGL.", "\n", "\n", "The sample program ", " draws the\nbasic OpenGL triangle using Java.  The program ", "\ndoes the same using the C programming language.  And ", "\nis a version that uses my JavaScript simulator, which implements just the parts of\nOpenGL 1.1 that are covered in this book.\nAny of those programs could be used to experiment with 2D drawing in OpenGL.\nAnd you can use them to test your OpenGL programming environment.", "A common operation is to clear the drawing area by filling it with some\nbackground color.  It is be possible to do that by drawing a\nbig colored rectangle, but OpenGL has a potentially more efficient way to do it.  \nThe function", "sets up a color to be used for clearing the drawing area.\n(This only sets the color; the color isn't used until you actually give\nthe command to clear the drawing area.)\nThe parameters are floating point values in the range 0 to\u00a01.\nThere are no variants of this function; you must provide all four color\ncomponents, and they must be in the range 0 to 1.\nThe default clear color is all zeros, that is, black with an alpha\ncomponent also equal to zero.  The command to do the actual clearing is:", "The correct term for what I have been calling the drawing\narea is the ", ", where \"buffer\" is a general term\nreferring to a region in memory.  OpenGL uses several buffers in addition to the\ncolor buffer.  We will encounter the \"depth buffer\" in just a moment.\nThe ", " command can be used to clear several different\nbuffers at the same time, which can be more efficient than clearing\nthem separately since the clearing can be done in parallel.\nThe parameter to ", " tells it which buffer or buffers to clear.\nTo clear several buffers at once, combine the\nconstants that represent them with an arithmetic OR operation.  For example,", "This is the form of ", " that is generally used in 3D graphics,\nwhere the depth buffer plays an essential role.  For 2D graphics, the\ndepth buffer is generally not used, and the appropriate parameter for\n", " is just ", ".", "We have see that there are versions of ", " and ", "\nthat take different numbers and types of parameters.  There are also versions\nthat let you place all the data for the command in a single array parameter.\nThe names for such versions end with \"v\".  For example:  ", ",\n", ", ", ", and ", ".  The \"v\"\nactually stands for \"", ",\" meaning essentially a one-dimensional \narray of numbers.  For example, in the function call\n", "(", "), ", " would be an array\ncontaining at least three floating point numbers.", "The existence of array parameters in OpenGL forces some differences between\nOpenGL implementations in different programming languages.  Arrays in Java are\ndifferent from arrays in C, and arrays in JavaScript are different from both.\nLet's look at the situation in C first, since that's the language of the\noriginal OpenGL ", ".", "In C, arrays variables are a sort of variation on pointer variables, and\narrays and pointers can be used interchangeably in many circumstances.\nIn fact, in the C API, array parameters are actually specified as pointers.\nFor example, the parameter for ", " is of type \"pointer to float.\"\nThe actual parameter in a call to ", " can be an array variable,\nbut it can also be any pointer that points to the beginning of\na sequence of three floats.  As an example, suppose that we want to draw\na square.  We need two coordinates for each vertex of the square.\nIn C, we can put all 8 coordinates into one array and use ", "\nto pull out the coordinates that we need:", "This example uses \"pointer arithmetic,\" in which ", " represents\na pointer to the N-th element of the array.  An alternative notation would be &", "[", "],\nwhere \"&\" is the address operator, and &", "[", "] means \n\"a pointer to ", "[", "]\".\nThis will all seem very alien to people who are only familiar with Java or JavaScript.\nIn my examples, I will avoid using pointer arithmetic, but I will occasionally use address operators.", "As for Java, the people who designed ", " wanted to preserve\nthe ability to pull data out of the middle of an array.  However, it's not possible to work with pointers\nin Java.  The solution was to replace a pointer parameter in the C API with a pair\nof parameters in the JOGL API\u2014one parameter to specify the array that contains the data\nand one to specify the starting index of the data in the array.  For example, here is how\nthe square-drawing code translates into Java:", "Really not much difference in the parameters, although the zero in the first ", " is a little\nannoying.  The main difference is the prefixes \"gl2\" and \"GL2\", which are required by the object-oriented\nnature of the JOGL API.  I won't say more about JOGL here, but if you need to translate my\nexamples into JOGL, you should keep in mind the extra parameter that is required when working\nwith arrays.", "For the record, here are the ", " and ", " functions that I will use\nin this book.  This is not the complete set that is available in OpenGL:", "For ", ", keep in mind that the \"ub\" variations require integers in the range\n0 to 255, while the \"f\" and \"d\" variations require floating-point numbers in the range\n0.0 to\u00a01.0.", "An obvious point about viewing in 3D is that one object can be behind\nanother object.  When this happens, the back object is hidden from the viewer\nby the front object.  When we create an image of a 3D world, we have to\nmake sure that objects that are supposed to be hidden behind other objects\nare in fact not visible in the image.  This is the \n", ".", "The solution might seem simple enough:  Just draw the objects in order from\nback to front.  If one object is behind another, the back object will be covered\nup later when the front object is drawn.  This is called the\n", ".  It's essentially what you\nare used to doing in 2D.  Unfortunately, it's not so easy to implement.\nFirst of all, you can have objects that intersect, so that part of each\nobject is hidden by the other.  Whatever order you draw the objects in,\nthere will be some points where the wrong object is visible. To fix this, you would have to cut\nthe objects into pieces, along the intersection, and treat the pieces as\nas separate objects.  In fact, there can be problems even if there\nare no intersecting objects:  It's possible to have three non-intersecting\nobjects where the first object hides part of the second, the second hides\npart of the third, and the third hides part of the first. The painter's\nalgorithm will fail regardless of the order in which the three objects are drawn.\nThe solution again is to cut the objects into pieces, but now it's not\nso obvious where to cut.", "Even though these problems can be solved, there is another issue.\nThe correct drawing order can change when the point of view is changed or when a geometric\ntransformation is applied, which means that the correct drawing order has to be\nrecomputed every time that happens.  In an animation, that would mean for every\nframe.", "So, OpenGL does not use the painter's algorithm.  Instead, it uses a technique\ncalled the ", ".  The depth test solves the\nhidden surface problem no matter what order the objects are drawn in, so you can\ndraw them in any order you want!  The term \"depth\" here has to do with the\ndistance from the viewer to the object.  Objects at greater depth are farther\nfrom the viewer.  An object with smaller depth will hide an object with greater\ndepth.  To implement the depth test algorithm, OpenGL stores\na depth value for each pixel in the image.  The extra memory that is used\nto store these depth values  makes up the\n", " that I mentioned earlier.\nDuring the drawing process, the depth buffer is used to keep track of what is currently\nvisible at each pixel.  When a second object is drawn at that pixel, the\ninformation in the depth buffer can be used to decide whether the new object\nis in front of or behind the object that is currently visible there.  If the\nnew object is in front, then the color of the pixel is changed to show the\nnew object, and the depth buffer is also updated.  If the new object is\nbehind the current object, then the data for the new object is discarded and\nthe color and depth buffers are left unchanged.", "By default, the depth test is ", " turned on, which can lead to very\nbad results when drawing in 3D.  You can enable the depth test by calling", "It can be turned off by calling ", "(", ").\nIf you forget to enable the depth test when drawing in 3D, the image that\nyou get will likely be confusing and will make no sense physically.\nYou can also get quite a mess if you forget to clear the depth buffer,\nusing the ", " command shown earlier in this section, \nat the same time that you clear the color buffer.", "Here is a demo \nthat lets you experiment with the depth test.  It also lets\nyou see what happens when part of your geometry extends outside the visible range\nof ", "-values.", "\n", "\n", "Here are are a few details about the implementation of the depth test:\nFor each pixel, the depth buffer stores a\nrepresentation of the distance from the viewer to the point that is currently\nvisible at that pixel.  This value is essentially the ", "-coordinate\nof the point, after any transformations have been applied.  (In fact, the\ndepth buffer is often called the \"z-buffer\".)\nThe range of possible ", "-coordinates is scaled to the range 0 to\u00a01.\nThe fact that there is only a limited range of depth buffer values means\nthat OpenGL can only display objects in a limited range of distances from\nthe viewer.  A depth value of 0 corresponds to the minimal distance;\na depth value of 1 corresponds to the maximal distance.  When you clear\nthe depth buffer, every depth value is set to 1, which can be thought\nof as representing the background of the image.", "You get to choose the range of ", "-values that is visible in the image, \nby the transformations that you apply.  The default range, in the absence of any\ntransformations, is -1\u00a0to\u00a01.  Points with ", "-values outside the\nrange are not visible in the image.  It is a common problem to use too small\na range of ", "-values, so that objects are missing from the scene, or \nhave their fronts or backs cut off, because they lie outside of the visible range.\nYou might be tempted to use a huge range, to make sure that the\nobjects that you want to include in the image are included within the range.\nHowever, that's not a good idea:  The depth buffer has a limited number of bits\nper pixel and therefore a limited amount of accuracy.  The larger the range of\nvalues that it must represent, the harder it is to distinguish between objects\nthat are almost at the same depth.  (Think about what would happen if all objects\nin your scene have depth values between 0.499999 and 0.500001\u2014the depth buffer\nmight see them all as being at exactly the same depth!)", "There is another issue with the depth buffer algorithm.  It \ncan give some strange results when two objects\nhave exactly the same depth value.  Logically, it's not even\nclear which object should be visible, but the real problem with the depth\ntest is that it might show one object at some points and the second object\nat some other points.  This is possible because numerical calculations are\nnot perfectly accurate.  Here an actual example:", "\n", "In the two pictures shown here, a gray square was drawn, followed\nby a white square, followed by a black square.  The squares all lie in the same\nplane.  A very small rotation was applied, to force the computer do some calculations\nbefore drawing the objects.\nThe picture on the left was drawn with the depth test disabled, so that, for example,\nwhen a pixel of the white square was drawn, the computer didn't try to figure out whether it\nlies in front of or behind the gray square; it simply colored the pixel white.\nOn the right, the depth test was enabled, and you can see the strange result.", "Finally, by the way, note that the discussion here assumes that there are no transparent\nobjects.  Unfortunately, the depth test does not handle transparency correctly, since\ntransparency means that two or more objects can contribute to the color of the pixel,\nbut the depth test assumes that the pixel color is the color of the object nearest\nto the viewer at that point.  To handle 3D transparency correctly in OpenGL, you pretty much have\nto resort to implementing the painter's algorithm by hand, at least for the transparent\nobjects in the scene."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.1}, {"section_title": "Polygonal Meshes and glDrawArrays", "chapter_id": "Chapter 3", "section_id": "Section 3.4", "content": ["We have drawn only very simple shapes with OpenGL.  In this section,\nwe look at how more complex shapes can be represented in a way that\nis convenient for rendering in OpenGL, and we introduce a new,\nmore efficient way to draw OpenGL primitives.", "OpenGL can only directly render points, lines, and polygons.  \n(In fact, in modern OpenGL, the only polygons that are used are triangles.)\nA ", " can be represented exactly, since a polyhedron\nhas faces that are polygons.  On the other hand, if only polygons are available, then \na curved surface, such as the surface of a sphere, can only be approximated.\nA polyhedron can be represented, or a curved surface can be approximated,\nas a ", ",\nthat is, a set of polygons that are connected along their edges.  If the\npolygons are small, the approximation can look like a curved surface.  (We will\nsee in the ", " how lighting effects\ncan be used to make a polygonal mesh look more like a curved surface and\nless like a polyhedron.)", "So, our problem is to represent a set of polygons\u2014most often a set\nof triangles.  We start by defining a convenient way to represent such a\nset as a data structure.", "The polygons in a polygonal mesh are also referred to as\n\"faces\" (as in the faces of a polyhedron), and one of the\nprimary means for representing a polygonal mesh is as\nan ", ", or IFS.\n", "The data for an IFS includes a list of all the\nvertices that appear in the mesh, giving the coordinates\nof each vertex.  A vertex can then be identified by\nan integer that specifies its index, or position, in the list.\nAs an example, consider this \"house,\" a polyhedron with\n10 vertices and 9 faces:\n", "\n", "The vertex list for this polyhedron has the form", "The order of the vertices is completely arbitrary.  The purpose is simply\nto allow each vertex to be identified by an integer.", "To describe one of the polygonal faces of a mesh, we just have to list its vertices, \nin order going around the polygon. For an IFS, we\ncan specify a vertex by giving its index in the list.  For example,\nwe can say that one of the triangular faces of the pyramid is the polygon\nformed by vertex #3, vertex #2, and vertex #4.  So, we can complete our\ndata for the mesh by giving a list of vertex indices for each face.\nHere is the face data for the house.  Remember that the numbers in parenthese\nare indices into the vertex list:", "Again, the order in which the faces are listed in arbitrary.\nThere is also some freedom in how the vertices for a face are listed.\nYou can start with any vertex.  Once you've picked a starting vertex,\nthere are two possible orderings, corresponding to the two possible\ndirections in which you can go around the circumference of the polygon.\nFor example, starting with vertex 0, the first face in the list could\nbe specified either as (0,1,2,3) or as (0,3,2,1).  However, the\nfirst possibility is the right one in this case, for the following reason.\nA polygon in 3D can be viewed from either side; we can think of it\nas having two faces, facing in opposite directions.  It turns out that\nit is often convenient to consider one of those faces to be the\n\"front face\" of the polygon and one to be the \"back face.\"\nFor a polyhedron like the house, the front face is the one that faces\nthe outside of the polyhedron.  The usual rule is that the\nvertices of a polygon should be listed in counter-clockwise order\nwhen looking at the front face of the polygon.  When looking at the\nback face, the vertices will be listed in clockwise order.  This is\nthe default rule used by OpenGL.", "\n", "The vertex and face data for an indexed face set can be represented as\na pair of two-dimensional arrays.  For the house, in a version for Java, we could use", "In most cases, there will be additional data for the IFS.  For example, if we\nwant to color the faces of the polyhedron, with a different color for each face,\nthen we could add another array, ", ", to hold the color data.\nEach element of ", " would be an array of three ", "\nvalues in the range 0.0 to 1.0, giving the ", " components for\none of the faces.  With this setup, we could use the following code to draw\nthe polyhedron, using Java and ", ":", "Note that every vertex index is used three or four times in the face data.\nWith the IFS representation, a vertex is represented in the face list by a single\ninteger.  This representation uses\nless memory space than the alternative, which would be to write out\nthe vertex in full each time it occurs in the face data.  For the house example,\nthe IFS representation uses 64 numbers to represent the vertices and faces of the polygonal mesh,\nas opposed to 102 numbers for the alternative representation.", "Indexed face sets have another advantage.  Suppose that we want to modify the shape of\nthe polygon mesh by moving its vertices.  We might do this in each frame of\nan animation, as a way of \"morphing\" the shape from one form to another.\nSince only the positions of the vertices are changing, and not the way\nthat they are connected together, it will only be necessary to update the\n30 numbers in the vertex list.  The values in the face list will remain\nunchanged.", "There are other ways to store the data for an IFS.\nIn C, for example, where two-dimensional arrays are more problematic, we might use one\ndimensional arrays for the data.  In that case, we would store all the vertex coordinates\nin a single array.  The length of the vertex array would\nbe three times the number of vertices, and the data for vertex number ", " will\nbegin at index 3*", " in the array.  For the face list, we have to deal with the\nfact that not all faces have the same number of vertices.  A common solution is to\nadd a -1 to the array after the data for each face.  In C, where it is not possible\nto determine the length of an array, we also need variables to store the number of\nvertices and the number of faces.  Using this representation,\nthe data for the house becomes:", "After adding a ", " array to hold color data for the faces,\nwe can use the following C code to draw the house:", "Note the use of the C address operator, &.  For example,\n", " is a pointer to element number ", " in\nthe ", " array.  That element is the first of the three color\ncomponent values for face number\u00a0", ".  This matches the parameter\ntype for ", " in C, since the parameter is a pointer type.", "We could easily draw the edges of the polyhedron instead of the\nfaces simply by using ", " instead of ", "\nin the drawing code (and probably leaving out the color changes).\nAn interesting issue comes up if we want to draw both the faces and\nthe edges.  This can be a nice effect, but we run into a problem with\nthe depth test:  Pixels along the edges lie at the same depth as\npixels on the faces.  As discussed in ", ", the depth\ntest cannot handle this situation well.  However, OpenGL has a solution:\na feature called \"polygon offset.\"  This feature can \nadjust the depth, in clip coordinates, of a polygon, in order to avoid having two\nobjects exactly at the same depth.  To apply polygon offset, you need to\nset the amount of offset by calling", "The second parameter gives the amount of offset, in units determined by the first parameter.\nThe meaning of the first parameter is somewhat obscure; a value of 1 seems to work\nin all cases.  You also have to enable the ", " feature while\ndrawing the faces.  An outline for the procedure is\n", "There is a sample program that can draw the house and a number of other\npolyhedra.  It uses drawing code very similar to what we have looked at here,\nincluding polygon offset.  The program is also an example of using the\ncamera and trackball API that was discussed in ", ",\nso that the user can rotate a polyhedron by dragging it with the mouse.\nThe program has menus that allow the user to turn rendering of edges \nand faces on and off, plus some other options.  The Java version of the\nprogram is ", ", and the C version is\n", ".  To get at the menu in the C version,\nright-click on the display.  The data for the polyhedra is\ncreated in ", " and \n", ".  And here is live demo version of the program\nfor you to try:", "\n", "\n", "All of the OpenGL commands that we have seen so far were part of the original\nOpenGL\u00a01.0.  OpenGL\u00a01.1 added some features to increase performance.  One\ncomplaint about the original OpenGL was the large number of function calls needed to\ndraw a ", " \nusing functions such as ", " and ", " with\n", ".  To address this issue, OpenGL\u00a01.1 introduced the\nfunctions ", " and ", ".  We will look at ", "\nfirst.  There are some differences between the C and the Java versions of the API.\nWe consider the C version first and will deal with the changes necessary for the\nJava version in the next subsection.", "When using ", ",\nall of the data needed to draw a primitive, including vertex coordinates, colors,\nand other vertex ", ", can be packed into\narrays.  Once that is done, the primitive can be drawn with a single call to\n", ".  Recall that a primitive such\nas a ", " or a ", " can include a large number of\nvertices, so that the reduction in the number of function calls can be \nsubstantial.", "To use ", ", you must store all of the vertex coordinates\nfor a primitive in a single one-dimensional array.  You can use an array of\n", ", ", ", or ", ", and you can have 2, 3, or\n4 coordinates for each vertex.  The data in the array are the same numbers\nthat you would pass as parameters to a function such as ", ",\nin the same order.  You need to tell OpenGL where to find the data by\ncalling", "The ", " parameter is the number of coordinates per vertex.  \n(You have to provide the same number of coordinates\nfor each vertex.)   The ", " is a constant that tells the data\ntype of each of the numbers in the array.  The possible values are\n", ", ", ", and ", ".  This parameter is\nnecessary because the array can contain different types of data.\nThe constant that you provide here must match the type of the array.\nThe ", " is usually 0, meaning that the data\nvalues are stored in consecutive locations in the array; if that is\nnot the case, then ", " gives the distance ", " between\nthe location of the data for one vertex and location for the next vertex.\n(This would allow you to store other data, along with the vertex coordinates,\nin the same array.) The final parameter is the array that contains the data.\nIt is listed as being of type \"", "\", which is a C data type for a\npointer that can point to any type of data.  (Recall that an array variable\nin C is a kind of pointer, so you can just pass an array variable as the\nfourth parameter.)  For example, suppose that we want to draw a square in \nthe ", "-plane.  We can set up the vertex array with", "In addition to setting the location of the vertex coordinates, you have to enable\nuse of the array by calling", "OpenGL ignores the vertex pointer except when this state is enabled.  You can use\n", " to disable use of the vertex array.  Finally,\nin order to actually draw the primitive, you would call the function", "This function call corresponds to one use of glBegin/glEnd.\nThe ", " tells which primitive type is being drawn,\nsuch as ", " or ", ".\nThe same ten primitive types that can be used with ", " can\nbe used here.  The parameter ", " is the number of the first\nvertex that is to used for drawing the primitive.  Note that the position\nis given in terms of vertex number; the corresponding array index would\nbe the vertex number times the number of coordinates per vertex, which\nwas set in the call to ", ".\nThe ", " parameter is the number of vertices to be used,\njust as if ", " were called ", " times.\nOften, ", " will be zero, and ", "\nwill be the total number of vertices in the array.  The command for\ndrawing the square in our example would be", "Often there is other data associated with each vertex in addition to\nthe vertex coordinates.  For example, you might want to specify a different\ncolor for each vertex.  The colors for the vertices can be put into\nanother array.  You have to specify the location of the data by calling", "which works just like ", ".  And you need to enable\nthe color array by calling", "With this setup, when you call ", ", OpenGL\nwill pull a color from the color array for each vertex at the\nsame time that it pulls the vertex coordinates from the vertex array.\nLater, we will encounter other kinds of vertex data besides coordinates\nand color that can be dealt with in much the same way.", "Let's put this together to draw the standard OpenGL red/green/blue triangle,\nwhich we drew using ", " in ", ".\nSince the vertices of the triangle have different colors, we will use a color array\nin addition to the vertex array.", "In practice, not all of this code has to be in the same place.  The function\nthat does the actual drawing, ", ", must be in the display routine\nthat draws the image.  The rest could be in the display routine, but could also\nbe done, for example, in an initialization routine.", "The function ", " is similar to ", ", but it is\ndesigned for use with data in a format similar to an indexed face set.\nWith ", ", OpenGL pulls data from the enabled arrays in order,\nvertex 0, then vertex 1, then vertex 2, and so on.  With ", ",\nyou provide a list of vertex numbers.  OpenGL will go through the list of\nvertex numbers, pulling data for the specified vertices from the arrays.\nThe advantage of this comes, as with indexed face sets, from the fact that the\nsame vertex can be reused several times.", "To use ", " to draw a primitive, you need an array to store the vertex numbers.\nThe numbers in the array can be 8, 16, or 32 bit integers.  (They are supposed to be unsigned\nintegers, but arrays of regular positive integers will also work.)  You also need\narrays to store the vertex coordinates and other vertex data, and you must enable \nthose arrays in the same way as for ", ", using functions such as\n", " and ", ".  To actually draw the primitive,\ncall the functions", "Here, ", " is one of the ten primitive types such as ", ",\n", " is the number of vertices to be drawn, ", " specifies the\ntype of data in the array, and ", " is the array that holds the list of\nvertex numbers.  The ", " must be given as one of the constants\n", ", ", ", or ", " to specify\n8, 16, or 32 bit integers respectively.", "As an example, we can draw a cube.  We can draw all six faces of the cube as one\nprimitive of type ", ".  We need the vertex coordinates in one array\nand the vertex numbers for the faces in another array.  I will also use a color\narray for vertex colors.  The vertex colors will be interpolated to pixels on the\nfaces, just like the red/green/blue triangle.\nHere is code that could be used to draw the cube.\nAgain, this would not necessarily be all in the same part of a program:", "Note that the second parameter is the number of vertices, not the number of quads.", "The sample program ", " uses this code\nto draw a cube.  It draws a second cube using ", ".  The Java\nversion is ", ", but you need to \nread the next subsection before you can understand it.  There is also a\nJavaScript version, ", ".", "Ordinary Java arrays are not suitable for use with ", " and\n", ", partly because of the format in which data is stored in them and\npartly because of inefficiency in transfer of data between Java arrays and the ", ".\nThese problems are solved by using ", ".\nThe term \"nio\" here refers to the package ", ", which contains classes for\ninput/output.  A \"buffer\" in this case is an object of the class ", "\nor one of its subclasses, such as ", " or ", ".\nFinally, \"direct\" means that the buffer is optimized for direct transfer of data between memory\nand other devices such as the GPU.  Like an array, an nio buffer is a numbered sequence of\nelements, all of the same type. A ", ", for example, contains\na numbered sequence of values of type ", ".  There are subclasses of\n", " for all of Java's primitive data types except ", ".", "Nio buffers are used in JOGL in several places where\narrays are used in the C API.  For example, JOGL has the following\n", " method in the ", " class:", "Only the last parameter differs from the C version.  The buffer can be of type\n", ", ", ",\nor ", ".  The type of buffer must match the ", "\nparameter in the method.   Functions such as ", " work the same way,\nand ", " takes the form", "where the ", " can be of type ", ",\n", ", or ", "\nto match the ", " ", ", ", ",\nor ", ".", "The class ", " contains static\nutility methods for working with direct nio buffers.  The easiest to use are\nmethods that create a buffer from a Java array.  For example, the method\n", "(", ") takes a ", "\narray as its parameter and creates a ", " of the\nsame length and containing the same data as the array.  These methods are\nused to create the buffers in the sample program ", ".\nFor example,", "The buffers can then be used when drawing the cube:", "There are also methods such as ", "(", "),\nwhich creates a ", " of length ", ".  Remember that\nan nio ", ", like an array, is simply a linear sequence of elements of a given type.\nIn fact, just as for an array, it is possible to refer to items in a buffer by\ntheir index or position in that sequence.  Suppose that ", " is a variable\nof type ", ", ", " is an ", " and\n", " is a ", ".  Then", "copies the value of ", " into position number ", " in the buffer.\nSimilarly, ", "(", ") can be used to retrieve the value at\nindex ", " in the buffer.  These methods make it possible to work with buffers\nin much the same way that you can work with arrays.", "All of the OpenGL drawing commands that we have considered so far have an unfortunate\ninefficiency when the same object is going be drawn more than once:  The commands and\ndata for drawing that object must be transmitted to the GPU each time the object is\ndrawn.  It should be possible to store information on the GPU, so that it can be reused\nwithout retransmitting it.  We will look at two techniques for doing this:\n", " and \n", " (VBOs).  Display lists\nwere part of the original OpenGL 1.0, but they are not part of the modern OpenGL API.\nVBOs were introduced in OpenGL 1.5 and are still important in modern OpenGL; we will discuss\nthem only briefly here and will consider them more fully when we get to ", ".", "Display lists are useful when the same sequence of OpenGL commands will be used\nseveral times.  A display list is a list of graphics commands and the data used by those commands.\nA display list can be stored in a GPU.\nThe contents of the display list only have to be transmitted once\nto the GPU.  Once a list has been created, it can be \"called.\"\nThe key point is that calling a list requires only one OpenGL\ncommand.  Although the same list of commands still has to be executed, only one\ncommand has to be transmitted from the CPU to the graphics card, and then the full power\nof hardware acceleration can be used to execute the commands at the highest possible\nspeed.", "Note that calling a display list twice can result in two different effects, since the effect\ncan depend on the OpenGL state at the time the display list is called.  For\nexample, a display list that generates the geometry for a sphere can draw\nspheres in different locations, as long as different modeling transforms are\nin effect each time the list is called.  The list can also produce spheres\nof different colors, as long as the drawing color is changed between calls to the\nlist.", "If you want to use a display list,\nyou first have to ask for an integer that will identify that list to the GPU.\nThis is done with a command such as", "The return value is an ", " which will be the identifier for the list.\nThe parameter to ", " is also an ", ", which is usually\u00a01.  \n(You can actually ask for\nseveral list IDs at once; the parameter tells how many you want.\nThe list IDs will be consecutive integers, so that if\n", " is the return value from ", "(3), then the identifiers for\nthe three lists will be ", ", ", "\u00a0+\u00a01, and ", "\u00a0+\u00a02.)", "Once you've allocated a list in this way, you can store commands into it.  If ", "\nis the ID for the list, you would do this with code of the form:", "The parameter ", " means that you only want to store commands into the list,\nnot execute them.  If you use the alternative parameter ", ",\nthen the commands will be executed immediately as well as stored in the list for later reuse.", "Once you have created a display list in this way, you can call the list with the command", "The effect of this command is to tell the GPU to execute a list that it\nhas already stored.  You can tell the graphics card that a list is no longer\nneeded by calling", "The second parameter in this method call plays the same role as the parameter in\n", "; that is, it allows you delete several sequentially numbered lists.\nDeleting a list when you are through with it allows the GPU to reuse the \nmemory that was used by that list.", "Vertex buffer objects take a different approach to reusing information.\nThey only store data, not commands.  A VBO is similar to an array.  In fact, it\nis essentially an array that can be stored on the GPU for efficiency of reuse. \nThere are OpenGL commands to\ncreate and delete VBOs and to transfer data from an array on the CPU side\ninto a VBO on the GPU.  You can configure ", "() and ", "()\nto take the data from a VBO instead of from an ordinary array (in C) or from an nio Buffer (in JOGL).\nThis means that you can send the data once to the GPU and use it any number of times.", "I will not discuss how to use VBOs here, since it was not a part of OpenGL 1.1.\nHowever, there is a sample program that lets you compare different techniques for\nrendering a complex image.  The C version of the program is\n", ", and the Java version is\n", ".  The program draws 1331 spheres,\narranged in an 11-by-11-by-11 cube.  The spheres are different colors, with the amount\nof red in the color varying along one axis, the amount of green along a second axis,\nand the amount of blue along the third.  Each sphere has 66 vertices, whose coordinates can\nbe computed using the math functions ", " and ", ".  The program allows\nyou to select from five different rendering methods, and it shows the time that\nit takes to render the spheres using the selected method.  (The Java version has a drop-down\nmenu for selecting the method; in the C version, right-click the image to get the menu.\nYou can use your mouse to rotate the cube of spheres, both to get a better view and to generate more\ndata for computing the average render time.)  The five rendering techniques are:", " In my own experiments, I found, as expected, that display \nlists and VBOs gave the shortest rendering times, with little difference between the two.\nThere were some interesting differences between the results for the C version and the\nresults for the Java version, which seem to be due to the fact that function calls in C\nare more efficient than method calls in Java.  You should try the program on your own computer, \nand compare the rendering times for the various rendering methods."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.4}, {"section_title": "Projection and Viewing", "chapter_id": "Chapter 3", "section_id": "Section 3.3", "content": ["In the ", ", we looked at the\nmodeling transformation, which transforms from object coordinates to world\ncoordinates.  However, when working with OpenGL 1.1, you need to know about\nseveral other coordinate systems and the transforms between them.  We discuss\nthem in this section.", "We start with an overview of the various coordinate systems.  Some of\nthis is review, and some of it is new.", "The coordinates that you actually use for drawing an object are called\n", ".  The object coordinate system is chosen to be\nconvenient for the object that is being drawn.   A ", "\ncan then be applied to set the size, orientation, and position of the object\nin the overall scene (or, in the case of hierarchical modeling, in the\nobject coordinate system of a larger, more complex object).  The modeling transformation\nis the first that is applied to the vertices of an object.", "The coordinates in which you build the complete scene are called\n", ".  These are the coordinates for\nthe overall scene, the imaginary 3D world that you are creating.\nThe modeling transformation maps from object coordinates to world coordinates.\n", "In the real world, what you see depends on where you are standing\nand the direction in which you are looking.  That is, you can't make a picture\nof the scene until you know the position of the \"viewer\" and where the\nviewer is looking\u2014and, if you think about it, how the viewer's head\nis tilted.  For the purposes of OpenGL, we imagine that the\nviewer is attached to their own individual coordinate system, which is known\nas ", ".\nIn this coordinate system, the viewer is at the origin, (0,0,0), looking\nin the direction of the negative ", "-axis, the positive direction of\nthe ", "-axis is pointing straight up, and the  ", "-axis is pointing\nto the right.  This is a viewer-centric coordinate\nsystem.   In other words, eye coordinates are (almost) the coordinates\nthat you actually want to ", " for drawing on the screen.\nThe transform from world coordinates to eye coordinates is called the \n", ". ", "If this is confusing, think of it this way: We are free to use any\ncoordinate system that we want on the world.  Eye coordinates are the\nnatural coordinate system for making a picture of the world as seen by\na viewer.  If we used a different coordinate system (world coordinates) when building the world,\nthen we have to transform those coordinates to eye coordinates to find\nout what the viewer actually sees.  That transformation is the viewing transform.\n", "Note, by the way, that OpenGL doesn't keep track of separate modeling\nand viewing transforms.  They are combined into a single transform, which is known as the\n", ".  In fact, even though world coordinates\nmight seem to be the most important and natural coordinate system, OpenGL doesn't have any\nrepresentation for them and desn't use them internally.  For OpenGL, only\nobject and eye coordinates have meaning.  OpenGL goes directly from object coordinates to\neye coordinates by applying the modelview transformation.", "We are not done.  The viewer can't see the entire 3D world,\nonly the part that fits into the ", ", which is the\nrectangular region of the screen or other display device where the\nimage will be drawn.  We say that the scene is\n\"clipped\" by the edges of the viewport.  Furthermore, in OpenGL,\nthe viewer can see only a limited range of ", "-values in the eye coordinate system.  \nPoints with larger\nor smaller ", "-values are clipped away and are not rendered into the image.\n(This is not, of course, the way that viewing works in the real world, but it's\nrequired by the use of the ", " in OpenGL.  See ", ".)\nThe volume of space that\nis actually rendered into the image is called the ", ".\nThings inside the view volume make it into the image; things that are not\nin the view volume are clipped and cannot be seen.  For purposes of drawing,\nOpenGL applies a coordinate transform that maps the view volume\nonto a ", ".  The cube is centered at the origin and extends from -1 to\n1 in the x-direction, in the y-direction, and in the z-direction.  The coordinate\nsystem on this cube is referred to as ", ".\nThe transformation from eye coordinates to clip coordinates is\ncalled the ", ".  At this point, we\nhaven't quite projected the 3D scene onto a 2D surface, but we can now do\nso simply by discarding the z-coordinate.  (The z-coordinate, however, is still\nneeded to provide the depth information that is needed for the depth test.)\n", "We ", " aren't done.  In the end, when things are actually drawn, there are\n", ", the 2D coordinate system in which the\nactual drawing takes place on a physical display device such as the computer screen.\nOrdinarily, in  device coordinates, the pixel is the unit of measure.\nThe drawing region is a rectangle of pixels.  This is the rectangle that is called\nthe viewport.  The ", "\ntakes x and y from the clip coordinates and scales them to fit\nthe viewport.", "Let's go through the sequence of transformations one more time.  Think of a \n", ", such\nas a line or triangle, that is part of the scene and that might appear in the image\nthat we want to make of the scene.  The primitive goes through the following sequence of\noperations:\n", "\n", "We need to consider these transforms in more detail and see how to use them in\nOpenGL\u00a01.1.", "The simplest of the transforms is the viewport transform.  It transforms\n", " and ", " clip coordinates to the coordinates that are used on\nthe display device.  To specify the viewport transform, it is only necessary\nto specify the rectangle on the device where the scene will be rendered.\nThis is done using the ", " function.", "OpenGL must be provided with a drawing surface by the environment in which\nit is running, such as ", " for Java or the ", " library for C.  \nThat drawing surface is a rectangular grid of pixels, with a horizontal size\nand a vertical size.  OpenGL uses a coordinate system on the drawing surface\nthat puts (0,0) at the lower left, with y increasing from bottom to top and\nx increasing from left to right.  When the drawing surface is first given to\nOpenGL, the viewport is set to be the entire drawing surface.  However,\nit is possible for OpenGL to draw to a different rectangle by calling", "where (", ") is the lower left corner of the viewport, in the\ndrawing surface coordinate system, and ", " and ", "\nare the size of the viewport.  Clip coordinates from -1 to 1 will then\nbe mapped to the specified viewport.  Note that this means in particular\nthat drawing is limited to the viewport.  It is not an error for the\nviewport to extend outside of the drawing surface, though it would be\nunusual to set up that situation deliberately.", "When the size of the drawing surface changes, such as when the user resizes\na window that contains the drawing surface, OpenGL does not automatically\nchange the viewport to match the new size.  However, the environment in which OpenGL\nis running might do that for you.  (See ", " for information \nabout how this is handled by JOGL and GLUT.)", "\n", " is often used to draw several different scenes, or several views\nof the same scene, on the same drawing surface.  Suppose, for example, that\nwe want to draw two scenes, side-by-side, and that the drawing surface is 600-by-400 pixels.\nAn outline for how to do that is very simple:\n", "The first ", " command establishes a 300-by-400 pixel viewport with\nits lower left corner at (0,0).  That is, the lower left corner of the viewport is\nat the lower left corner of the drawing surface.  This viewport fills the left\nhalf of the drawing surface.  Similarly, the second viewport, with its lower left\ncorner at (300,0), fills the right half of the drawing surface.", "We turn next to the projection transformation.  Like any transform, the projection\nis represented in OpenGL as a ", ".  OpenGL keeps track of the \nprojection matrix separately from the matrix that represents the modelview\ntransformation.  The same transform functions, such as ", ", can be applied to both\nmatrices, so OpenGL needs some way to know which matrix those functions apply to.\nThis is determined by an OpenGL state property called the ", ".\nThe value of the matrix mode is a constant such as ", " or ", ".\nWhen a function such as ", " is called, it modifies a matrix; which matrix\nis modified depends on the current value of the matrix mode.\nThe value is set by calling the function ", ".  The initial value is\n", ".  This means that if you want to work on the projection matrix,\nyou must first call", "If you want to go back to working on the modelview matrix, you must call", "In my programs, I generally set the matrix mode to ", ", set up\nthe projection transformation, and and then immediately set the matrix mode\nback to ", ".  This means that anywhere else in the program, I can\nbe sure that the matrix mode is ", ".", "To help you to understand projection, remember that\na 3D image can show only a part of the infinite 3D world.  The view volume\nis the part of the world that is visible in the image.  The view volume\nis determined by a combination of the viewing transformation and the projection\ntransformation.  The viewing transform determines where the viewer is located and\nwhat direction the viewer is facing,  but it doesn't say how much of the world the \nviewer can see.  The projection transform does that:  It specifies the shape and \nextent of the region that is in view.   Think of the viewer as a camera, with \na big invisible box attached to the front of the camera that encloses\nthe part of the world that that camera has in view.  The inside of the box is the\nview volume.  As the camera moves around in the world, the box moves with it,\nand the view volume changes.  But the shape and size of the box don't change.\nThe shape and size of the box correspond to the projection transform.\nThe position and orientation of the camera correspond to the viewing\ntransform.", "This is all just another way of saying that, mathematically, \nthe OpenGL projection transformation transforms eye coordinates\nto clip coordinates, mapping the view volume onto the 2-by-2-by-2 clipping cube that contains\neverything that will be visible in the image.  To specify a projection just means\nspecifying the size and shape of the view volume, relative to the viewer.", "There are two general types of projection, ", "\nand ", ".  Perspective projection is more\nphysically realistic.  That is, it shows what you would see if the OpenGL display rectangle\non your computer screen were a window into an actual 3D world (one that\ncould extend in front of the screen as well as behind it).  It shows a view that\nyou could get by taking a picture of a 3D world with a camera.  In a perspective\nview, the apparent size of an object depends on how far it is away from the\nviewer.  Only things that are in front of the viewer can be seen.  In fact,\nignoring clipping in the ", "-direction for the moment,\nthe part of the world that is in view is an infinite pyramid, with the\nviewer at the apex of the pyramid, and with the sides of the pyramid\npassing through the sides of the viewport rectangle.", "However, OpenGL can't actually show everything in this pyramid, because of\nits use of the ", " to solve the ", ".  Since the\ndepth buffer can only store a finite range of depth values, it can't\nrepresent the entire range of depth values for the infinite pyramid that \nis theoretically in view.  Only objects in a certain range of distances from \nthe viewer can be part of the image.  That range of distances \nis specified by two values,  ", " and ", ".  \nFor a perspective transformation, both of these values must be positive numbers, and ", " must be greater \nthan ", ".  Anything that is closer to the viewer than the ", " distance\nor farther away than the ", " distance is discarded and does not appear\nin the rendered image.  The volume of space that is represented in the image\nis thus a \"truncated pyramid.\"  This pyramid is the view volume for a perspective\nprojection: \n", "\n", "The view volume is bounded by six planes\u2014the four sides plus the top and bottom\nof the truncated pyramid.  These\nplanes are called clipping planes because anything that lies on the wrong side\nof each plane is clipped away. The projection transformation maps the six sides of the\ntruncated pyramid in eye coordinates to the six sides of the clipping cube in clip coordinates.", "In OpenGL, setting up the projection transformation is equivalent to defining\nthe view volume.  For a perspective transformation, you have to set up a view volume\nthat is a truncated pyramid.  A rather obscure term for this shape is a ", ".\nA perspective transformation can be set up with the ", " command:", "The last two parameters specify the ", " and ", " distances from the\nviewer, as already discussed.  The viewer is assumed to be at the origin, (0,0,0), facing\nin the direction of the negative z-axis.  (This is the eye coordinate system.)  So,\nthe near clipping plane is at ", " = \u2212", ", and the far clipping\nplane is at ", " = \u2212", ".  (Notice the minus signs!)\nThe first four parameters specify the sides of the pyramid: \n ", ", ", ", ", ", and ", " specify the\nhorizontal and vertical limits of the view volume ", ".\nFor example, the coordinates of the upper-left corner of the small end of the\npyramid are (", ",\u00a0", ",\u00a0-", ").  The ", "\nand ", " limits at the far clipping plane are larger, usually much larger, than\nthe limits specified in the ", " command.", "Note that ", " and ", " limits in ", " are usually symmetrical\nabout zero.  That is, ", " is usually equal to the negative of ", " and ", " \nis usually equal to the negative of ", ".  However, this is not required.  It is possible to have\nasymmetrical view volumes where the z-axis does not point directly down the center of the view.", "Since the matrix mode must be set to ", " to work on the projection transformation,\n", " is often used in a code segment of the form", "The call to ", " ensures that the starting point is the ", ".\nThis is important since ", " modifies the existing projection matrix rather than replacing\nit, and although it is theoretically possible, you don't even want to try to think about what would happen if you\ncombine several projection transformations into one.", "Compared to perspective projections, orthographic projections are easier to understand: \nIn an orthographic projection, the 3D world is projected onto\na 2D image by discarding the ", "-coordinate of the eye-coordinate system.  This type of projection\nis unrealistic in that it is not what a viewer would see.  For example, the apparent size of\nan object does not depend on its distance from the viewer.  Objects in back of the viewer\nas well as in front of the viewer can be visible in the image.  \nOrthographic projections are still useful, however, especially in interactive modeling \nprograms where it is useful to see true sizes and angles, undistorted by perspective.", "In fact, it's not really clear what\nit means to say that there is a viewer in the case of orthographic projection.\nNevertheless, for orthographic projection in OpenGL, there is considered to be a viewer.\nThe viewer is located at the eye-coordinate \norigin, facing in the direction of the negative z-axis.  Theoretically, a rectangular corridor\nextending infinitely in both directions, in front of the viewer and in back, would be in view.\nHowever, as with perspective projection, only a finite segment of this infinite corridor can\nactually be shown in an OpenGL image.  This finite view volume is a parallelepiped\u2014a\u00a0rectangular\nsolid\u2014that is cut out of the infinite corridor by a ", " clipping plane and a ", "\nclipping plane.  The value of ", " must be greater than ", ", but for an orthographic\nprojection, the value of ", " is allowed to be negative, putting the \"near\" clipping plane\nbehind the viewer, as shown in the lower section of this illustration: \n", "\n", "Note that a negative value for ", " puts the near clipping plane on the ", "\n", "-axis, which is behind the viewer.", "An orthographic projection can be set up in OpenGL using the ", " method, which is\nhas the following form:\n", "The first four parameters specify the ", "- and ", "-coordinates of the left, right, bottom, and\ntop of the view volume.  Note that the last two parameters are ", " and ", ",\nnot ", " and ", ".  In fact, the minimum z-value for the view volume is\n\u2212", " and the maximum z-value is \u2212", ".  However, it is often\nthe case that ", " = \u2212", ", and if that is true then the minimum and\nmaximum z-values turn out to be ", " and ", " after all!", "As with ", ", ", " should be called when the matrix mode is\n", ".  As an example, suppose that we want the view volume to be the\nbox centered at the origin containing ", ", ", ", and ", " values in\nthe range from -10 to 10.  This can be accomplished with", "Now, as it turns out, the effect of ", " in this simple case is exactly the\nsame as the effect of ", "(0.1,\u00a00.1,\u00a0-0.1), since the projection\njust scales the box down by a factor of 10.  But it's usually better to think of projection\nas a different sort of thing from scaling.  (The minus sign on the ", " scaling\nfactor is there because projection reverses the direction of the ", "-axis, transforming\nthe conventionally right-handed eye coordinate system into OpenGL's left-handed default\ncoordinate system.)", "The ", " method is not particularly easy to use.  There is a library\nknown as ", " that contains some utility functions for use with OpenGL.\nThe GLU library includes the method ", " as an easier way to set up a perspective\nprojection.  The command", "can be used instead of ", ".  The ", " is the vertical\nangle, measured in degrees, between the upper side of the view volume pyramid and the lower side.  \nTypical values are in the range 30 to 60 degrees.  The ", " parameter is the\naspect ratio of the view, that is, the width of a cross-section of the pyramid\ndivided by its height.  The value of ", " should\ngenerally be set to the aspect ratio of the viewport.  The ", " and ", "\nparameters in ", " have the same meaning as for ", ".", "\"Modeling\" and \"viewing\" might seem like very different things, conceptually, but OpenGL\ncombines them into a single transformation.  This is because there is no way to distinguish\nbetween them in principle; the difference is purely conceptual.  That is,\na given transformation can be considered to be either a modeling transformation or a\nviewing transformation, depending on how you think about it.  (One significant difference, conceptually,\nis that the viewing transformation usually replies to an entire scene as a whole, while modeling\ntransformations are applied to individual objects.  But this is not a difference in principle.) \nWe have already seen something\nsimilar in 2D graphics (", "), but let's think about how it works in 3D.", "For example, suppose that there is a model of a house at the origin, facing towards the\ndirection of the positive ", "-axis.  Suppose the viewer is on the positive ", "-axis,\nlooking back towards the origin.  The viewer is looking directly at the front of the\nhouse. Now, you might apply a modeling transformation to the house, to rotate it by\n90 degrees about the ", "-axis.  After this transformation, the house is facing in\nthe positive direction of the ", "-axis, and  the viewer is looking directly\nat the ", " side of the house.  On the other hand, you might rotate the\n", " by ", " 90 degrees about the ", "-axis.  This would put the viewer on\nthe negative ", "-axis, which would give it a view of the ", " side of the house.\nThe net result after either transformation is that the viewer ends up with exactly the\nsame view of the house.  Either transformation can be implemented in OpenGL with the\ncommand\n", "That is, this command represents either a modeling transformation that rotates\nan object by 90 degrees or a viewing transformation that rotates the viewer by -90\ndegrees about the ", "-axis.  Note that the effect on the viewer is the inverse of the effect on the object.\nModeling and viewing transforms are always related in this way.  For example, if you\nare looking at an object, you can move yourself 5 feet to the ", " (viewing transform), or you can move\nthe object 5 feet to the ", " (modeling transform).  In either case, you end up with\nthe same view of the object.  Both transformations would be represented in OpenGL as", "This even works for scaling:  If the viewer ", ", it will look to the\nviewer exactly the same as if the world is expanding, and vice-versa.", "Although modeling and viewing transformations are the same in principle, they remain\nvery different conceptually, and they are typically applied at different points in the code.\nIn general when drawing a scene, you will do the following:  (1)\u00a0Load the identity matrix,\nfor a well-defined starting point; (2)\u00a0apply the viewing transformation; and (3)\u00a0draw\nthe objects in the scene, each with its own modeling transformation.  Remember that OpenGL keeps\ntrack of several transformations, and that this must all be done while the modelview transform\nis current; if you are not sure of that then before step \n(1), you should call ", "(", ").\nDuring step (3), you will probably use ", "() and ", "() to limit\neach modeling transform to a particular object.", "After loading the identity matrix, the viewer is in the default position, at the origin,\nlooking down the negative ", "-axis, with the positive ", "-axis pointing upwards in the\nview.  Suppose, for example, that we would like to move the viewer from its\ndefault location at the origin back along the positive z-axis \nto the point (0,0,20).  This operation has exactly the same effect as moving the world,\nand the objects that it contains, 20 units in the negative direction along the z-axis.  Whichever\noperation is performed, the viewer ends up in exactly the same position relative to the\nobjects.  Both operations are implemented by the same OpenGL\ncommand, ", "(0,0,-20).  For another example,\nsuppose that we use two commands", "to establish the viewing transformation.  As a modeling transform, these commands would\nfirst translate an object 10 units \nin the positive ", "-direction, then rotate the object 90 degrees about the ", "-axis.  (Remember that\nmodeling transformations are applied to objects in the order opposite to their order in the code.)\nWhat do these commands do as a viewing transformation?  The effect on the view is the inverse\nof the effect on objects.  The inverse of \"translate 90 then rotate 10\" is \"rotate -10\nthen translate -90.\"  That is, to do the inverse, you have to undo the rotation ", "\nyou undo the translation.   The effect as a viewing transformation is first to rotate the viewer\nby -90 degrees about the ", "-axis, then to translate the viewer by -10 along the ", "-axis.\n(You should think about how the two interpretations affect the view of an object that starts out\nat the origin.)  Note that the order in which viewing transformations are applied is the\n", " the order in which they occur in the code.", "\nHere is a demo that illustrates the equivalence between modeling and viewing.\nThe translucent gray box in the lower images represents the view volume that is used\nto create the image that is shown in the upper left.  In this case, the projection\nis a perspective projection, and the view volume is a frustum.\nRead the help text in the demo for more information.\n\n", "\n", "\n", "It can be difficult to set up a view by combining rotations, scalings, and translations,\nso OpenGL provides an easier way to set up a typical view.\nThe command is not part of OpenGL itself but is part of the GLU library.", "The GLU library provides the following convenient method for\nsetting up a viewing transformation:", "This method places the viewer at the point (", ",", ",", "),\nlooking towards the point (", ",", ",", ").\nThe viewer is oriented so that the vector (", ",", ",", ")\npoints upwards in the viewer's view.  For example, to position the viewer on the positive\n", "-axis, 10 units from the origin, looking back at the origin, with the positive\ndirection of the ", "-axis pointing up as usual, use", "With all this, we can give an outline for a typical display routine for drawing\nan image of a 3D scene with OpenGL\u00a01.1:", "Projection and viewing are often discussed using the analogy of a ", ".\nA real camera is used to take a picture of a 3D world.  For 3D graphics, it useful to\nimagine using a virtual camera to do the same thing.  Setting up the viewing transformation is like\npositioning and pointing the camera.  The projection transformation determines the properties\nof the camera:  What is its field of view, what sort of lens does it use?  \n(Of course, the analogy breaks for OpenGL in at least one respect,\nsince a real camera doesn't do clipping in its ", "-direction.)", "I have written a camera utility to implement this idea.  The camera is meant to\ntake over the job of setting the projection and view.  Instead of doing that by hand, you set\nproperties of the camera.  The API is available for both C and Java.  The two versions are\nsomewhat different because the Java version is object-oriented.  I will discuss the\nC implementation first. (See ", " for basic information about\nprogramming OpenGL in C and Java.  For an example of using a camera in a program, see\nthe polyhedron viewer example in the ", ".\nNote also that there is a version of the camera for use with my JavaScript simulator for OpenGL;\nit is part of the simulator library ", " and has an API almost identical\nto the Java API.)", "In C, the camera is defined by the sample .c file, ", " and\na corresponding header file, ", ".  Full documentation for the API can\nbe found in the header file.  To use the camera, you should ", " at\nthe start of your program, and when you compile the program, you should include ", " in\nthe list of files that you want to compile.  The camera depends on the GLU library and on\nC's standard math library, so you have to make sure that those libraries are available\nwhen it is compiled.  To use the camera, you should call", "to set up the projection and viewing transform before drawing the scene.\nCalling this function replaces the usual code\ncode for setting up the projection and viewing transformations. It leaves the\nmatrix mode set to ", ".", "The remaining functions in the API are used to configure the camera.  This would usually\nbe done as part of initialization, but it is possible to change the configuration at any time.\nHowever, remember that the settings are not used until you call ", ".\nAvailable functions include:", "In many cases, the default settings are sufficient.  Note in particular how\n", " and ", " work together to set up the view and\nprojection.  The parameters to ", " represent three points in\nworld coordinates.  The view reference point, (", "),\nshould be somewhere in the middle of the scene that you want to render.\nThe parameters to ", " define a box about that view reference\npoint that should contain everything that you want to appear in the image.", "For use with ", " in Java, the camera API is implemented as\na class named ", ", defined in the file\n", ".  The camera\nis meant for use with a ", " or\n", " that is being used as an OpenGL drawing surface.\nTo use a camera, create an object of type ", " as\nan instance variable:", "Before drawing the scene, call", "where ", " is the OpenGL drawing context of type ", ".\n(Note the presence of the parameter ", ", which was not necessary in\u00a0C; it is\nrequired because the OpenGL drawing context in JOGL is implemented as an object.)\nAs in the C version, this sets the viewing and projection transformations and\ncan replace any other code that you would use for that purpose.\nThe functions for configuring the camera are the same in Java as in C,\nexcept that they become methods in the ", " object, and true/false\nparameters are ", " instead of ", ":", "The camera comes with a simulated \"trackball.\"\nThe trackball allows the user to rotate the view by clicking and dragging the mouse\non the display.  To use it with ", " in C, you just need to install a mouse function and\na mouse motion function by calling", "The functions\n", " and ", " are defined\nas part of the camera API and are declared and documented in ", ".\nThe trackball works by modifying the viewing transformation associated with\nthe camera, and it only works if ", "() is called at the\nbeginning of the display function to set the viewing and projection\ntransformations.  To install a trackball for use with a ", "\nobject in JOGL, call", "where ", " is the component on which the camera\nis used."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.3}, {"section_title": "Hardware and Software", "chapter_id": "Chapter 1", "section_id": "Section 1.3", "content": ["We will be using OpenGL as the primary basis for 3D graphics programming.\nThe original version of OpenGL was released in 1992 by a company named\nSilicon Graphics, which was known for its graphics workstations\u2014powerful,\nexpensive computers designed for intensive graphical applications.  (Today,\nyou probably have more graphics computing power on your smart phone.)  OpenGL\nis supported by the graphics hardware in most modern computing devices, including\ndesktop computers, laptops, and many mobile devices.  This section will give\nyou a bit of background about the history of OpenGL and about the graphics \nhardware that supports it.", "In the first desktop computers, the contents of the screen were managed\ndirectly by the ", ".  For example, to draw a line segment on the screen, the CPU\nwould run a loop to set the color of each pixel that lies along the line.\nNeedless to say, graphics could take up a lot of the CPU's time.  And graphics\nperformance was very slow, compared to what we expect today.  So what has changed?\nComputers are much faster in general, of course, but the big change is that\nin modern computers, graphics processing is done by a specialized component\ncalled a ", ", or Graphics Processing Unit.  A GPU includes processors\nfor doing graphics computations; in fact, it can include a large number of such\nprocessors that work in parallel to greatly speed up graphical operations.  \nIt also includes its own dedicated memory for storing things like images and \nlists of coordinates.  GPU processors have very fast\naccess to data that is stored in GPU memory\u2014much faster than their access to data\nstored in the computer's main memory.", "To draw a line or perform some other graphical operation, the CPU simply has to\nsend commands, along with any necessary data, to the GPU, which is responsible\nfor actually carrying out those commands.  The CPU offloads most of the graphical\nwork to the GPU, which is optimized to carry out that work very quickly.\nThe set of commands that the GPU understands make up the ", "\nof the GPU.  OpenGL is an example of a graphics API, and most GPUs support\nOpenGL in the sense that they can understand OpenGL commands, or at least\nthat OpenGL commands can efficiently be translated into commands that the\nGPU can understand.", "OpenGL is not the only graphics API.  The best-known alternative is probably \nDirect3D, a 3D graphics API used for Microsoft Windows.  OpenGL is more widely\navailable, since it is not limited to Microsoft, but Direct3D is supported by\nmost graphics cards, and it has often introduced new features earlier than OpenGL. ", "I have said that OpenGL is an API, but in fact it is a series of APIs that have\nbeen subject to repeated extension and revision.  The current version, in early\n2015, is 4.5, and it is very different from the 1.0 version from 1992.  Furthermore,\nthere is a specialized version called OpengGL\u00a0ES for \"embedded systems\" such\nas mobile phones and tablets.  And there is also WebGL, for use in Web browsers,\nwhich is basically a port of OpenGL ES\u00a02.0.  It's useful to know something\nabout how and why OpenGL has changed.", "First of all, you should know that OpenGL was designed as a \"client/server\"\nsystem.  The server, which is responsible for controlling the computer's\ndisplay and performing graphics computations, carries out commands issued by the\nclient.  Typically, the server is a GPU, including its graphics processors and memory.\nThe server executes OpenGL commands.  The client is the CPU in the same computer, along \nwith the application program that it is running. OpenGL commands come from the\nprogram that is running on the CPU.  However,\nit is actually possible to run OpenGL programs remotely over a network.  That\nis, you can execute an application program on a remote computer (the OpenGL client), while\nthe graphics computations and display are done on the computer that you are\nactually using (the OpenGL server).", "The key idea is that the client and the server are separate components, and there\nis a communication channel between those components.  OpenGL commands and the\ndata that they need are communicated from the client (the CPU) to the server (the GPU)\nover that channel.  The capacity of the channel can be a limiting factor in graphics\nperformance.  Think of drawing an image onto the screen.  If the GPU can draw the\nimage in microseconds, but it takes milliseconds to send the data for the image\nfrom the CPU to the GPU, then the great speed of the GPU is irrelevant\u2014most of\nthe time that it takes to draw the image is communication time.", "For this reason, one of the driving factors in the evolution of OpenGL has been\nthe desire to limit the amount of communication that is needed between the CPU and\nthe GPU.  One approach is to store information in the GPU's memory.  If some data\nis going to be used several times, it can be transmitted to the GPU once and\nstored in memory there, where it will be immediately accessible to the GPU.\nAnother approach is to try to decrease the number of OpenGL commands that must\nbe transmitted to the GPU to draw a given image.", "OpenGL draws ", " such as triangles.\nSpecifying a primitive means specifying ", "\nand ", " for each of its ", ".  In the\noriginal OpenGL\u00a01.0, a separate command was used to specify the coordinates of each vertex,\nand a command was needed each time the value of an attribute changed.  To draw a single \ntriangle would require three or more commands.  Drawing a complex object made up of\nthousands of triangles would take many thousands of commands.  Even in OpenGL\u00a01.1,\nit became possible to draw such an object with a single command instead of thousands.  All the data\nfor the object would be loaded into arrays, which could then be sent in a single\nstep to the GPU.  Unfortunately, if the object was going to be drawn more than\nonce, then the data would have to be retransmitted each time the object was drawn.\nThis was fixed in OpenGL\u00a01.5 with ", ".\nA VBO is a block of memory in the GPU that can store the coordinates or attribute values for\na set of vertices.  This makes it possible to reuse the data without having to retransmit it\nfrom the CPU to the GPU every time it is used.", "Similarly, OpenGL 1.1 introduced ", "\nto make it possible to store several images on the GPU for use as ", ".\nThis means that texture images that are going to be reused several times can be loaded once\ninto the GPU, so that the GPU can easily switch between images without having to reload them.", "As new capabilities were added to OpenGL, the API grew in size.  But the growth was still\noutpaced by the invention of new, more sophisticated techniques for doing graphics.  Some\nof these new techniques were added to OpenGL, but\nthe problem is that no matter how many features you add, there will always be demands for \nnew features\u2014as well as complaints that all the new features are making things too \ncomplicated! OpenGL was a giant machine, with new pieces always being tacked onto it, \nbut still not pleasing everyone. The real solution was to make the machine ", ".\nWith OpenGL 2.0, it became possible to write programs to be executed as part of the\ngraphical computation in the GPU.  The programs are run on the GPU at GPU speed.\nA programmer who wants to use a new graphics technique can write a program to \nimplement the feature and just hand it to the GPU.  The OpenGL API doesn't have to\nbe changed.  The only thing that the API has to support is the ability to send programs\nto the GPU for execution.", "The programs are called ", " (although the term does't\nreally describe what most of them actually do).  The first shaders to be introduced were\n", " and ", ".\nWhen a ", " is drawn, some work has to be done at each vertex of the primitive,\nsuch as applying a ", " to the vertex coodinates or\nusing the ", " and global ", " environment\nto compute the color of that vertex.  A vertex shader is a program that can take over the\njob of doing such \"per-vertex\" computations.  Similarly, some work has to be done for each\npixel inside the primitive.  A fragment shader can take over the job of performing such\n\"per-pixel\" computations.  (Fragment shaders are also called pixel shaders.)", "The idea of programmable graphics hardware was very successful\u2014so successful that\nin OpenGL\u00a03.0, the usual per-vertex and per-fragment processing\nwas deprecated (meaning that its use was discouraged). \nAnd in OpenGL\u00a03.1, it was removed from\nthe OpenGL standard, although it is still present as an optional extension.  In practice,\nall the original features of OpenGL are still supported in desktop versions of OpenGL and will\nprobably continue to be available in the future.  On the embedded system side, however,\nwith OpenGL\u00a0ES\u00a02.0 and later, the use of shaders is mandatory, and a large part\nof the OpenGL\u00a01.1 API has been completely removed.\nWebGL, the version of OpenGL for use in web browsers, \nis based on OpenGL\u00a0ES\u00a02.0, and it also requires shaders to get anything at all done.\nNevertheless, we will begin our study of OpenGL with version 1.1.  Most of the concepts and\nmany of the details from that version are still relevant, and it offers an easier entry point\nfor someone new to 3D graphics programming.", "OpenGL shaders are written in ", " (OpenGL Shading Language).  Like\nOpenGL itself, GLSL has gone through several versions. We will spend some time later in the\ncourse studying GLSL\u00a0ES\u00a01.0, the version used with WebGL\u00a01.0 and\nOpenGL\u00a0ES\u00a02.0.  GLSL uses a syntax similar to the C programming language.", "As a final remark on GPU hardware, I should note that the computations that are done for\ndifferent vertices are pretty much independent, and so can potentially be done in parallel.\nThe same is true of the computations for different fragments.  In fact, GPUs can\nhave hundreds or thousands of processors that can operate in parallel.  Admittedly, the\nindividual processors are much less powerful than a CPU, but then typical per-vertex\nand per-fragment computations are not very complicated.  The large number of processors,\nand the large amount of parallelism that is possible in graphics computations, makes\nfor impressive graphics performance even on fairly inexpensive GPUs."], "chapter_title": "Introduction", "id": 1.3}, {"section_title": "Elements of 3D Graphics", "chapter_id": "Chapter 1", "section_id": "Section 1.2", "content": ["When we turn to 3D graphics, we find that the most common approaches have more\nin common with ", " than with ", ".  \nThat is, the content of an image is specified as a list of geometric objects. \nThe technique is referred to as ", ". The starting point\nis to construct an \"artificial 3D world\" as a collection of simple geometric shapes, arranged in\nthree-dimensional space.  The objects can have ", " that, combined with\nglobal properties of the world, determine the appearance of the objects.\nOften, the range of basic shapes is very limited, perhaps including only points, line segments,\nand triangles.  A more complex shape such as a polygon or sphere can be built or\napproximated as a collection of more basic shapes, if it is not itself considered\nto be basic.  To make a two-dimensional image of the scene,\nthe scene is ", " from three dimensions \ndown to two dimensions.  Projection is the equivalent of\ntaking a photograph of the scene.  Let's look at how it all works in a\nlittle more detail.", "\n", "\nWe start with an empty 3D space or \"world.\" Of course, this space exists only conceptually, but it's \nuseful to think of it as real and to be able to visualize it in your mind.  The space needs\na ", " that associates each point in the space with three numbers, \nusually referred to as the ", ", ", ", and ", " coordinates of the point.\nThis coordinate system is referred to as \"world coordinates.\"", "We want to build a scene inside the world, made up of geometric objects.  For example, \nwe can specify a line segment in the scene by giving the coordinates of its two endpoints, \nand we can specify a triangle by giving the coordinates of its three vertices.  The smallest building\nblocks that we have to work with, such as line segments and triangles, are called\n", ".  Different graphics\nsystems make different sets of primitive available, but in many cases only very basic\nshapes such as lines and triangles are considered primitive.  A complex scene can contain\na large number of primitives, and it would be very difficult to create the scene by\ngiving explicit coordinates for each individual primitive.  The solution, as any programmer\nshould immediately guess, is to chunk together primitives into reusable components.\nFor example, for a scene that contains several automobiles, we might create a geometric\nmodel of a wheel.  An automobile can be modeled as four wheels together with models of\nother components.  And we could then use several copies of the automobile model in the\nscene.  Note that once a geometric model has been designed, it can be used as a\ncomponent in more complex models.  This is referred to as ", ".", "Suppose that we have constructed a model of a wheel out of geometric primitives.\nWhen that wheel is moved into position in the model of an automobile, the coordinates of\nall of its primitives will have to be adjusted.  So what exactly have we gained by building\nthe wheel?  The point is that all of the coordinates in the wheel are adjusted\n", ".  That is, to place the wheel in the automobile, we just have to\nspecify a single adjustment that is applied to the wheel as a whole.  The type of \"adjustment\"\nthat is used is called a ", " (or geometric\ntransformation).  A geometric transform\nis used to adjust the size, orientation, and position of a geometric object.  When making\na model of an automobile, we build ", " wheel.  We then apply four different\ntransforms to the wheel model to add four copies of the wheel to the automobile.\nSimilarly, we can add several automobiles to a scene by applying different transforms\nto the same automobile model.", "The three most basic kinds of geometric transform are called ", ",\n", ", and ", ".\nA scaling transform is used to set the size of an object, that is, to make it bigger or smaller\nby some specified factor.\nA rotation transform is used to set an object's orientation, by rotating it by some  angle\nabout some specific axis.  A translation transform is used to set the position of an\nobject, by displacing it by a given amount from its original position.\nIn this book, we will meet these transformations first in two dimensions, where they\nare easier to understand. But it is in 3D graphics that they become truly essential.", "\n", "\nGeometric shapes by themselves are not very interesting.  You have to be able\nto set their appearance.  This is done by assigning ", "\nto the geometric objects.  An obvious attribute is color, but getting a realistic \nappearance turns out to be a lot more complicated than simply specifying a color\nfor each primitive.  In 3D graphics, instead of color, we usually talk about\n", ".   The term material here refers to the properties that determine the\nintrinsic visual appearance of a surface.  Essentially, this means how the surface\ninteracts with light that hits the surface.  Material properties can include a basic\ncolor as well as other properties such as shininess, roughness, and transparency.\n", "One of the most useful kinds of material property is a ", ".\nIn most general terms, a texture is a way of varying material properties from point-to-point\non a surface.  The most common use of texture is to allow different colors for different\npoints.  This is done by using a 2D image as a texture, which can be applied to a surface\nso that the image looks like it is \"painted\" onto the surface.\nHowever, texture can also refer to changing values for things like transparency or\n\"bumpiness.\"  Textures allow us to add detail to a scene without using a huge number of\ngeometric primitives; instead, you can use a smaller number of textured primitives.", "A material is an intrinsic property of an object, but the actual appearance of the\nobject also depends on the environment in which the object is viewed.\nIn the real world, you don't see anything unless there is some light in the environment.\nThe same is true in 3D graphics:  you have to add simulated ", "\nto a scene.  There can be several sources of light in a scene.  Each light source can have \nits own color, intensity, and direction or position.  The light from those sources will \nthen interact with the material properties of the objects in the scene.  Support for\nlighting in a graphics system can range from fairly simple to very complex and computationally\nintensive.", "\n", "\nIn general, the ultimate goal of 3D graphics is to produce 2D images of the 3D world.  \nThe transformation from 3D to 2D involves ", " and\n", ".  The world looks different when seen from different\npoints of view.  To set up a point of view, we need to specify the position of the viewer \nand the direction that the viewer is looking.  It is also necessary to specify\nan \"up\" direction, a direction that will be pointing upwards in the final image.\nThis can be thought of as placing a \"virtual camera\" into the scene.  Once the\nview is set up, the world as seen from that point of view can be projected into\n2D.  Projection is analogous to taking a picture with the camera.", "The final step in 3D graphics is to assign colors to individual pixels in \nthe 2D image. This process is called ", ",\nand the whole process of producing an image is referred to as ", "\nthe scene.", "In many cases the ultimate goal is not to create a single image, but to create an\n", ", consisting a sequence of images that show the world\nat different times.  In an animation, there are small changes from one image in the\nsequence to the next.  Almost any aspect of a scene can change during an animation,\nincluding coordinates of primitives, transformations, material properties, and the view.\nFor example, an object can be made to grow over the course of an animation by\ngradually increasing the scale factor in a scaling transformation that is applied to\nthe object.  And changing the view during an animation can give the effect of\nmoving or flying through the scene.  Of course, it can be difficult to compute\nthe necessary changes.  There are many techniques to help with the computation.  One of the most \nimportant is to use a \"physics engine,\" which computes the motion \nand interaction of objects based on the laws of physics.  (However, you won't learn\nabout physics engines in this book.)"], "chapter_title": "Introduction", "id": 1.2}, {"section_title": "Painting and Drawing", "chapter_id": "Chapter 1", "section_id": "Section 1.1", "content": ["The main focus of this book is three-dimensional (3D) graphics,\nwhere most of the work goes into producing a 3D model of a scene.\nBut ultimately, in almost all cases, the end result of a computer\ngraphics project is a two-dimensional image.   And of course,\nthe direct production and manipulation of 2D images is an important\ntopic in its own right.  Furthermore, a lot of ideas carry over\nfrom two dimensions to three.  So, it makes sense to start\nwith graphics in 2D.", "An image that is presented on the computer screen is made up of ", ".  The screen\nconsists of a rectangular grid of pixels, arranged in rows and columns.  The pixels are small enough that\nthey are not easy to see individually.  In fact, for many very high-resolution displays, they become\nessentially invisible.  At a given time, each pixel can\nshow only one color.  Most screens these days use 24-bit color, where\na color can be specified by three 8-bit numbers, giving the levels of red, green, and blue in the color.\nAny color that can be shown on the screen is made up of some combination of these three \"primary\" colors.\nOther formats are possible, such as ", ", where each pixel is some shade of gray\nand the pixel color is given by one number that specifies the level of gray on a black-to-white scale.\nTypically, 256 shades of gray are used.\nEarly computer screens used ", ", where only a small set of colors, usually\n16 or 256, could be displayed.  For an indexed color display, there is a numbered list of possible colors,\nand the color of a pixel is specified by an integer giving the position of the color in the list.", "In any case, the color values for all the pixels on the screen are stored in a large block of memory\nknown as a ", ".  Changing the image on the screen requires changing\ncolor values that are stored in the frame buffer.  The screen is redrawn many times per second, so that almost immediately\nafter the color values are changed in the frame buffer, the colors of the pixels on the screen will\nbe changed to match, and the displayed image will change.", "A computer screen used in this way is the basic model of ", ".  The term \"raster\" technically\nrefers to the mechanism used on older vacuum tube computer monitors:  An electron beam would move along\nthe rows of pixels, making them glow.  The beam was moved\nacross the screen by powerful magnets that would deflect the path of the electrons.\nThe stronger the beam, the brighter the glow of the pixel, so the brightness of\nthe pixels could be controlled by modulating the intensity of the electron beam.  The color values\nstored in the frame buffer were used to determine the intensity of the electron beam.  (For a color\nscreen, each pixel had a red dot, a green dot, and a blue dot, which were separately illuminated by\nthe beam.)\n", "A modern flat-screen computer monitor is not a raster in the same sense.  There is no moving\nelectron beam.  The mechanism that controls the colors of the pixels is different for different\ntypes of screen.  But the screen is still made up of pixels, and the color values for all the\npixels are still stored in a frame buffer.  The idea of an image consisting of a grid of\npixels, with numerical color values for each pixel, defines raster graphics.\n", "Although images on the computer screen are represented using pixels, specifying individual\npixel colors is not always the best way to create an image.  Another way\nis to specify the basic geometric objects that it contains, shapes such as lines, circles,\ntriangles, and rectangles.  This is the idea that defines ", ":  Represent an\nimage as a list of the geometric shapes that it contains.  To make things more interesting,\nthe shapes can have ", ", such as the thickness of a line or the\ncolor that fills a rectangle.  Of course, not every image can be composed from simple\ngeometric shapes. This approach certainly wouldn't work for a picture of a beautiful sunset\n(or for most any other photographic image).  However, it works well for many types of\nimages, such as architectural blueprints and scientific illustrations.\n", "In fact, early in the history of computing, vector graphics was even used directly on\ncomputer screens.  When the first graphical computer displays were developed,\nraster displays were too slow and expensive to be practical.  Fortunately, it was\npossible to use vacuum tube technology in another way:  The electron beam could be made\nto directly draw a line on the screen, simply by sweeping the beam along that line.\nA vector graphics display would store a ", " of lines\nthat should appear on the screen.  Since a point on the screen would glow only very briefly\nafter being illuminated by the electron beam, the graphics display would go through\nthe display list over and over, continually redrawing all the lines on the list.\nTo change the image, it would only be necessary to change the contents of the display list.\nOf course, if the display list became too long, the image would start to flicker\nbecause a line would have a chance to visibly fade before its next turn to be redrawn.\n", "But here is the point: For an image that can be specified as a reasonably small\nnumber of geometric shapes, the amount of information needed to represent the image\nis much smaller using a vector representation than using a raster representation.\nConsider an image made up of one thousand line segments.  For a vector representation\nof the image, you only need to store the coordinates of two thousand points, the\nendpoints of the lines.  This would take up only a few kilobytes of memory.  To store\nthe image in a frame buffer for a raster display would require much more memory.  \nSimilarly, a vector display could\ndraw the lines on the screen more quickly than a raster display could copy the the same image from\nthe frame buffer to the screen.  (As soon as raster displays became fast and \ninexpensive, however, they quickly displaced vector displays because of their\nability to display all types of images reasonably well.)\n", "The divide between raster graphics and vector graphics persists in several areas\nof computer graphics.  For example, it can be seen in a division between two\ncategories of programs that can be used to create images:  ", "\nand ", ".  In a painting program, the image is represented\nas a grid of pixels, and the user creates an image by assigning colors to pixels.\nThis might be done by using a \"drawing tool\" that acts like a painter's brush,\nor even by tools that draw geometric shapes such as lines or rectangles. But the point in a\npainting program is to color the individual pixels, and it is only the pixel colors that are saved.\nTo make this clearer, suppose that you use a painting program to draw a house, then\ndraw a tree in front of the house.  If you then erase the tree, you'll only reveal a blank\nbackground, not a house.  In fact, the image never really contained a \"house\" at all\u2014only\nindividually colored pixels that the viewer might perceive as making up a picture of a house.\n", "In a drawing program, the user creates an image by adding geometric shapes, and the\nimage is represented as a list of those shapes.  If you place a house shape (or collection of shapes\nmaking up a house) in the image, and you then place a tree shape on\ntop of the house, the house is still there, since it\nis stored in the list of shapes that the image contains.  If you delete the tree, the house will\nstill be in the image, just as it was before you added the tree.  Furthermore, you should be able\nto select one of the shapes in the image and move it or change its size, so drawing programs\noffer a rich set of editing operations that are not possible in painting programs.  (The reverse,\nhowever, is also true.)\n", "A practical program for image creation and editing might combine elements of painting and\ndrawing, although one or the other is usually dominant.  For example, a drawing program might\nallow the user to include a raster-type image, treating it as one shape.  A painting program\nmight let the user create \"layers,\" which are separate images that can be layered one on top\nof another to create the final image.  The layers can then be manipulated much like the shapes\nin a drawing program (so that you could keep both your house and your tree in separate layers,\neven if in the image of the house is in back of the tree).\n", "Two well-known graphics programs are ", " and ", ".\n", " is in the category of painting programs, while ", " is more of a drawing program.\nIn the world of free software, the GNU image-processing program, ", ", is a good alternative\nto ", ", while ", " is a reasonably capable free drawing program.\nShort introductions to Gimp and Inkscape can be found in ", ".", "The divide between raster and vector graphics also appears in the field of graphics file formats.\nThere are many ways to represent an image as data stored in a file.  If the original image\nis to be recovered from the bits stored in the file, the representation must follow some exact, known\nspecification.  Such a specification is called a graphics file format.  Some popular graphics file\nformats include GIF, PNG, JPEG, and SVG.  Most images used on the Web are GIF, PNG, or JPEG.\nModern web browsers also have support for SVG images.\n", "GIF, PNG, and JPEG are basically raster graphics formats; an image is specified by storing a color \nvalue for each pixel.  GIF is an older file format,\nwhich has largely been superseded by PNG, but you can still find GIF images on the web.  (The GIF\nformat supports animated images, so GIFs are often used for simple animations on Web pages.) GIF uses\nan indexed color model with a maximum of 256 colors.  PNG can use either indexed or full 24-bit color,\nwhile JPEG is meant for full color images. \n", "The amount of data necessary to represent a raster image can be quite\nlarge.  However, the data usually contains a lot of redundancy, and the data can be \"compressed\"\nto reduce its size.  GIF and PNG use ", ", which means that the\noriginal image can be recovered perfectly from the compressed data.\nJPEG uses a ", " algorithm,\nwhich means that the image that is recovered from\na JPEG file is not exactly the same as the original image; some information has been lost.\nThis might not sound like a good idea, but in fact the difference is often not very noticeable, and\nusing lossy compression usually permits a greater reduction in the size of the compressed data.\nJPEG generally works well for photographic images, but not as well for images that have sharp edges\nbetween different colors.  It is especially bad for line drawings and images that contain text; PNG is \nthe preferred format for such images.  \n", "SVG, on the other hand, is fundamentally a vector graphics format (although SVG images can include\nraster images).  SVG is actually an XML-based language for describing two-dimensional vector graphics\nimages.  \"SVG\" stands for \"Scalable Vector Graphics,\" and the term \"scalable\" indicates one of the\nadvantages of vector graphics: There is no loss of quality  when the size of the image is increased.\nA line between two points can be represented at any scale, and it is still the same perfect geometric line.\nIf you try to greatly increase the size of a raster image, on the other hand, you will find that you don't\nhave enough color values for all the pixels in the new image; each pixel from the original image\nwill be expanded to cover a rectangle of pixels in the scaled image, and you will get multi-pixel blocks of\nuniform color.  The scalable nature of SVG images make them a good choice for web browsers and for\ngraphical elements on your computer's desktop.  And indeed, some desktop environments are now using\nSVG images for their desktop icons.", "A digital image, no matter what its format, is specified using a ", ".\nA coordinate system sets up a correspondence between numbers and geometric points.  In two dimensions,\neach point is assigned a pair of numbers, which are called the coordinates of the point.  The two coordinates\nof a point are often called its ", "-coordinate and ", "-coordinate, although the names\n\"x\" and \"y\" are arbitrary.", "A raster image is a two-dimensional grid of pixels arranged into\nrows and columns.  As such, it has a natural coordinate system in which each pixel corresponds \nto a pair of integers giving the number of the row and the number of the column that contain the\npixel.  (Even in this simple case, there is some disagreement as to whether the rows should be numbered from top-to-bottom\nor from bottom-to-top.)", "For a vector image, it is natural to use real-number coordinates.  The coordinate system for an\nimage is arbitrary to some degree; that is, the same image can be specified using different coordinate\nsystems.  I do not want to say a lot about coordinate systems here, but they will be a major\nfocus of a large part of the book, and they are even more important in three-dimensional graphics\nthan in two dimensions."], "chapter_title": "Introduction", "id": 1.1}, {"section_title": "3D Coordinates and Transforms", "chapter_id": "Chapter 3", "section_id": "Section 3.2", "content": ["In ", ", we looked fairly closely at \n", " and \n", " \nin two-dimensional computer graphics.  In this section and the\n", ", we will move that \ndiscussion into 3D.  Things are more complicated in three\ndimensions, but a lot of the basic concepts remain the\nsame.", "A coordinate system is a way of assigning numbers to points.\nIn two dimensions, you need a pair of numbers to specify a\npoint.  The coordinates are often referred to as ", " and\n", ", although of course, the names are arbitrary.  More than that,\nthe assignment of pairs of numbers to points is itself arbitrary to\na large extent.  Points and objects are real things, but\ncoordinates are just numbers that we assign to them so that\nwe can refer to them easily and work with them mathematically.\nWe have seen the power of this when we discussed transforms, \nwhich are defined mathematically in terms of coordinates but which have\nreal, useful physical meanings.", "In three dimensions, you need three numbers to specify a point.\n(That's essentially what it means to be three dimensional.)\nThe third coordinate is often called ", ".  The ", "-axis\nis perpendicular to both the ", "-axis and the ", "-axis.", "This demo illustrates a 3D coordinate\nsystem.  The positive directions of the ", ", ", ",\nand ", " axes are shown as big arrows.  The ", "-axis is green,\nthe ", "-axis is blue, and the ", "-axis is red. You can drag on the axes to rotate the image.", "\n", "\n", "This example is a 2D image, but it has a 3D look.  (The illusion is\nmuch stronger if you rotate the image.)  Several things\ncontribute to the effect.  For one thing, objects that are farther\naway from the viewer in 3D look smaller in the 2D image.  This is\ndue to the way that the 3D scene is \"projected\" onto 2D.  We will\ndiscuss projection in the ", ".\nAnother factor is the \"shading\" of the objects.  The objects are shaded\nin a way that imitates the interaction of objects with the light that\nilluminates them.  We will put off a discussion of lighting until\n", ".  In this section, we will concentrate on\nhow to construct a scene in 3D\u2014what we have referred to as\n", ".", "OpenGL programmers usually think in terms of a coordinate system in which\nthe ", "- and ", "-axes lie in the plane of the screen, and the ", "-axis is perpendicular\nto the screen with the positive direction of the ", "-axis pointing ", " the screen towards the viewer.   Now, the default\ncoordinate system in OpenGL, the one that you are using if you\napply no transformations at all, is similar but has the positive direction of the ", "-axis\npointing ", " the screen.  This is not a contradiction:  The coordinate\nsystem that is actually used is arbitrary.  It is set up by a transformation.\nThe convention in OpenGL is to work with a coordinate system in which the\npositive ", "-direction points toward the viewer and the negative\n", "-direction points away from the viewer.  The transformation into\ndefault coordinates reverses the direction of the ", "-axis.", "This conventional arrangement of the axes produces a \n", ".  This means that if\nyou point the thumb of your right hand in the direction of the positive\n", "-axis, then when you curl the fingers of that hand, they will curl\nin the direction from the positive ", "-axis towards the positive ", "-axis.\nIf you are looking at the tip of your thumb, the curl will be in the counterclockwise\ndirection.  Another way to think about it is that if you curl the figures of your\nright hand from the positive ", " to the positive ", "-axis, then your\nthumb will point in the direction of the positive ", "-axis.\nThe default OpenGL coordinate system (which, again, is hardly ever used)\nis a left-handed system.  You should spend some time trying to visualize\nright- and left-handed coordinates systems.  Use your hands!", "All of that describes the natural coordinate system from the viewer's point of view,\nthe so-called \"eye\" or \"viewing\" coordinate system.\nHowever, these ", " are not necessarily the natural coordinates on the world.\nThe coordinate system on the world\u2014the coordinate system in\nwhich the scene is assembled\u2014is referred to as\n", ".", "Recall that objects are not usually specified directly in world coordinates.\nInstead, objects are specified in their own coordinate system,\nknown as ", ", and then ", "\nare applied to place the objects into the world, or into more complex objects.\nIn OpenGL, object coordinates are the numbers that are used in the\n", " function to specify the vertices of the object.  However,\nbefore the objects appear on the screen, they are usually subject to a sequence\nof transformations, starting with a modeling transform.", "The basic transforms in 3D are extensions of the basic transforms that\nyou are already familiar with from 2D:  ", ", \n", ", and ", ".  We will look at\nthe 3D equivalents and see how they affect objects when applied as\nmodeling transforms.  We will also discuss how to use the transforms\nin OpenGL.", "Translation is easiest.  In 2D, a translation adds some number onto each coordinate.  The\nsame is true in 3D; we just need three numbers, to specify the amount of motion\nin the direction of each of the coordinate axes.  A translation by (", ")\ntransforms a point (", ") to the point (", ").\nIn OpenGL, this translation would be specified by the command", "or by the command", "The translation will affect any drawing that is done after the command is given.\nNote that there are two versions of the command.  The first, with a name ending in \"f\",\ntakes three ", " values as parameters.  The second, with a name ending in \"d\",\ntakes parameters of type ", ".  As an example,", "would translate objects by one unit in the ", " direction.", "Scaling works in a similar way: Instead of one scaling factor, you need three.  The\nOpenGL command for scaling is ", ", where the \"*\" can be either \"f\" or \"d\".\nThe command", "transforms a point (", ") to (", ").  That is,\nit scales by a factor of ", " in the ", " direction, ", " in the ", "\ndirection, and ", " in the ", " direction.  Scaling is about the origin;\nthat is, it moves points farther from or closed to the origin, (0,0,0).  For\nuniform scaling, all three factors would be the same.  You can use scaling by\na factor of minus one to apply a reflection.  For example,", "reflects objects through the ", "-plane by reversing the sign of the ", "\ncoordinate.  Note that a reflection will convert a right-handed coordinate system into\na left-handed coordinate system, and ", ".  Remember\nthat the left/right handed distinction is not a property of the world, just\nof the way that one chooses to lay out coordinates on the world.", "Rotation in 3D is harder.  In 2D, rotation is rotation about a point, which is usually\ntaken to be the origin.  In 3D, rotation is rotation about a line, which is called\nthe ", ".  Think of the Earth rotating\nabout its axis.  The axis of rotation is the line that passes through the North Pole \nand the South Pole.  The axis stays fixed as the Earth rotates around it,\nand points that are not on the\naxis move in circles about the axis.  Any line can be an axis of rotation, but\nwe generally use an axis that passes through the origin.  The most\ncommon choices for axis or rotation are the coordinates axes, that is,\nthe ", "-axis, the ", "-axis, or the ", "-axis.  Sometimes,\nhowever, it's convenient to be able to use a different line\nas the axis.", "There is an easy way to specify a line that\npasses through the origin:  Just specify one other\npoint that is on the line, in addition to the origin.   That's how things are\ndone in OpenGL:  An axis of rotation is specified by three numbers,\n(", "), which are not all zero.  The axis is the line\nthrough (0,0,0) and (", ").  To specify a rotation transformation in 3D,\nyou have to specify an axis and the angle of rotation about that axis.", "We still have to account for the difference between positive and negative\nangles.  We can't just say clockwise or counterclockwise.  If you look down on\nthe rotating Earth from above the North pole, you see a clockwise rotation; if you\nlook down on it from above the South pole, you see a counterclockwise rotation.\nSo, the difference between the two is not well-defined.  To define the\ndirection of rotation in 3D, we use the ", ", which\nsays:  Point the thumb of your right hand in the direction of the\naxis, from the point (0,0,0) to the point (", ") that determines the\naxis. Then the direction of rotation for positive angles is given by the\ndirection in which your fingers curl. I should emphasize that the right-hand rule\nonly works if you are working in a right-handed coordinate system.  If you have\nswitched to a left-handed coordinate system, then you need to use a\nleft-hand rule to determine the positive direction of rotation.", "You can use the following demo to help you understand rotation about an\naxis in three-dimensional space.  Use the buttons labeled \"+X\", \"-X\", and so on to\nmake the cube rotate about the coordinate axes, or enter any (", ") point\nand click \"Set\".  Drag your mouse on the image to rotate the \nscene.", "\n", "\n", "The rotation function in OpenGL is ", "(", ").\nYou can also use ", ".\nThe first parameter specifies the angle of rotation, measured in degrees.\nThe other three parameters specify the axis of rotation, which is the line\nfrom (0,0,0) to (", ").", "Here are a few examples of scaling, translation, and scaling in OpenGL:", "Remember that transforms are applied to objects that are drawn after\nthe transformation function is called, and that transformations apply to\nobjects in the opposite order of the order in which they appear in the code.", "Of course, OpenGL can draw in 2D as well as in 3D.  For 2D drawing in OpenGL,\nyou can draw on the ", "-plane, using zero for the ", " coordinate.\nWhen drawing in 2D, you will probably want to apply 2D versions of rotation, scaling,\nand translation.  OpenGL does not have 2D transform functions, but you can just use\nthe 3D versions with appropriate parameters:", "Modeling transformations are often used in ", ",\nwhich allows complex objects to be built up out of simpler objects.  See\n", ".  To review briefly:  In hierarchical modeling, an object can\nbe defined in its own natural coordinate system, usually using (0,0,0) as a reference\npoint.  The object can then be scaled, rotated, and translated to place it into\nworld coordinates or into a more complex object.  To implement this, we need\na way of limiting the effect of a modeling transformation to one object or to\npart of an object.  That can be done using a stack of transforms.  Before\ndrawing an object, push a copy of the current transform onto the stack.  After drawing\nthe object and its sub-objects, using any necessary temporary transformations,\nrestore the previous transform by popping it from the stack.", "OpenGL 1.1 maintains a stack of transforms and provides functions for\nmanipulating that stack.  (In fact it has several transform stacks, for different\npurposes, which introduces some complications that we will postpone to the \n", ".)  Since transforms are\nrepresented as ", ", the stack is actually a stack of matrices.\nIn OpenGL, the functions for operating on the stack are named ", "() and\n", "().", "These functions do not take parameters or return a value.  OpenGL keeps track of\na current matrix, which is the composition of all transforms that have been applied.\nCalling a function such as ", " simply modifies the current matrix.  When\nan object is drawn, using the ", " functions, the coordinates that are specified\nfor the object are transformed by the current matrix.  There is another function\nthat affects the current matrix:  ", "().  Calling\n", " sets the current matrix to be the ", ",\nwhich represents no change of coordinates at all and is the usual starting point for\na series of transformations.", "When the function ", "() is called, a copy of the current matrix is\npushed onto the stack.  Note that this does not change the current matrix; it just\nsaves a copy on the stack.  When ", "() is called, the matrix on the\ntop of the stack is popped from the stack, and that matrix replaces the current matrix.\nNote that ", " and ", " must always occur in\ncorresponding pairs; ", " saves a copy of the current matrix, and a corresponding\ncall to ", " restores that copy.  Between a call to ", " and\nthe corresponding call to ", ", there can be additional calls of these\nfunctions, as long as they are properly paired.  Usually, you will call ", "\nbefore drawing an object and ", " after finishing that object.  In between,\ndrawing sub-objects might require additional pairs of calls to those functions.", "As an example, suppose that we want to draw a cube.  It's not hard to draw each\nface using glBegin/glEnd, but let's do it with transformations.  We can start with\na function that draws a square in the position of the front face of the cube.\nFor a cube of size 1, the front face would sit one-half unit in front of the screen,\nin the plane ", "\u00a0=\u00a00.5, and it would have vertices at\n(-0.5,\u00a0-0.5,\u00a00.5), (0.5,\u00a0-0.5,\u00a00.5), (0.5,\u00a00.5,\u00a00.5), and (-0.5,\u00a00.5,\u00a00.5).\nHere is a function that draws the square.  The parameters are floating \npoint numbers in the range 0.0 to 1.0\nthat give the RGB color of the square:", "To make a red front face for the cube, we just need to call ", "(1,0,0).\nNow, consider the right face, which is perpendicular to the ", "-axis, in the plane\n", "\u00a0=\u00a00.5.  To make a right face, we can start with a front face and rotate\nit 90 degrees about the ", "-axis.  Think about rotating the front face (red) to the position\nof the right face (green) in this illustration by rotating the front face about the ", ":", "\n", "So, we can draw a green right face for the cube with", "The calls to ", " and ", " ensure that the rotation that\nis applied to the square will not carry over to objects that are drawn later.\nThe other four faces can be made in a similar way, by rotating the front face about the\ncoordinate axes.  You should try to visualize the rotation that you need in each case.\nWe can combine it all into a function that draws a cube.  To make it more interesting,\nthe size of the cube is a parameter:", "The sample program ", " uses this function to\ndraw a cube, and lets you rotate the cube by pressing the arrow keys.\nA Java version is ", ", and a web version is\n", ".  Here is an image of the cube, rotated\nby 15 degrees about the ", "-axis and -15 degrees about the ", "-axis to make\nthe top and right sides visible:", "\n", "For a more complex example of hierarchical modeling with ", " and\n", ", you can check out an OpenGL equivalent of the \"cart and windmills\"\nanimation that was used as an example in ", ".  \nThe three versions of the example are:\n", ",\n", ", and\n", ".\nThis program is an example of hierarchical 2D graphics in OpenGL."], "chapter_title": "OpenGL 1.1: Geometry", "id": 3.2}]